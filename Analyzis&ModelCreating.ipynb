{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–µ–∫—Ç - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "–ö–æ–º–ø–∞–Ω–∏—è –æ–Ω–ª–∞–π–Ω-—Å–µ—Ä–≤–∏—Å —Å –≤—ã—Å–æ–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –≤—Ö–æ–¥—è—â–µ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ –∏–º–µ–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–¥–µ–ª –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –∞–Ω–∞–ª–∏–∑–æ–º —Ç—Ä–∞—Ñ–∏–∫–∞. –°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ —ç—Ç–æ–≥–æ –æ—Ç–¥–µ–ª–∞ –æ–±—Ä–∞—Ç–∏–ª–∏—Å—å –∑–∞ –ø–æ–º–æ—â—å—é –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª—å–Ω–æ–≥–æ –∏ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞—Ñ–∏–∫ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –∏ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–π, –≤–∫–ª—é—á–∞—è —Å–ª–µ–¥—É—é—â–∏–µ —Ç–∏–ø—ã –∞—Ç–∞–∫: DDoS, SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏, –±—Ä—É—Ç—Ñ–æ—Ä—Å, –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ —Ç.–¥.\n",
    "\n",
    "–í —Ö–æ–¥–µ —Ä–∞–±–æ—Ç—ã –≤—ã –ø—Ä–æ–π–¥–µ—Ç–µ –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:\\\n",
    "üî∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏,\\\n",
    "üî∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞,\\\n",
    "üî∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ä–∞–∑–≤–µ–¥–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑,\\\n",
    "üî∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\\\n",
    "üî∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å,\\\n",
    "üî∏ –æ—Ç–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\\\n",
    "üî∏ –≤—ã–±–æ—Ä –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π,\\\n",
    "üî∏ –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏,\\\n",
    "üî∏ –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –µ–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "### –ó–∞–¥–∞—á–∞\n",
    "\n",
    "üî∏ –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞—Ñ–∏–∫ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –∏ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–π. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Ä–∞–±–æ—Ç–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ü–µ–Ω–∞ –æ—à–∏–±–∫–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞.\\\n",
    "üî∏ –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: precision, recall, f1_score, accuracy.\\\n",
    "üî∏ (*) –î–µ–ø–ª–æ–π: —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å REST API —Å–µ—Ä–≤–∏—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞—Ñ–∏–∫–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∫–ª–∞—Å—Å —ç—Ç–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞.\\\n",
    "\n",
    "### –≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:\n",
    "\n",
    "üî∏ –í–≤–æ–¥–Ω—ã–π –≤–µ–±–∏–Ω–∞—Ä, –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\\\n",
    "üî∏ –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞, –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ Stand-up, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–∞–Ω–∞–ª–µ\\\n",
    "üî∏ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "\n",
    "üî∏ –°—Å—ã–ª–∫–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç - https://disk.yandex.ru/d/QYraoEwmfQZ90Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã\n",
    "\n",
    "**1. –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –¥–∞–Ω–Ω—ã–º–∏**\n",
    "\n",
    "    1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "    \n",
    "    1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    \n",
    "    1.3 –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (—Ä–∞–∑–º–µ—Ä, –ø—Ä–∏–∑–Ω–∞–∫–∏, —Å—Ç—Ä–æ–∫–∏, —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∏, –∞–Ω–æ–º–∞–ª–∏–∏, –¥—É–±–ª–∏–∫–∞—Ç—ã, –∫–æ—Ä–µ–ª—è—Ü–∏—è, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\n",
    "    \n",
    " **2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**\n",
    "    \n",
    "    2.1 –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö \n",
    "    \n",
    "    2.2 –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏\n",
    "    \n",
    "    2.3 –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    \n",
    " **3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**\n",
    " \n",
    " \n",
    " **4. –í—ã–≤–æ–¥**\n",
    " \n",
    " **5. –û—Ç—á–µ—Ç**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost.utils import eval_metric\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# settings for pandas\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.options.mode.chained_assignment = None\n",
    "\n",
    "# warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\network_traffic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 539616 entries, 0 to 539615\n",
      "Data columns (total 79 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0    Destination Port             539616 non-null  int64  \n",
      " 1    Flow Duration                539616 non-null  int64  \n",
      " 2    Total Fwd Packets            539616 non-null  int64  \n",
      " 3    Total Backward Packets       539616 non-null  int64  \n",
      " 4   Total Length of Fwd Packets   539616 non-null  int64  \n",
      " 5    Total Length of Bwd Packets  539616 non-null  int64  \n",
      " 6    Fwd Packet Length Max        539616 non-null  int64  \n",
      " 7    Fwd Packet Length Min        539616 non-null  int64  \n",
      " 8    Fwd Packet Length Mean       539616 non-null  float64\n",
      " 9    Fwd Packet Length Std        539616 non-null  float64\n",
      " 10  Bwd Packet Length Max         539616 non-null  int64  \n",
      " 11   Bwd Packet Length Min        539616 non-null  int64  \n",
      " 12   Bwd Packet Length Mean       539616 non-null  float64\n",
      " 13   Bwd Packet Length Std        539616 non-null  float64\n",
      " 14  Flow Bytes/s                  539128 non-null  float64\n",
      " 15   Flow Packets/s               539616 non-null  float64\n",
      " 16   Flow IAT Mean                539616 non-null  float64\n",
      " 17   Flow IAT Std                 539616 non-null  float64\n",
      " 18   Flow IAT Max                 539616 non-null  int64  \n",
      " 19   Flow IAT Min                 539616 non-null  int64  \n",
      " 20  Fwd IAT Total                 539616 non-null  int64  \n",
      " 21   Fwd IAT Mean                 539616 non-null  float64\n",
      " 22   Fwd IAT Std                  539616 non-null  float64\n",
      " 23   Fwd IAT Max                  539616 non-null  int64  \n",
      " 24   Fwd IAT Min                  539616 non-null  int64  \n",
      " 25  Bwd IAT Total                 539616 non-null  int64  \n",
      " 26   Bwd IAT Mean                 539616 non-null  float64\n",
      " 27   Bwd IAT Std                  539616 non-null  float64\n",
      " 28   Bwd IAT Max                  539616 non-null  int64  \n",
      " 29   Bwd IAT Min                  539616 non-null  int64  \n",
      " 30  Fwd PSH Flags                 539616 non-null  int64  \n",
      " 31   Bwd PSH Flags                539616 non-null  int64  \n",
      " 32   Fwd URG Flags                539616 non-null  int64  \n",
      " 33   Bwd URG Flags                539616 non-null  int64  \n",
      " 34   Fwd Header Length            539616 non-null  int64  \n",
      " 35   Bwd Header Length            539616 non-null  int64  \n",
      " 36  Fwd Packets/s                 539616 non-null  float64\n",
      " 37   Bwd Packets/s                539616 non-null  float64\n",
      " 38   Min Packet Length            539616 non-null  int64  \n",
      " 39   Max Packet Length            539616 non-null  int64  \n",
      " 40   Packet Length Mean           539616 non-null  float64\n",
      " 41   Packet Length Std            539616 non-null  float64\n",
      " 42   Packet Length Variance       539616 non-null  float64\n",
      " 43  FIN Flag Count                539616 non-null  int64  \n",
      " 44   SYN Flag Count               539616 non-null  int64  \n",
      " 45   RST Flag Count               539616 non-null  int64  \n",
      " 46   PSH Flag Count               539616 non-null  int64  \n",
      " 47   ACK Flag Count               539616 non-null  int64  \n",
      " 48   URG Flag Count               539616 non-null  int64  \n",
      " 49   CWE Flag Count               539616 non-null  int64  \n",
      " 50   ECE Flag Count               539616 non-null  int64  \n",
      " 51   Down/Up Ratio                539616 non-null  int64  \n",
      " 52   Average Packet Size          539616 non-null  float64\n",
      " 53   Avg Fwd Segment Size         539616 non-null  float64\n",
      " 54   Avg Bwd Segment Size         539616 non-null  float64\n",
      " 55   Fwd Header Length.1          539616 non-null  int64  \n",
      " 56  Fwd Avg Bytes/Bulk            539616 non-null  int64  \n",
      " 57   Fwd Avg Packets/Bulk         539616 non-null  int64  \n",
      " 58   Fwd Avg Bulk Rate            539616 non-null  int64  \n",
      " 59   Bwd Avg Bytes/Bulk           539616 non-null  int64  \n",
      " 60   Bwd Avg Packets/Bulk         539616 non-null  int64  \n",
      " 61  Bwd Avg Bulk Rate             539616 non-null  int64  \n",
      " 62  Subflow Fwd Packets           539616 non-null  int64  \n",
      " 63   Subflow Fwd Bytes            539616 non-null  int64  \n",
      " 64   Subflow Bwd Packets          539616 non-null  int64  \n",
      " 65   Subflow Bwd Bytes            539616 non-null  int64  \n",
      " 66  Init_Win_bytes_forward        539616 non-null  int64  \n",
      " 67   Init_Win_bytes_backward      539616 non-null  int64  \n",
      " 68   act_data_pkt_fwd             539616 non-null  int64  \n",
      " 69   min_seg_size_forward         539616 non-null  int64  \n",
      " 70  Active Mean                   539616 non-null  float64\n",
      " 71   Active Std                   539616 non-null  float64\n",
      " 72   Active Max                   539616 non-null  int64  \n",
      " 73   Active Min                   539616 non-null  int64  \n",
      " 74  Idle Mean                     539616 non-null  float64\n",
      " 75   Idle Std                     539616 non-null  float64\n",
      " 76   Idle Max                     539616 non-null  int64  \n",
      " 77   Idle Min                     539616 non-null  int64  \n",
      " 78  Label                         539616 non-null  object \n",
      "dtypes: float64(24), int64(54), object(1)\n",
      "memory usage: 325.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79 —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ 539616 –∑–Ω–∞—á–µ–Ω–∏–π. –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ Label.–Ø–≤–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç. –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–µ–º—ã–º.\n",
    "\n",
    "–í –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å —Å—Ç–æ–ª–±—Ü—ã —Å min max mean std –≤–æ–∑–º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <th>Bwd Packet Length Min</th>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <th>Flow IAT Std</th>\n",
       "      <th>Flow IAT Max</th>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <th>Fwd IAT Total</th>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <th>Bwd IAT Min</th>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <th>Min Packet Length</th>\n",
       "      <th>Max Packet Length</th>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <th>Packet Length Std</th>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <th>SYN Flag Count</th>\n",
       "      <th>RST Flag Count</th>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <th>URG Flag Count</th>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <th>Average Packet Size</th>\n",
       "      <th>Avg Fwd Segment Size</th>\n",
       "      <th>Avg Bwd Segment Size</th>\n",
       "      <th>Fwd Header Length.1</th>\n",
       "      <th>Fwd Avg Bytes/Bulk</th>\n",
       "      <th>Fwd Avg Packets/Bulk</th>\n",
       "      <th>Fwd Avg Bulk Rate</th>\n",
       "      <th>Bwd Avg Bytes/Bulk</th>\n",
       "      <th>Bwd Avg Packets/Bulk</th>\n",
       "      <th>Bwd Avg Bulk Rate</th>\n",
       "      <th>Subflow Fwd Packets</th>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <th>Init_Win_bytes_forward</th>\n",
       "      <th>Init_Win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>5480074</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1826691.40</td>\n",
       "      <td>3131700.20</td>\n",
       "      <td>5442804</td>\n",
       "      <td>101</td>\n",
       "      <td>5480074</td>\n",
       "      <td>2740037.00</td>\n",
       "      <td>3822289.80</td>\n",
       "      <td>5442804</td>\n",
       "      <td>37270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>32</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.29</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>42780</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443</td>\n",
       "      <td>711977</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>703</td>\n",
       "      <td>3950</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>78.10</td>\n",
       "      <td>103.30</td>\n",
       "      <td>1448</td>\n",
       "      <td>0</td>\n",
       "      <td>395.00</td>\n",
       "      <td>587.50</td>\n",
       "      <td>6535.32</td>\n",
       "      <td>26.69</td>\n",
       "      <td>39554.28</td>\n",
       "      <td>50154.62</td>\n",
       "      <td>120501</td>\n",
       "      <td>1</td>\n",
       "      <td>616301</td>\n",
       "      <td>77037.62</td>\n",
       "      <td>72995.98</td>\n",
       "      <td>215614</td>\n",
       "      <td>230</td>\n",
       "      <td>616874</td>\n",
       "      <td>68541.55</td>\n",
       "      <td>71985.97</td>\n",
       "      <td>199836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "      <td>328</td>\n",
       "      <td>12.64</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0</td>\n",
       "      <td>1448</td>\n",
       "      <td>232.60</td>\n",
       "      <td>442.80</td>\n",
       "      <td>196012.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>244.90</td>\n",
       "      <td>78.10</td>\n",
       "      <td>395.00</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>703</td>\n",
       "      <td>10</td>\n",
       "      <td>3950</td>\n",
       "      <td>29200</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>153398</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>224</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1981.77</td>\n",
       "      <td>26.08</td>\n",
       "      <td>51132.67</td>\n",
       "      <td>88558.31</td>\n",
       "      <td>153391</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>13.04</td>\n",
       "      <td>13.04</td>\n",
       "      <td>40</td>\n",
       "      <td>112</td>\n",
       "      <td>68.80</td>\n",
       "      <td>39.44</td>\n",
       "      <td>1555.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>224</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>57660</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>128</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3017.69</td>\n",
       "      <td>34.69</td>\n",
       "      <td>57660.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>57660</td>\n",
       "      <td>57660</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.34</td>\n",
       "      <td>46</td>\n",
       "      <td>128</td>\n",
       "      <td>73.30</td>\n",
       "      <td>47.34</td>\n",
       "      <td>2241.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>128.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8446</td>\n",
       "      <td>767</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>14.34</td>\n",
       "      <td>14.44</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63885.27</td>\n",
       "      <td>5215.12</td>\n",
       "      <td>255.67</td>\n",
       "      <td>394.29</td>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>713</td>\n",
       "      <td>356.50</td>\n",
       "      <td>499.92</td>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>3911.34</td>\n",
       "      <td>1303.78</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.18</td>\n",
       "      <td>125.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.75</td>\n",
       "      <td>14.34</td>\n",
       "      <td>6.00</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1017</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
       "0                 80         5480074                   3   \n",
       "1                443          711977                   9   \n",
       "2                 53          153398                   2   \n",
       "3                 53           57660                   1   \n",
       "4               8446             767                   3   \n",
       "\n",
       "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
       "0                        1                           12   \n",
       "1                       10                          703   \n",
       "2                        2                           80   \n",
       "3                        1                           46   \n",
       "4                        1                           43   \n",
       "\n",
       "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
       "0                             0                       6   \n",
       "1                          3950                     267   \n",
       "2                           224                      40   \n",
       "3                           128                      46   \n",
       "4                             6                      31   \n",
       "\n",
       "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
       "0                       0                     4.00                    3.46   \n",
       "1                       0                    78.10                  103.30   \n",
       "2                      40                    40.00                    0.00   \n",
       "3                      46                    46.00                    0.00   \n",
       "4                       6                    14.34                   14.44   \n",
       "\n",
       "   Bwd Packet Length Max   Bwd Packet Length Min   Bwd Packet Length Mean  \\\n",
       "0                      0                       0                     0.00   \n",
       "1                   1448                       0                   395.00   \n",
       "2                    112                     112                   112.00   \n",
       "3                    128                     128                   128.00   \n",
       "4                      6                       6                     6.00   \n",
       "\n",
       "    Bwd Packet Length Std  Flow Bytes/s   Flow Packets/s   Flow IAT Mean  \\\n",
       "0                    0.00          2.19             0.73      1826691.40   \n",
       "1                  587.50       6535.32            26.69        39554.28   \n",
       "2                    0.00       1981.77            26.08        51132.67   \n",
       "3                    0.00       3017.69            34.69        57660.00   \n",
       "4                    0.00      63885.27          5215.12          255.67   \n",
       "\n",
       "    Flow IAT Std   Flow IAT Max   Flow IAT Min  Fwd IAT Total   Fwd IAT Mean  \\\n",
       "0     3131700.20        5442804            101        5480074     2740037.00   \n",
       "1       50154.62         120501              1         616301       77037.62   \n",
       "2       88558.31         153391              3              3           3.00   \n",
       "3           0.00          57660          57660              0           0.00   \n",
       "4         394.29            710              3            713         356.50   \n",
       "\n",
       "    Fwd IAT Std   Fwd IAT Max   Fwd IAT Min  Bwd IAT Total   Bwd IAT Mean  \\\n",
       "0    3822289.80       5442804         37270              0           0.00   \n",
       "1      72995.98        215614           230         616874       68541.55   \n",
       "2          0.00             3             3              4           4.00   \n",
       "3          0.00             0             0              0           0.00   \n",
       "4        499.92           710             3              0           0.00   \n",
       "\n",
       "    Bwd IAT Std   Bwd IAT Max   Bwd IAT Min  Fwd PSH Flags   Bwd PSH Flags  \\\n",
       "0          0.00             0             0              0               0   \n",
       "1      71985.97        199836             1              0               0   \n",
       "2          0.00             4             4              0               0   \n",
       "3          0.00             0             0              0               0   \n",
       "4          0.00             0             0              0               0   \n",
       "\n",
       "    Fwd URG Flags   Bwd URG Flags   Fwd Header Length   Bwd Header Length  \\\n",
       "0               0               0                  72                  32   \n",
       "1               0               0                 296                 328   \n",
       "2               0               0                  40                  40   \n",
       "3               0               0                  20                  20   \n",
       "4               0               0                  60                  20   \n",
       "\n",
       "   Fwd Packets/s   Bwd Packets/s   Min Packet Length   Max Packet Length  \\\n",
       "0           0.55            0.18                   0                   6   \n",
       "1          12.64           14.05                   0                1448   \n",
       "2          13.04           13.04                  40                 112   \n",
       "3          17.34           17.34                  46                 128   \n",
       "4        3911.34         1303.78                   6                  31   \n",
       "\n",
       "    Packet Length Mean   Packet Length Std   Packet Length Variance  \\\n",
       "0                 2.40                3.29                    10.80   \n",
       "1               232.60              442.80                196012.66   \n",
       "2                68.80               39.44                  1555.20   \n",
       "3                73.30               47.34                  2241.33   \n",
       "4                11.00               11.18                   125.00   \n",
       "\n",
       "   FIN Flag Count   SYN Flag Count   RST Flag Count   PSH Flag Count  \\\n",
       "0               0                0                0                1   \n",
       "1               0                0                0                1   \n",
       "2               0                0                0                0   \n",
       "3               0                0                0                0   \n",
       "4               0                0                0                0   \n",
       "\n",
       "    ACK Flag Count   URG Flag Count   CWE Flag Count   ECE Flag Count  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                1                0                0                0   \n",
       "\n",
       "    Down/Up Ratio   Average Packet Size   Avg Fwd Segment Size  \\\n",
       "0               0                  3.00                   4.00   \n",
       "1               1                244.90                  78.10   \n",
       "2               1                 86.00                  40.00   \n",
       "3               1                110.00                  46.00   \n",
       "4               0                 13.75                  14.34   \n",
       "\n",
       "    Avg Bwd Segment Size   Fwd Header Length.1  Fwd Avg Bytes/Bulk  \\\n",
       "0                   0.00                    72                   0   \n",
       "1                 395.00                   296                   0   \n",
       "2                 112.00                    40                   0   \n",
       "3                 128.00                    20                   0   \n",
       "4                   6.00                    60                   0   \n",
       "\n",
       "    Fwd Avg Packets/Bulk   Fwd Avg Bulk Rate   Bwd Avg Bytes/Bulk  \\\n",
       "0                      0                   0                    0   \n",
       "1                      0                   0                    0   \n",
       "2                      0                   0                    0   \n",
       "3                      0                   0                    0   \n",
       "4                      0                   0                    0   \n",
       "\n",
       "    Bwd Avg Packets/Bulk  Bwd Avg Bulk Rate  Subflow Fwd Packets  \\\n",
       "0                      0                  0                    3   \n",
       "1                      0                  0                    9   \n",
       "2                      0                  0                    2   \n",
       "3                      0                  0                    1   \n",
       "4                      0                  0                    3   \n",
       "\n",
       "    Subflow Fwd Bytes   Subflow Bwd Packets   Subflow Bwd Bytes  \\\n",
       "0                  12                     1                   0   \n",
       "1                 703                    10                3950   \n",
       "2                  80                     2                 224   \n",
       "3                  46                     1                 128   \n",
       "4                  43                     1                   6   \n",
       "\n",
       "   Init_Win_bytes_forward   Init_Win_bytes_backward   act_data_pkt_fwd  \\\n",
       "0                    8192                     42780                  2   \n",
       "1                   29200                       252                  4   \n",
       "2                      -1                        -1                  1   \n",
       "3                      -1                        -1                  0   \n",
       "4                    1017                         0                  2   \n",
       "\n",
       "    min_seg_size_forward  Active Mean   Active Std   Active Max   Active Min  \\\n",
       "0                     20         0.00         0.00            0            0   \n",
       "1                     32         0.00         0.00            0            0   \n",
       "2                     20         0.00         0.00            0            0   \n",
       "3                     20         0.00         0.00            0            0   \n",
       "4                     20         0.00         0.00            0            0   \n",
       "\n",
       "   Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
       "0       0.00       0.00          0          0  BENIGN  \n",
       "1       0.00       0.00          0          0  BENIGN  \n",
       "2       0.00       0.00          0          0  BENIGN  \n",
       "3       0.00       0.00          0          0  BENIGN  \n",
       "4       0.00       0.00          0          0  BENIGN  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <th>Bwd Packet Length Min</th>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <th>Flow IAT Std</th>\n",
       "      <th>Flow IAT Max</th>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <th>Fwd IAT Total</th>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <th>Bwd IAT Min</th>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <th>Min Packet Length</th>\n",
       "      <th>Max Packet Length</th>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <th>Packet Length Std</th>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <th>SYN Flag Count</th>\n",
       "      <th>RST Flag Count</th>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <th>URG Flag Count</th>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <th>Average Packet Size</th>\n",
       "      <th>Avg Fwd Segment Size</th>\n",
       "      <th>Avg Bwd Segment Size</th>\n",
       "      <th>Fwd Header Length.1</th>\n",
       "      <th>Fwd Avg Bytes/Bulk</th>\n",
       "      <th>Fwd Avg Packets/Bulk</th>\n",
       "      <th>Fwd Avg Bulk Rate</th>\n",
       "      <th>Bwd Avg Bytes/Bulk</th>\n",
       "      <th>Bwd Avg Packets/Bulk</th>\n",
       "      <th>Bwd Avg Bulk Rate</th>\n",
       "      <th>Subflow Fwd Packets</th>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <th>Init_Win_bytes_forward</th>\n",
       "      <th>Init_Win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539128.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616.00</td>\n",
       "      <td>539616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5563.06</td>\n",
       "      <td>21066929.37</td>\n",
       "      <td>6.12</td>\n",
       "      <td>5.99</td>\n",
       "      <td>408.24</td>\n",
       "      <td>7999.78</td>\n",
       "      <td>169.45</td>\n",
       "      <td>11.86</td>\n",
       "      <td>44.68</td>\n",
       "      <td>59.53</td>\n",
       "      <td>1675.07</td>\n",
       "      <td>23.51</td>\n",
       "      <td>550.04</td>\n",
       "      <td>697.31</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>2134902.45</td>\n",
       "      <td>5264534.55</td>\n",
       "      <td>17318403.50</td>\n",
       "      <td>294558.36</td>\n",
       "      <td>20756340.18</td>\n",
       "      <td>4036797.30</td>\n",
       "      <td>6666492.26</td>\n",
       "      <td>17200410.93</td>\n",
       "      <td>1035760.29</td>\n",
       "      <td>9601754.01</td>\n",
       "      <td>2028745.22</td>\n",
       "      <td>2413091.18</td>\n",
       "      <td>6464188.15</td>\n",
       "      <td>814778.83</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4593.96</td>\n",
       "      <td>-4290.45</td>\n",
       "      <td>71421.49</td>\n",
       "      <td>7885.31</td>\n",
       "      <td>9.61</td>\n",
       "      <td>1723.70</td>\n",
       "      <td>276.58</td>\n",
       "      <td>544.09</td>\n",
       "      <td>1112687.62</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>305.36</td>\n",
       "      <td>44.68</td>\n",
       "      <td>550.04</td>\n",
       "      <td>-4593.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.12</td>\n",
       "      <td>408.24</td>\n",
       "      <td>5.99</td>\n",
       "      <td>7998.77</td>\n",
       "      <td>7564.76</td>\n",
       "      <td>1277.94</td>\n",
       "      <td>2.95</td>\n",
       "      <td>-2429.31</td>\n",
       "      <td>115498.42</td>\n",
       "      <td>40375.70</td>\n",
       "      <td>167288.24</td>\n",
       "      <td>89830.17</td>\n",
       "      <td>16101546.63</td>\n",
       "      <td>958564.89</td>\n",
       "      <td>16872266.93</td>\n",
       "      <td>15391495.53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14921.26</td>\n",
       "      <td>38121719.63</td>\n",
       "      <td>345.11</td>\n",
       "      <td>463.49</td>\n",
       "      <td>7478.35</td>\n",
       "      <td>1014441.80</td>\n",
       "      <td>560.53</td>\n",
       "      <td>60.96</td>\n",
       "      <td>150.87</td>\n",
       "      <td>217.73</td>\n",
       "      <td>2843.53</td>\n",
       "      <td>55.20</td>\n",
       "      <td>869.70</td>\n",
       "      <td>1255.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5824584.75</td>\n",
       "      <td>10551103.82</td>\n",
       "      <td>33957014.03</td>\n",
       "      <td>4269410.12</td>\n",
       "      <td>38121061.80</td>\n",
       "      <td>10627587.74</td>\n",
       "      <td>13914086.55</td>\n",
       "      <td>34021535.79</td>\n",
       "      <td>8949950.83</td>\n",
       "      <td>28072181.79</td>\n",
       "      <td>9045625.35</td>\n",
       "      <td>8975672.92</td>\n",
       "      <td>21921965.46</td>\n",
       "      <td>7698886.82</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2095355.38</td>\n",
       "      <td>2076610.16</td>\n",
       "      <td>268528.74</td>\n",
       "      <td>38472.68</td>\n",
       "      <td>21.55</td>\n",
       "      <td>2862.90</td>\n",
       "      <td>416.17</td>\n",
       "      <td>903.66</td>\n",
       "      <td>2468133.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.63</td>\n",
       "      <td>457.15</td>\n",
       "      <td>150.87</td>\n",
       "      <td>869.70</td>\n",
       "      <td>2095355.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>345.11</td>\n",
       "      <td>7478.35</td>\n",
       "      <td>463.49</td>\n",
       "      <td>1013814.39</td>\n",
       "      <td>13440.65</td>\n",
       "      <td>6590.25</td>\n",
       "      <td>267.51</td>\n",
       "      <td>1052328.64</td>\n",
       "      <td>778752.95</td>\n",
       "      <td>434703.20</td>\n",
       "      <td>1059625.81</td>\n",
       "      <td>709971.01</td>\n",
       "      <td>33154633.35</td>\n",
       "      <td>6461275.27</td>\n",
       "      <td>34038514.45</td>\n",
       "      <td>32956945.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>-13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1073741320.00</td>\n",
       "      <td>-1073741320.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1073741320.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-536870660.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.04</td>\n",
       "      <td>0.73</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.00</td>\n",
       "      <td>49739.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2263.83</td>\n",
       "      <td>81.92</td>\n",
       "      <td>17000.58</td>\n",
       "      <td>10944.70</td>\n",
       "      <td>42572.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>271.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>439.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.92</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>50.80</td>\n",
       "      <td>18.86</td>\n",
       "      <td>355.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.50</td>\n",
       "      <td>11.22</td>\n",
       "      <td>54.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>274.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>443.00</td>\n",
       "      <td>10640295.25</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>8216.25</td>\n",
       "      <td>272.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>48.56</td>\n",
       "      <td>91.70</td>\n",
       "      <td>2796.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>824.50</td>\n",
       "      <td>835.50</td>\n",
       "      <td>125000.00</td>\n",
       "      <td>28985.51</td>\n",
       "      <td>1734360.65</td>\n",
       "      <td>3073641.88</td>\n",
       "      <td>8020226.50</td>\n",
       "      <td>54.00</td>\n",
       "      <td>8699965.50</td>\n",
       "      <td>2153087.25</td>\n",
       "      <td>2907944.03</td>\n",
       "      <td>7341524.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>151874.25</td>\n",
       "      <td>33099.85</td>\n",
       "      <td>58444.52</td>\n",
       "      <td>136467.50</td>\n",
       "      <td>46.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>164.00</td>\n",
       "      <td>136.00</td>\n",
       "      <td>14705.88</td>\n",
       "      <td>8695.65</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2896.00</td>\n",
       "      <td>484.57</td>\n",
       "      <td>812.50</td>\n",
       "      <td>659877.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>523.00</td>\n",
       "      <td>48.56</td>\n",
       "      <td>824.50</td>\n",
       "      <td>164.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8216.25</td>\n",
       "      <td>8192.00</td>\n",
       "      <td>235.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7125040.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7143036.50</td>\n",
       "      <td>6028677.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65533.00</td>\n",
       "      <td>119999993.00</td>\n",
       "      <td>200755.00</td>\n",
       "      <td>270686.00</td>\n",
       "      <td>2866110.00</td>\n",
       "      <td>591000000.00</td>\n",
       "      <td>24820.00</td>\n",
       "      <td>2065.00</td>\n",
       "      <td>5940.00</td>\n",
       "      <td>7050.00</td>\n",
       "      <td>17376.00</td>\n",
       "      <td>2042.00</td>\n",
       "      <td>5800.00</td>\n",
       "      <td>8190.00</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>84700000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>83200000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>81700000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4173072.00</td>\n",
       "      <td>5413720.00</td>\n",
       "      <td>3000000.00</td>\n",
       "      <td>2000000.00</td>\n",
       "      <td>1330.00</td>\n",
       "      <td>24820.00</td>\n",
       "      <td>2160.00</td>\n",
       "      <td>4732.00</td>\n",
       "      <td>22400000.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2508.00</td>\n",
       "      <td>5940.00</td>\n",
       "      <td>5800.00</td>\n",
       "      <td>4173072.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>200755.00</td>\n",
       "      <td>2866110.00</td>\n",
       "      <td>270686.00</td>\n",
       "      <td>590596141.00</td>\n",
       "      <td>65535.00</td>\n",
       "      <td>65535.00</td>\n",
       "      <td>192491.00</td>\n",
       "      <td>138.00</td>\n",
       "      <td>102000000.00</td>\n",
       "      <td>63500000.00</td>\n",
       "      <td>102000000.00</td>\n",
       "      <td>102000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>76900000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>120000000.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Destination Port   Flow Duration   Total Fwd Packets  \\\n",
       "count           539616.00       539616.00           539616.00   \n",
       "unique                NaN             NaN                 NaN   \n",
       "top                   NaN             NaN                 NaN   \n",
       "freq                  NaN             NaN                 NaN   \n",
       "mean              5563.06     21066929.37                6.12   \n",
       "std              14921.26     38121719.63              345.11   \n",
       "min                  0.00          -12.00                1.00   \n",
       "25%                 80.00           73.00                1.00   \n",
       "50%                 80.00        49739.00                2.00   \n",
       "75%                443.00     10640295.25                6.00   \n",
       "max              65533.00    119999993.00           200755.00   \n",
       "\n",
       "         Total Backward Packets  Total Length of Fwd Packets  \\\n",
       "count                 539616.00                    539616.00   \n",
       "unique                      NaN                          NaN   \n",
       "top                         NaN                          NaN   \n",
       "freq                        NaN                          NaN   \n",
       "mean                       5.99                       408.24   \n",
       "std                      463.49                      7478.35   \n",
       "min                        0.00                         0.00   \n",
       "25%                        1.00                         2.00   \n",
       "50%                        2.00                        44.00   \n",
       "75%                        5.00                       320.00   \n",
       "max                   270686.00                   2866110.00   \n",
       "\n",
       "         Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
       "count                      539616.00               539616.00   \n",
       "unique                           NaN                     NaN   \n",
       "top                              NaN                     NaN   \n",
       "freq                             NaN                     NaN   \n",
       "mean                         7999.78                  169.45   \n",
       "std                       1014441.80                  560.53   \n",
       "min                             0.00                    0.00   \n",
       "25%                             0.00                    2.00   \n",
       "50%                           105.00                   23.00   \n",
       "75%                          8216.25                  272.00   \n",
       "max                     591000000.00                24820.00   \n",
       "\n",
       "         Fwd Packet Length Min   Fwd Packet Length Mean  \\\n",
       "count                539616.00                539616.00   \n",
       "unique                     NaN                      NaN   \n",
       "top                        NaN                      NaN   \n",
       "freq                       NaN                      NaN   \n",
       "mean                     11.86                    44.68   \n",
       "std                      60.96                   150.87   \n",
       "min                       0.00                     0.00   \n",
       "25%                       0.00                     2.00   \n",
       "50%                       0.00                    11.22   \n",
       "75%                       6.00                    48.56   \n",
       "max                    2065.00                  5940.00   \n",
       "\n",
       "         Fwd Packet Length Std  Bwd Packet Length Max   Bwd Packet Length Min  \\\n",
       "count                539616.00              539616.00               539616.00   \n",
       "unique                     NaN                    NaN                     NaN   \n",
       "top                        NaN                    NaN                     NaN   \n",
       "freq                       NaN                    NaN                     NaN   \n",
       "mean                     59.53                1675.07                   23.51   \n",
       "std                     217.73                2843.53                   55.20   \n",
       "min                       0.00                   0.00                    0.00   \n",
       "25%                       0.00                   0.00                    0.00   \n",
       "50%                       0.00                  60.00                    0.00   \n",
       "75%                      91.70                2796.00                    6.00   \n",
       "max                    7050.00               17376.00                 2042.00   \n",
       "\n",
       "         Bwd Packet Length Mean   Bwd Packet Length Std  Flow Bytes/s  \\\n",
       "count                 539616.00               539616.00     539128.00   \n",
       "unique                      NaN                     NaN           NaN   \n",
       "top                         NaN                     NaN           NaN   \n",
       "freq                        NaN                     NaN           NaN   \n",
       "mean                     550.04                  697.31           inf   \n",
       "std                      869.70                 1255.31           NaN   \n",
       "min                        0.00                    0.00  -12000000.00   \n",
       "25%                        0.00                    0.00        102.04   \n",
       "50%                       54.00                    0.00       2263.83   \n",
       "75%                      824.50                  835.50     125000.00   \n",
       "max                     5800.00                 8190.00           inf   \n",
       "\n",
       "         Flow Packets/s   Flow IAT Mean   Flow IAT Std   Flow IAT Max  \\\n",
       "count         539616.00       539616.00      539616.00      539616.00   \n",
       "unique              NaN             NaN            NaN            NaN   \n",
       "top                 NaN             NaN            NaN            NaN   \n",
       "freq                NaN             NaN            NaN            NaN   \n",
       "mean                inf      2134902.45     5264534.55    17318403.50   \n",
       "std                 NaN      5824584.75    10551103.82    33957014.03   \n",
       "min         -2000000.00          -12.00           0.00         -12.00   \n",
       "25%                0.73           59.00           0.00          71.00   \n",
       "50%               81.92        17000.58       10944.70       42572.00   \n",
       "75%            28985.51      1734360.65     3073641.88     8020226.50   \n",
       "max                 inf    120000000.00    84700000.00   120000000.00   \n",
       "\n",
       "         Flow IAT Min  Fwd IAT Total   Fwd IAT Mean   Fwd IAT Std  \\\n",
       "count       539616.00      539616.00      539616.00     539616.00   \n",
       "unique            NaN            NaN            NaN           NaN   \n",
       "top               NaN            NaN            NaN           NaN   \n",
       "freq              NaN            NaN            NaN           NaN   \n",
       "mean        294558.36    20756340.18     4036797.30    6666492.26   \n",
       "std        4269410.12    38121061.80    10627587.74   13914086.55   \n",
       "min            -13.00           0.00           0.00          0.00   \n",
       "25%              3.00           0.00           0.00          0.00   \n",
       "50%              5.00         482.00         271.00          0.00   \n",
       "75%             54.00     8699965.50     2153087.25    2907944.03   \n",
       "max      120000000.00   120000000.00   120000000.00   83200000.00   \n",
       "\n",
       "         Fwd IAT Max   Fwd IAT Min  Bwd IAT Total   Bwd IAT Mean  \\\n",
       "count      539616.00     539616.00      539616.00      539616.00   \n",
       "unique           NaN           NaN            NaN            NaN   \n",
       "top              NaN           NaN            NaN            NaN   \n",
       "freq             NaN           NaN            NaN            NaN   \n",
       "mean     17200410.93    1035760.29     9601754.01     2028745.22   \n",
       "std      34021535.79    8949950.83    28072181.79     9045625.35   \n",
       "min             0.00        -12.00           0.00           0.00   \n",
       "25%             0.00          0.00           0.00           0.00   \n",
       "50%           439.00          3.00           3.00           3.00   \n",
       "75%       7341524.00         49.00      151874.25       33099.85   \n",
       "max     120000000.00  120000000.00   120000000.00   120000000.00   \n",
       "\n",
       "         Bwd IAT Std   Bwd IAT Max   Bwd IAT Min  Fwd PSH Flags  \\\n",
       "count      539616.00     539616.00     539616.00      539616.00   \n",
       "unique           NaN           NaN           NaN            NaN   \n",
       "top              NaN           NaN           NaN            NaN   \n",
       "freq             NaN           NaN           NaN            NaN   \n",
       "mean      2413091.18    6464188.15     814778.83           0.04   \n",
       "std       8975672.92   21921965.46    7698886.82           0.19   \n",
       "min             0.00          0.00          0.00           0.00   \n",
       "25%             0.00          0.00          0.00           0.00   \n",
       "50%             0.00          3.00          1.00           0.00   \n",
       "75%         58444.52     136467.50         46.00           0.00   \n",
       "max      81700000.00  120000000.00  120000000.00           1.00   \n",
       "\n",
       "         Bwd PSH Flags   Fwd URG Flags   Bwd URG Flags   Fwd Header Length  \\\n",
       "count        539616.00       539616.00       539616.00           539616.00   \n",
       "unique             NaN             NaN             NaN                 NaN   \n",
       "top                NaN             NaN             NaN                 NaN   \n",
       "freq               NaN             NaN             NaN                 NaN   \n",
       "mean              0.00            0.00            0.00            -4593.96   \n",
       "std               0.00            0.01            0.00          2095355.38   \n",
       "min               0.00            0.00            0.00      -1073741320.00   \n",
       "25%               0.00            0.00            0.00               40.00   \n",
       "50%               0.00            0.00            0.00               64.00   \n",
       "75%               0.00            0.00            0.00              164.00   \n",
       "max               0.00            1.00            0.00          4173072.00   \n",
       "\n",
       "         Bwd Header Length  Fwd Packets/s   Bwd Packets/s   Min Packet Length  \\\n",
       "count            539616.00      539616.00       539616.00           539616.00   \n",
       "unique                 NaN            NaN             NaN                 NaN   \n",
       "top                    NaN            NaN             NaN                 NaN   \n",
       "freq                   NaN            NaN             NaN                 NaN   \n",
       "mean              -4290.45       71421.49         7885.31                9.61   \n",
       "std             2076610.16      268528.74        38472.68               21.55   \n",
       "min         -1073741320.00           0.00            0.00                0.00   \n",
       "25%                  20.00           0.53            0.06                0.00   \n",
       "50%                  40.00          40.92            4.99                0.00   \n",
       "75%                 136.00       14705.88         8695.65                6.00   \n",
       "max             5413720.00     3000000.00      2000000.00             1330.00   \n",
       "\n",
       "         Max Packet Length   Packet Length Mean   Packet Length Std  \\\n",
       "count            539616.00            539616.00           539616.00   \n",
       "unique                 NaN                  NaN                 NaN   \n",
       "top                    NaN                  NaN                 NaN   \n",
       "freq                   NaN                  NaN                 NaN   \n",
       "mean               1723.70               276.58              544.09   \n",
       "std                2862.90               416.17              903.66   \n",
       "min                   0.00                 0.00                0.00   \n",
       "25%                   6.00                 3.33                2.19   \n",
       "50%                  71.00                50.80               18.86   \n",
       "75%                2896.00               484.57              812.50   \n",
       "max               24820.00              2160.00             4732.00   \n",
       "\n",
       "         Packet Length Variance  FIN Flag Count   SYN Flag Count  \\\n",
       "count                 539616.00       539616.00        539616.00   \n",
       "unique                      NaN             NaN              NaN   \n",
       "top                         NaN             NaN              NaN   \n",
       "freq                        NaN             NaN              NaN   \n",
       "mean                 1112687.62            0.06             0.04   \n",
       "std                  2468133.75            0.24             0.19   \n",
       "min                        0.00            0.00             0.00   \n",
       "25%                        4.80            0.00             0.00   \n",
       "50%                      355.95            0.00             0.00   \n",
       "75%                   659877.56            0.00             0.00   \n",
       "max                 22400000.00            1.00             1.00   \n",
       "\n",
       "         RST Flag Count   PSH Flag Count   ACK Flag Count   URG Flag Count  \\\n",
       "count         539616.00        539616.00        539616.00        539616.00   \n",
       "unique              NaN              NaN              NaN              NaN   \n",
       "top                 NaN              NaN              NaN              NaN   \n",
       "freq                NaN              NaN              NaN              NaN   \n",
       "mean               0.00             0.38             0.37             0.06   \n",
       "std                0.01             0.48             0.48             0.24   \n",
       "min                0.00             0.00             0.00             0.00   \n",
       "25%                0.00             0.00             0.00             0.00   \n",
       "50%                0.00             0.00             0.00             0.00   \n",
       "75%                0.00             1.00             1.00             0.00   \n",
       "max                1.00             1.00             1.00             1.00   \n",
       "\n",
       "         CWE Flag Count   ECE Flag Count   Down/Up Ratio  \\\n",
       "count         539616.00        539616.00       539616.00   \n",
       "unique              NaN              NaN             NaN   \n",
       "top                 NaN              NaN             NaN   \n",
       "freq                NaN              NaN             NaN   \n",
       "mean               0.00             0.00            0.64   \n",
       "std                0.01             0.01            0.63   \n",
       "min                0.00             0.00            0.00   \n",
       "25%                0.00             0.00            0.00   \n",
       "50%                0.00             0.00            1.00   \n",
       "75%                0.00             0.00            1.00   \n",
       "max                1.00             1.00           29.00   \n",
       "\n",
       "         Average Packet Size   Avg Fwd Segment Size   Avg Bwd Segment Size  \\\n",
       "count              539616.00              539616.00              539616.00   \n",
       "unique                   NaN                    NaN                    NaN   \n",
       "top                      NaN                    NaN                    NaN   \n",
       "freq                     NaN                    NaN                    NaN   \n",
       "mean                  305.36                  44.68                 550.04   \n",
       "std                   457.15                 150.87                 869.70   \n",
       "min                     0.00                   0.00                   0.00   \n",
       "25%                     5.00                   2.00                   0.00   \n",
       "50%                    64.50                  11.22                  54.00   \n",
       "75%                   523.00                  48.56                 824.50   \n",
       "max                  2508.00                5940.00                5800.00   \n",
       "\n",
       "         Fwd Header Length.1  Fwd Avg Bytes/Bulk   Fwd Avg Packets/Bulk  \\\n",
       "count              539616.00           539616.00              539616.00   \n",
       "unique                   NaN                 NaN                    NaN   \n",
       "top                      NaN                 NaN                    NaN   \n",
       "freq                     NaN                 NaN                    NaN   \n",
       "mean                -4593.96                0.00                   0.00   \n",
       "std               2095355.38                0.00                   0.00   \n",
       "min           -1073741320.00                0.00                   0.00   \n",
       "25%                    40.00                0.00                   0.00   \n",
       "50%                    64.00                0.00                   0.00   \n",
       "75%                   164.00                0.00                   0.00   \n",
       "max               4173072.00                0.00                   0.00   \n",
       "\n",
       "         Fwd Avg Bulk Rate   Bwd Avg Bytes/Bulk   Bwd Avg Packets/Bulk  \\\n",
       "count            539616.00            539616.00              539616.00   \n",
       "unique                 NaN                  NaN                    NaN   \n",
       "top                    NaN                  NaN                    NaN   \n",
       "freq                   NaN                  NaN                    NaN   \n",
       "mean                  0.00                 0.00                   0.00   \n",
       "std                   0.00                 0.00                   0.00   \n",
       "min                   0.00                 0.00                   0.00   \n",
       "25%                   0.00                 0.00                   0.00   \n",
       "50%                   0.00                 0.00                   0.00   \n",
       "75%                   0.00                 0.00                   0.00   \n",
       "max                   0.00                 0.00                   0.00   \n",
       "\n",
       "        Bwd Avg Bulk Rate  Subflow Fwd Packets   Subflow Fwd Bytes  \\\n",
       "count           539616.00            539616.00           539616.00   \n",
       "unique                NaN                  NaN                 NaN   \n",
       "top                   NaN                  NaN                 NaN   \n",
       "freq                  NaN                  NaN                 NaN   \n",
       "mean                 0.00                 6.12              408.24   \n",
       "std                  0.00               345.11             7478.35   \n",
       "min                  0.00                 1.00                0.00   \n",
       "25%                  0.00                 1.00                2.00   \n",
       "50%                  0.00                 2.00               44.00   \n",
       "75%                  0.00                 6.00              320.00   \n",
       "max                  0.00            200755.00          2866110.00   \n",
       "\n",
       "         Subflow Bwd Packets   Subflow Bwd Bytes  Init_Win_bytes_forward  \\\n",
       "count              539616.00           539616.00               539616.00   \n",
       "unique                   NaN                 NaN                     NaN   \n",
       "top                      NaN                 NaN                     NaN   \n",
       "freq                     NaN                 NaN                     NaN   \n",
       "mean                    5.99             7998.77                 7564.76   \n",
       "std                   463.49          1013814.39                13440.65   \n",
       "min                     0.00                0.00                   -1.00   \n",
       "25%                     1.00                0.00                    0.00   \n",
       "50%                     2.00              105.00                  274.00   \n",
       "75%                     5.00             8216.25                 8192.00   \n",
       "max                270686.00        590596141.00                65535.00   \n",
       "\n",
       "         Init_Win_bytes_backward   act_data_pkt_fwd   min_seg_size_forward  \\\n",
       "count                  539616.00          539616.00              539616.00   \n",
       "unique                       NaN                NaN                    NaN   \n",
       "top                          NaN                NaN                    NaN   \n",
       "freq                         NaN                NaN                    NaN   \n",
       "mean                     1277.94               2.95               -2429.31   \n",
       "std                      6590.25             267.51             1052328.64   \n",
       "min                        -1.00               0.00          -536870660.00   \n",
       "25%                        -1.00               0.00                  20.00   \n",
       "50%                         0.00               1.00                  24.00   \n",
       "75%                       235.00               2.00                  32.00   \n",
       "max                     65535.00          192491.00                 138.00   \n",
       "\n",
       "        Active Mean   Active Std   Active Max   Active Min    Idle Mean  \\\n",
       "count     539616.00    539616.00    539616.00    539616.00    539616.00   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      115498.42     40375.70    167288.24     89830.17  16101546.63   \n",
       "std       778752.95    434703.20   1059625.81    709971.01  33154633.35   \n",
       "min            0.00         0.00         0.00         0.00         0.00   \n",
       "25%            0.00         0.00         0.00         0.00         0.00   \n",
       "50%            0.00         0.00         0.00         0.00         0.00   \n",
       "75%            6.00         0.00         6.00         5.00   7125040.75   \n",
       "max    102000000.00  63500000.00 102000000.00 102000000.00 120000000.00   \n",
       "\n",
       "          Idle Std     Idle Max     Idle Min   Label  \n",
       "count    539616.00    539616.00    539616.00  539616  \n",
       "unique         NaN          NaN          NaN      15  \n",
       "top            NaN          NaN          NaN  BENIGN  \n",
       "freq           NaN          NaN          NaN  240000  \n",
       "mean     958564.89  16872266.93  15391495.53     NaN  \n",
       "std     6461275.27  34038514.45  32956945.19     NaN  \n",
       "min           0.00         0.00         0.00     NaN  \n",
       "25%           0.00         0.00         0.00     NaN  \n",
       "50%           0.00         0.00         0.00     NaN  \n",
       "75%           0.00   7143036.50   6028677.00     NaN  \n",
       "max    76900000.00 120000000.00 120000000.00     NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = []\n",
    "for column in df.columns:\n",
    "    new_col_names.append(column.strip().replace(' ', '_').lower())\n",
    "\n",
    "df.columns = new_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA –ø—Ä–æ–≤–µ–¥–µ–º —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ SweetViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = sv.analyze(df)\n",
    "#report.show_html('common analysis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## destination_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Ä—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è - –ø–æ—Ä—Ç –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –ø–∞–∫–µ—Ç. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   539616.00\n",
       "mean      5563.06\n",
       "std      14921.26\n",
       "min          0.00\n",
       "25%         80.00\n",
       "50%         80.00\n",
       "75%        443.00\n",
       "max      65533.00\n",
       "Name: destination_port, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['destination_port'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –ø–æ—Ä—Ç 0 –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏ –∫ –Ω–µ–º—É –Ω–µ–ª—å–∑—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è. –û–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–∞–∫–µ—Ç –Ω–∞ –ø–æ—Ä—Ç 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destination_port\n",
       "80      229165\n",
       "53      101315\n",
       "443      53430\n",
       "21        8638\n",
       "22        7245\n",
       "123       2412\n",
       "8080      1488\n",
       "137        810\n",
       "389        760\n",
       "88         630\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['destination_port'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{158, 1346, 6037, 8067, 204285}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.query('label != \"BENIGN\"')['destination_port'].value_counts().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ã—á–Ω—ã–µ –ø–æ—Ä—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ —Å–µ—Ç—è—Ö. –í–æ–∑–º–æ–∂–Ω–æ —á–∞—Å—Ç–æ—Ç–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∏–Ω—Ç—É–∏—Ç–≤–Ω–æ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–∞–∫–æ–π –≤—ã—Å–æ–∫–æ–π, –Ω–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å–µ—Ç–∏ –º—ã –Ω–µ –∑–Ω–∞–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑ —Ç–æ–ø –ø–æ—Ä—Ç–æ–≤ –¥–ª—è –∞—Ç–∞–∫ –≤—ã–ø–∞–¥–∞–µ—Ç 53 –ø–æ—Ä—Ç, –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å DNS-—Å–µ—Ä–≤–µ—Ä–∞–º–∏. –¢–∞–∫–∂–µ –≤ —Ç–æ–ø –ø–æ–ø–∞–¥–∞—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23, 139, 444, 5432, 6779}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.query('label != \"BENIGN\"')['destination_port'].value_counts().head(10).index) - \\\n",
    "set(df['destination_port'].value_counts().head(10).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 - Telnet (–ø—Ä–æ—Ç–æ–∫–æ–ª —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞)\\\n",
    "139 - –ü–æ—Ä—Ç 139 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—â–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º –∏ –ø—Ä–∏–Ω—Ç–µ—Ä–∞–º, —ç—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–∏–±–æ–ª–µ–µ –æ–ø–∞—Å–Ω—ã–π –ø–æ—Ä—Ç –ø–æ—Ç–æ–º—É, —á—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –∂–µ—Å—Ç–∫–∏–π –¥–∏—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ–∑–∞—â–∏—â–µ–Ω–Ω—ã–º\\\n",
    "444 - SNMP –ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ –≤ IP-—Å–µ—Ç—è—Ö(–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä—ã, –∫–æ–º–º—É—Ç–∞—Ç–æ—Ä—ã, —Å–µ—Ä–≤–µ—Ä—ã, —Ä–∞–±–æ—á–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏, –ø—Ä–∏–Ω—Ç–µ—Ä—ã, –º–æ–¥–µ–º–Ω—ã–µ —Å—Ç–æ–π–∫–∏ –∏ –¥—Ä—É–≥–∏–µ)\\\n",
    "5432 - PostgreSQL database\\\n",
    "6779 - –æ–±—ã—á–Ω—ã–π tcp/udp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Ä—Ç –±—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['destination_port', 'flow_duration', 'total_fwd_packets',\n",
       "       'total_backward_packets', 'total_length_of_fwd_packets',\n",
       "       'total_length_of_bwd_packets', 'fwd_packet_length_max',\n",
       "       'fwd_packet_length_min', 'fwd_packet_length_mean',\n",
       "       'fwd_packet_length_std', 'bwd_packet_length_max',\n",
       "       'bwd_packet_length_min', 'bwd_packet_length_mean',\n",
       "       'bwd_packet_length_std', 'flow_bytes/s', 'flow_packets/s',\n",
       "       'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n",
       "       'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max',\n",
       "       'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std',\n",
       "       'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags',\n",
       "       'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length',\n",
       "       'bwd_header_length', 'fwd_packets/s', 'bwd_packets/s',\n",
       "       'min_packet_length', 'max_packet_length', 'packet_length_mean',\n",
       "       'packet_length_std', 'packet_length_variance', 'fin_flag_count',\n",
       "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
       "       'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down/up_ratio',\n",
       "       'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size',\n",
       "       'fwd_header_length.1', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk',\n",
       "       'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk',\n",
       "       'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes',\n",
       "       'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward',\n",
       "       'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean',\n",
       "       'idle_std', 'idle_max', 'idle_min', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤:**\\\n",
    "`destination_port` - –ü–æ—Ä—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–π –ø–æ—Ä—Ç, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –ø–∞–∫–µ—Ç—ã –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.\\\n",
    "`flow_duration` - –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Ç–æ–∫–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç –≤—Ä–µ–º—è –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç—Ä–∞—Ç–∏–ª–æ—Å—å –Ω–∞ –ø–µ—Ä–µ–¥–∞—á—É –ø–æ—Ç–æ–∫–∞.\\\n",
    "`total_fwd_packets` - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ - —ç—Ç–æ —Å—É–º–º–∞ –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`total_backward_packets` - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ - —ç—Ç–æ —Å—É–º–º–∞ –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`total_length_of_fwd_packets` - –û–±—â–∞—è –¥–ª–∏–Ω–∞ –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—â—É—é –¥–ª–∏–Ω—É –≤ –±–∞–π—Ç–∞—Ö –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`total_length_of_bwd_packets` - –û–±—â–∞—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—â—É—é –¥–ª–∏–Ω—É –≤ –±–∞–π—Ç–∞—Ö –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`fwd_packet_length_max` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ –Ω–∞–∏–±–æ–ª—å—à–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_packet_length_min` - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ –Ω–∞–∏–º–µ–Ω—å—à–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_packet_length_mean` - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª–∏–Ω –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_packet_length_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ –º–µ—Ä–∞ —Ä–∞–∑–±—Ä–æ—Å–∞ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª–∏–Ω –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`bwd_packet_length_max` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ –Ω–∞–∏–±–æ–ª—å—à–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_packet_length_min` - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ –Ω–∞–∏–º–µ–Ω—å—à–∞—è –¥–ª–∏–Ω–∞ –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_packet_length_mean` - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª–∏–Ω –≤ –±–∞–π—Ç–∞—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_packet_length_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ä–∞–∑–±—Ä–æ—Å –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª–∏–Ω –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`flow_bytes/s` - –ë–∞–π—Ç—ã –ø–æ—Ç–æ–∫–∞ –≤ —Å–µ–∫—É–Ω–¥—É –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∫–æ–ª—å–∫–æ –±–∞–π—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–µ—Ç–∏ –∑–∞ –æ–¥–Ω—É —Å–µ–∫—É–Ω–¥—É.\\\n",
    "`flow_packets/s` - –ü–∞–∫–µ—Ç—ã –ø–æ—Ç–æ–∫–∞ –≤ —Å–µ–∫—É–Ω–¥—É –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∫–æ–ª—å–∫–æ –ø–∞–∫–µ—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–µ—Ç–∏ –∑–∞ –æ–¥–Ω—É —Å–µ–∫—É–Ω–¥—É.\\\n",
    "`flow_iat_mean` - –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ—Ç–æ–∫–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ –ø–µ—Ä–µ–¥–∞—á—É –ø–∞–∫–µ—Ç–æ–≤ –ø–æ—Ç–æ–∫–∞.\\\n",
    "`flow_iat_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ—Ç–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ä–æ–π —Ä–∞–∑–±—Ä–æ—Å–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ—Ç–æ–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`flow_iat_max` - –ù–∞–∏–±–æ–ª—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ—Ç–æ–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –ø—Ä–∏–±—ã—Ç–∏–µ–º –ø–∞–∫–µ—Ç–æ–≤ –ø–æ—Ç–æ–∫–∞.\\\n",
    "`flow_iat_min` - –ù–∞–∏–º–µ–Ω—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ—Ç–æ–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –ø—Ä–∏–±—ã—Ç–∏–µ–º –ø–∞–∫–µ—Ç–æ–≤ –ø–æ—Ç–æ–∫–∞.\\\n",
    "`fwd_iat_total`- –û–±—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç —Å—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_iat_mean` - –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_iat_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ä–æ–π —Ä–∞–∑–±—Ä–æ—Å–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`fwd_iat_max` - –ù–∞–∏–±–æ–ª—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_iat_min` - –ù–∞–∏–º–µ–Ω—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä—è–º—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_iat_total` - –û–±—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç —Å—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_iat_mean` - –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_iat_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ä–æ–π —Ä–∞–∑–±—Ä–æ—Å–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`bwd_iat_max` - –ù–∞–∏–±–æ–ª—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`bwd_iat_min` - –ù–∞–∏–º–µ–Ω—å—à–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–±—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–æ–π –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤.\\\n",
    "`fwd_psh_flags` - –§–ª–∞–≥–∏ Push –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø–∞–∫–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–∏–µ–º–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É –±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏.\\\n",
    "`bwd_psh_flags` - –§–ª–∞–≥–∏ Push –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø–∞–∫–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Ç–ø—Ä–∞–≤–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É –±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏.\\\n",
    "`fwd_urg_flags` - –§–ª–∞–≥–∏ Urgent –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø–∞–∫–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏ —Ç—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.\\\n",
    "`bwd_urg_flags` - –§–ª–∞–≥–∏ Urgent –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø–∞–∫–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏ —Ç—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.\\\n",
    "`fwd_header_length` - –î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`bwd_header_length` - –î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`fwd_packets/s` - –ü—Ä—è–º—ã–µ –ø–∞–∫–µ—Ç—ã –≤ —Å–µ–∫—É–Ω–¥—É –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∫–æ–ª—å–∫–æ –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–µ—Ç–∏ –∑–∞ –æ–¥–Ω—É —Å–µ–∫—É–Ω–¥—É.\\\n",
    "`bwd_packets/s` - –û–±—Ä–∞—Ç–Ω—ã–µ –ø–∞–∫–µ—Ç—ã –≤ —Å–µ–∫—É–Ω–¥—É –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∫–æ–ª—å–∫–æ –æ–±—Ä–∞—Ç–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–µ—Ç–∏ –∑–∞ –æ–¥–Ω—É —Å–µ–∫—É–Ω–¥—É.\\\n",
    "`min_packet_length` - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞–∫–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞–∏–º–µ–Ω—å—à—É—é –¥–ª–∏–Ω—É –ø–∞–∫–µ—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`max_packet_length` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞–∫–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞–∏–±–æ–ª—å—à—É—é –¥–ª–∏–Ω—É –ø–∞–∫–µ—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`packet_length_mean` - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–∞–∫–µ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª–∏–Ω –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`packet_length_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–∞–∫–µ—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –¥–ª–∏–Ω –≤—Å–µ—Ö –ø–∞–∫–µ—Ç–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`packet_length_variance` - –î–∏—Å–ø–µ—Ä—Å–∏—è –¥–ª–∏–Ω—ã –ø–∞–∫–µ—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ä–æ–π —Ä–∞–∑–±—Ä–æ—Å–∞ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª–∏–Ω –ø–∞–∫–µ—Ç–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`fin_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º FIN —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ FIN.\\\n",
    "`syn_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º SYN —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ SYN.\\\n",
    "`rst_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º RST —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ RST.\\\n",
    "`psh_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º PSH —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ PSH.\\\n",
    "`ack_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º ACK —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ ACK.\\\n",
    "`urg_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º URG —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ URG.\\\n",
    "`cwe_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º CWE —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ CWE.\\\n",
    "`ece_flag_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ —Å —Ñ–ª–∞–≥–æ–º ECE —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ ECE.\\\n",
    "`down/up_ratio` - –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ \"—Å–∫–∞—á–∞—Ç—å/–æ—Ç–ø—Ä–∞–≤–∏—Ç—å\" –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞–∫–µ—Ç–æ–≤ –≤ —Å—Ç–æ—Ä–æ–Ω—É –∑–∞–≥—Ä—É–∑–∫–∏ (—Å–∫–∞—á–∏–≤–∞–Ω–∏—è) –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–∞–∫–µ—Ç–æ–≤ –≤ —Å—Ç–æ—Ä–æ–Ω—É –æ—Ç–ø—Ä–∞–≤–∫–∏.\\\n",
    "`average_packet_size` - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø–∞–∫–µ—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`avg_fwd_segment_size` - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø—Ä—è–º–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø—Ä—è–º–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`avg_bwd_segment_size` - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`fwd_header_length.1` - –î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö.\\\n",
    "`fwd_avg_bytes/bulk` - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç –≤ –ø–∞–∫–µ—Ç–µ –ø—Ä—è–º–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç –≤ –∫–∞–∂–¥–æ–º –ø–∞–∫–µ—Ç–µ –ø—Ä—è–º–æ–≥–æ –ø–æ—Ç–æ–∫–∞.\\\n",
    "`fwd_avg_packets/bulk` - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –ø—Ä—è–º–æ–º –ø–æ—Ç–æ–∫–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –ø—Ä—è–º–æ–º –ø–æ—Ç–æ–∫–µ.\\\n",
    "`fwd_avg_bulk_rate` - –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –±–ª–æ–∫–æ–≤ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ä–µ–¥–Ω—é—é —Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –±–ª–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`bwd_avg_bytes/bulk` - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç –≤ –±–ª–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –±–ª–æ–∫–µ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`bwd_avg_packets/bulk` - –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –±–ª–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –±–ª–æ–∫–µ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`bwd_avg_bulk_rate` - –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –±–ª–æ–∫–æ–≤ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ä–µ–¥–Ω—é—é —Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –±–ª–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`subflow_fwd_packets` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`subflow_fwd_bytes` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç–æ–≤ –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`subflow_bwd_packets` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤ –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`subflow_bwd_bytes` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç–æ–≤ –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–π—Ç–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –ø–æ–¥–ø–æ—Ç–æ–∫–µ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`init_win_bytes_forward` - –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–∏–µ–º–∞ –≤ –±–∞–π—Ç–∞—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–∏–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–π—Ç–∞—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`init_win_bytes_backward` - –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–∏–µ–º–∞ –≤ –±–∞–π—Ç–∞—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–∏–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–π—Ç–∞—Ö –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`act_data_pkt_fwd` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä—è–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö.\\\n",
    "`min_seg_size_forward` - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä—è–º–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\\\n",
    "`active_mean` - –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏.\\\n",
    "`active_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ç–µ–ø–µ–Ω—å –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`active_max` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏.\\\n",
    "`active_min` - –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏.\\\n",
    "`idle_mean` - –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏.\\\n",
    "`idle_std` - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ç–µ–ø–µ–Ω—å –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\\\n",
    "`idle_max` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏.\\\n",
    "`idle_min` - –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flow_duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      539616.00\n",
       "mean     21066929.37\n",
       "std      38121719.63\n",
       "min           -12.00\n",
       "25%            73.00\n",
       "50%         49739.00\n",
       "75%      10640295.25\n",
       "max     119999993.00\n",
       "Name: flow_duration, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['flow_duration'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏, –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>bwd_packet_length_max</th>\n",
       "      <th>bwd_packet_length_min</th>\n",
       "      <th>bwd_packet_length_mean</th>\n",
       "      <th>bwd_packet_length_std</th>\n",
       "      <th>flow_bytes/s</th>\n",
       "      <th>flow_packets/s</th>\n",
       "      <th>flow_iat_mean</th>\n",
       "      <th>flow_iat_std</th>\n",
       "      <th>flow_iat_max</th>\n",
       "      <th>flow_iat_min</th>\n",
       "      <th>fwd_iat_total</th>\n",
       "      <th>fwd_iat_mean</th>\n",
       "      <th>fwd_iat_std</th>\n",
       "      <th>fwd_iat_max</th>\n",
       "      <th>fwd_iat_min</th>\n",
       "      <th>bwd_iat_total</th>\n",
       "      <th>bwd_iat_mean</th>\n",
       "      <th>bwd_iat_std</th>\n",
       "      <th>bwd_iat_max</th>\n",
       "      <th>bwd_iat_min</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets/s</th>\n",
       "      <th>bwd_packets/s</th>\n",
       "      <th>min_packet_length</th>\n",
       "      <th>max_packet_length</th>\n",
       "      <th>packet_length_mean</th>\n",
       "      <th>packet_length_std</th>\n",
       "      <th>packet_length_variance</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down/up_ratio</th>\n",
       "      <th>average_packet_size</th>\n",
       "      <th>avg_fwd_segment_size</th>\n",
       "      <th>avg_bwd_segment_size</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>fwd_avg_bytes/bulk</th>\n",
       "      <th>fwd_avg_packets/bulk</th>\n",
       "      <th>fwd_avg_bulk_rate</th>\n",
       "      <th>bwd_avg_bytes/bulk</th>\n",
       "      <th>bwd_avg_packets/bulk</th>\n",
       "      <th>bwd_avg_bulk_rate</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>5480074</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1826691.40</td>\n",
       "      <td>3131700.20</td>\n",
       "      <td>5442804</td>\n",
       "      <td>101</td>\n",
       "      <td>5480074</td>\n",
       "      <td>2740037.00</td>\n",
       "      <td>3822289.80</td>\n",
       "      <td>5442804</td>\n",
       "      <td>37270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>32</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.29</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8192</td>\n",
       "      <td>42780</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443</td>\n",
       "      <td>711977</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>703</td>\n",
       "      <td>3950</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>78.10</td>\n",
       "      <td>103.30</td>\n",
       "      <td>1448</td>\n",
       "      <td>0</td>\n",
       "      <td>395.00</td>\n",
       "      <td>587.50</td>\n",
       "      <td>6535.32</td>\n",
       "      <td>26.69</td>\n",
       "      <td>39554.28</td>\n",
       "      <td>50154.62</td>\n",
       "      <td>120501</td>\n",
       "      <td>1</td>\n",
       "      <td>616301</td>\n",
       "      <td>77037.62</td>\n",
       "      <td>72995.98</td>\n",
       "      <td>215614</td>\n",
       "      <td>230</td>\n",
       "      <td>616874</td>\n",
       "      <td>68541.55</td>\n",
       "      <td>71985.97</td>\n",
       "      <td>199836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "      <td>328</td>\n",
       "      <td>12.64</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0</td>\n",
       "      <td>1448</td>\n",
       "      <td>232.60</td>\n",
       "      <td>442.80</td>\n",
       "      <td>196012.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>244.90</td>\n",
       "      <td>78.10</td>\n",
       "      <td>395.00</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>703</td>\n",
       "      <td>10</td>\n",
       "      <td>3950</td>\n",
       "      <td>29200</td>\n",
       "      <td>252</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8446</td>\n",
       "      <td>767</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>14.34</td>\n",
       "      <td>14.44</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63885.27</td>\n",
       "      <td>5215.12</td>\n",
       "      <td>255.67</td>\n",
       "      <td>394.29</td>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>713</td>\n",
       "      <td>356.50</td>\n",
       "      <td>499.92</td>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>3911.34</td>\n",
       "      <td>1303.78</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.18</td>\n",
       "      <td>125.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.75</td>\n",
       "      <td>14.34</td>\n",
       "      <td>6.00</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1017</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>443</td>\n",
       "      <td>65786373</td>\n",
       "      <td>122</td>\n",
       "      <td>191</td>\n",
       "      <td>23544</td>\n",
       "      <td>275883</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>193.00</td>\n",
       "      <td>366.20</td>\n",
       "      <td>4380</td>\n",
       "      <td>0</td>\n",
       "      <td>1444.00</td>\n",
       "      <td>689.00</td>\n",
       "      <td>4551.50</td>\n",
       "      <td>4.76</td>\n",
       "      <td>210853.77</td>\n",
       "      <td>2919396.00</td>\n",
       "      <td>51185854</td>\n",
       "      <td>1</td>\n",
       "      <td>65786373</td>\n",
       "      <td>543689.06</td>\n",
       "      <td>4680394.50</td>\n",
       "      <td>51185854</td>\n",
       "      <td>1</td>\n",
       "      <td>14585289</td>\n",
       "      <td>76764.68</td>\n",
       "      <td>484025.84</td>\n",
       "      <td>4827482</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2464</td>\n",
       "      <td>3832</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0</td>\n",
       "      <td>4380</td>\n",
       "      <td>953.50</td>\n",
       "      <td>846.00</td>\n",
       "      <td>715368.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>956.50</td>\n",
       "      <td>193.00</td>\n",
       "      <td>1444.00</td>\n",
       "      <td>2464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>23544</td>\n",
       "      <td>191</td>\n",
       "      <td>275883</td>\n",
       "      <td>65535</td>\n",
       "      <td>75</td>\n",
       "      <td>121</td>\n",
       "      <td>20</td>\n",
       "      <td>14600467.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14600467</td>\n",
       "      <td>14600467</td>\n",
       "      <td>51185856.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51185854</td>\n",
       "      <td>51185854</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1877</td>\n",
       "      <td>213</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>14.34</td>\n",
       "      <td>14.44</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>230046.95</td>\n",
       "      <td>18779.34</td>\n",
       "      <td>71.00</td>\n",
       "      <td>80.58</td>\n",
       "      <td>164</td>\n",
       "      <td>22</td>\n",
       "      <td>213</td>\n",
       "      <td>106.50</td>\n",
       "      <td>81.32</td>\n",
       "      <td>164</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>14084.51</td>\n",
       "      <td>4694.84</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.18</td>\n",
       "      <td>125.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.75</td>\n",
       "      <td>14.34</td>\n",
       "      <td>6.00</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1311</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539611</th>\n",
       "      <td>80</td>\n",
       "      <td>11512204</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>326</td>\n",
       "      <td>11632</td>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "      <td>40.75</td>\n",
       "      <td>115.25</td>\n",
       "      <td>10184</td>\n",
       "      <td>0</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>4436.00</td>\n",
       "      <td>1038.72</td>\n",
       "      <td>1.13</td>\n",
       "      <td>959350.30</td>\n",
       "      <td>2262615.80</td>\n",
       "      <td>6507197</td>\n",
       "      <td>1</td>\n",
       "      <td>6510770</td>\n",
       "      <td>930110.00</td>\n",
       "      <td>2460477.20</td>\n",
       "      <td>6509948</td>\n",
       "      <td>1</td>\n",
       "      <td>11500000</td>\n",
       "      <td>2878039.50</td>\n",
       "      <td>3378874.20</td>\n",
       "      <td>6507197</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>168</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>10184</td>\n",
       "      <td>854.00</td>\n",
       "      <td>2714.00</td>\n",
       "      <td>7361769.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>920.00</td>\n",
       "      <td>40.75</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>326</td>\n",
       "      <td>5</td>\n",
       "      <td>11632</td>\n",
       "      <td>29200</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>892.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>892</td>\n",
       "      <td>892</td>\n",
       "      <td>6507197.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6507197</td>\n",
       "      <td>6507197</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539612</th>\n",
       "      <td>80</td>\n",
       "      <td>11513325</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>471</td>\n",
       "      <td>3525</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>94.20</td>\n",
       "      <td>210.60</td>\n",
       "      <td>2077</td>\n",
       "      <td>0</td>\n",
       "      <td>705.00</td>\n",
       "      <td>990.50</td>\n",
       "      <td>347.08</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1279258.40</td>\n",
       "      <td>2565152.80</td>\n",
       "      <td>6508582</td>\n",
       "      <td>53</td>\n",
       "      <td>6512631</td>\n",
       "      <td>1628157.80</td>\n",
       "      <td>3255638.80</td>\n",
       "      <td>6511616</td>\n",
       "      <td>176</td>\n",
       "      <td>11500000</td>\n",
       "      <td>2878318.00</td>\n",
       "      <td>3379306.00</td>\n",
       "      <td>6508582</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2077</td>\n",
       "      <td>363.20</td>\n",
       "      <td>720.00</td>\n",
       "      <td>518083.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>399.50</td>\n",
       "      <td>94.20</td>\n",
       "      <td>705.00</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>471</td>\n",
       "      <td>5</td>\n",
       "      <td>3525</td>\n",
       "      <td>29200</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>918.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>918</td>\n",
       "      <td>918</td>\n",
       "      <td>6508582.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6508582</td>\n",
       "      <td>6508582</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539613</th>\n",
       "      <td>80</td>\n",
       "      <td>11509201</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>314</td>\n",
       "      <td>11632</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>44.84</td>\n",
       "      <td>118.70</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>1939.00</td>\n",
       "      <td>2544.00</td>\n",
       "      <td>1037.95</td>\n",
       "      <td>1.13</td>\n",
       "      <td>959100.06</td>\n",
       "      <td>2261842.00</td>\n",
       "      <td>6503248</td>\n",
       "      <td>46</td>\n",
       "      <td>6507056</td>\n",
       "      <td>1084509.40</td>\n",
       "      <td>2655701.20</td>\n",
       "      <td>6505437</td>\n",
       "      <td>255</td>\n",
       "      <td>11500000</td>\n",
       "      <td>2301830.50</td>\n",
       "      <td>3195689.20</td>\n",
       "      <td>6503248</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>200</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>5792</td>\n",
       "      <td>853.50</td>\n",
       "      <td>1857.00</td>\n",
       "      <td>3449144.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>919.00</td>\n",
       "      <td>44.84</td>\n",
       "      <td>1939.00</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>314</td>\n",
       "      <td>6</td>\n",
       "      <td>11632</td>\n",
       "      <td>29200</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>899.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>899</td>\n",
       "      <td>899</td>\n",
       "      <td>6503248.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6503248</td>\n",
       "      <td>6503248</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539614</th>\n",
       "      <td>80</td>\n",
       "      <td>11509095</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>369</td>\n",
       "      <td>11632</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>46.12</td>\n",
       "      <td>130.50</td>\n",
       "      <td>10184</td>\n",
       "      <td>0</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>4436.00</td>\n",
       "      <td>1042.74</td>\n",
       "      <td>1.13</td>\n",
       "      <td>959091.25</td>\n",
       "      <td>2262122.20</td>\n",
       "      <td>6504954</td>\n",
       "      <td>47</td>\n",
       "      <td>6507794</td>\n",
       "      <td>929684.90</td>\n",
       "      <td>2458891.20</td>\n",
       "      <td>6505926</td>\n",
       "      <td>168</td>\n",
       "      <td>11500000</td>\n",
       "      <td>2877262.00</td>\n",
       "      <td>3377879.00</td>\n",
       "      <td>6504954</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>168</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>10184</td>\n",
       "      <td>857.00</td>\n",
       "      <td>2712.00</td>\n",
       "      <td>7358407.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>923.00</td>\n",
       "      <td>46.12</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>369</td>\n",
       "      <td>5</td>\n",
       "      <td>11632</td>\n",
       "      <td>29200</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>914.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>6504954.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6504954</td>\n",
       "      <td>6504954</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539615</th>\n",
       "      <td>80</td>\n",
       "      <td>11512230</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>672</td>\n",
       "      <td>3525</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>134.40</td>\n",
       "      <td>300.50</td>\n",
       "      <td>2077</td>\n",
       "      <td>0</td>\n",
       "      <td>705.00</td>\n",
       "      <td>990.50</td>\n",
       "      <td>364.57</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1279136.60</td>\n",
       "      <td>2565059.80</td>\n",
       "      <td>6506213</td>\n",
       "      <td>4</td>\n",
       "      <td>6509115</td>\n",
       "      <td>1627278.80</td>\n",
       "      <td>3253892.20</td>\n",
       "      <td>6508117</td>\n",
       "      <td>4</td>\n",
       "      <td>11500000</td>\n",
       "      <td>2878045.50</td>\n",
       "      <td>3378842.00</td>\n",
       "      <td>6506213</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2077</td>\n",
       "      <td>381.50</td>\n",
       "      <td>725.50</td>\n",
       "      <td>526087.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>419.80</td>\n",
       "      <td>134.40</td>\n",
       "      <td>705.00</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>672</td>\n",
       "      <td>5</td>\n",
       "      <td>3525</td>\n",
       "      <td>29200</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>821.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>821</td>\n",
       "      <td>821</td>\n",
       "      <td>6506213.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6506213</td>\n",
       "      <td>6506213</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333847 rows √ó 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "0                     80        5480074                  3   \n",
       "1                    443         711977                  9   \n",
       "4                   8446            767                  3   \n",
       "7                    443       65786373                122   \n",
       "8                   1877            213                  3   \n",
       "...                  ...            ...                ...   \n",
       "539611                80       11512204                  8   \n",
       "539612                80       11513325                  5   \n",
       "539613                80       11509201                  7   \n",
       "539614                80       11509095                  8   \n",
       "539615                80       11512230                  5   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                            1                           12   \n",
       "1                           10                          703   \n",
       "4                            1                           43   \n",
       "7                          191                        23544   \n",
       "8                            1                           43   \n",
       "...                        ...                          ...   \n",
       "539611                       5                          326   \n",
       "539612                       5                          471   \n",
       "539613                       6                          314   \n",
       "539614                       5                          369   \n",
       "539615                       5                          672   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "0                                 0                      6   \n",
       "1                              3950                    267   \n",
       "4                                 6                     31   \n",
       "7                            275883                   1093   \n",
       "8                                 6                     31   \n",
       "...                             ...                    ...   \n",
       "539611                        11632                    326   \n",
       "539612                         3525                    471   \n",
       "539613                        11632                    314   \n",
       "539614                        11632                    369   \n",
       "539615                         3525                    672   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "0                           0                    4.00                   3.46   \n",
       "1                           0                   78.10                 103.30   \n",
       "4                           6                   14.34                  14.44   \n",
       "7                           0                  193.00                 366.20   \n",
       "8                           6                   14.34                  14.44   \n",
       "...                       ...                     ...                    ...   \n",
       "539611                      0                   40.75                 115.25   \n",
       "539612                      0                   94.20                 210.60   \n",
       "539613                      0                   44.84                 118.70   \n",
       "539614                      0                   46.12                 130.50   \n",
       "539615                      0                  134.40                 300.50   \n",
       "\n",
       "        bwd_packet_length_max  bwd_packet_length_min  bwd_packet_length_mean  \\\n",
       "0                           0                      0                    0.00   \n",
       "1                        1448                      0                  395.00   \n",
       "4                           6                      6                    6.00   \n",
       "7                        4380                      0                 1444.00   \n",
       "8                           6                      6                    6.00   \n",
       "...                       ...                    ...                     ...   \n",
       "539611                  10184                      0                 2326.00   \n",
       "539612                   2077                      0                  705.00   \n",
       "539613                   5792                      0                 1939.00   \n",
       "539614                  10184                      0                 2326.00   \n",
       "539615                   2077                      0                  705.00   \n",
       "\n",
       "        bwd_packet_length_std  flow_bytes/s  flow_packets/s  flow_iat_mean  \\\n",
       "0                        0.00          2.19            0.73     1826691.40   \n",
       "1                      587.50       6535.32           26.69       39554.28   \n",
       "4                        0.00      63885.27         5215.12         255.67   \n",
       "7                      689.00       4551.50            4.76      210853.77   \n",
       "8                        0.00     230046.95        18779.34          71.00   \n",
       "...                       ...           ...             ...            ...   \n",
       "539611                4436.00       1038.72            1.13      959350.30   \n",
       "539612                 990.50        347.08            0.87     1279258.40   \n",
       "539613                2544.00       1037.95            1.13      959100.06   \n",
       "539614                4436.00       1042.74            1.13      959091.25   \n",
       "539615                 990.50        364.57            0.87     1279136.60   \n",
       "\n",
       "        flow_iat_std  flow_iat_max  flow_iat_min  fwd_iat_total  fwd_iat_mean  \\\n",
       "0         3131700.20       5442804           101        5480074    2740037.00   \n",
       "1           50154.62        120501             1         616301      77037.62   \n",
       "4             394.29           710             3            713        356.50   \n",
       "7         2919396.00      51185854             1       65786373     543689.06   \n",
       "8              80.58           164            22            213        106.50   \n",
       "...              ...           ...           ...            ...           ...   \n",
       "539611    2262615.80       6507197             1        6510770     930110.00   \n",
       "539612    2565152.80       6508582            53        6512631    1628157.80   \n",
       "539613    2261842.00       6503248            46        6507056    1084509.40   \n",
       "539614    2262122.20       6504954            47        6507794     929684.90   \n",
       "539615    2565059.80       6506213             4        6509115    1627278.80   \n",
       "\n",
       "        fwd_iat_std  fwd_iat_max  fwd_iat_min  bwd_iat_total  bwd_iat_mean  \\\n",
       "0        3822289.80      5442804        37270              0          0.00   \n",
       "1          72995.98       215614          230         616874      68541.55   \n",
       "4            499.92          710            3              0          0.00   \n",
       "7        4680394.50     51185854            1       14585289      76764.68   \n",
       "8             81.32          164           49              0          0.00   \n",
       "...             ...          ...          ...            ...           ...   \n",
       "539611   2460477.20      6509948            1       11500000    2878039.50   \n",
       "539612   3255638.80      6511616          176       11500000    2878318.00   \n",
       "539613   2655701.20      6505437          255       11500000    2301830.50   \n",
       "539614   2458891.20      6505926          168       11500000    2877262.00   \n",
       "539615   3253892.20      6508117            4       11500000    2878045.50   \n",
       "\n",
       "        bwd_iat_std  bwd_iat_max  bwd_iat_min  fwd_psh_flags  bwd_psh_flags  \\\n",
       "0              0.00            0            0              0              0   \n",
       "1          71985.97       199836            1              0              0   \n",
       "4              0.00            0            0              0              0   \n",
       "7         484025.84      4827482            1              0              0   \n",
       "8              0.00            0            0              0              0   \n",
       "...             ...          ...          ...            ...            ...   \n",
       "539611   3378874.20      6507197          196              0              0   \n",
       "539612   3379306.00      6508582          192              0              0   \n",
       "539613   3195689.20      6503248           47              0              0   \n",
       "539614   3377879.00      6504954          196              0              0   \n",
       "539615   3378842.00      6506213           49              0              0   \n",
       "\n",
       "        fwd_urg_flags  bwd_urg_flags  fwd_header_length  bwd_header_length  \\\n",
       "0                   0              0                 72                 32   \n",
       "1                   0              0                296                328   \n",
       "4                   0              0                 60                 20   \n",
       "7                   0              0               2464               3832   \n",
       "8                   0              0                 60                 20   \n",
       "...               ...            ...                ...                ...   \n",
       "539611              0              0                264                168   \n",
       "539612              0              0                168                168   \n",
       "539613              0              0                232                200   \n",
       "539614              0              0                264                168   \n",
       "539615              0              0                168                168   \n",
       "\n",
       "        fwd_packets/s  bwd_packets/s  min_packet_length  max_packet_length  \\\n",
       "0                0.55           0.18                  0                  6   \n",
       "1               12.64          14.05                  0               1448   \n",
       "4             3911.34        1303.78                  6                 31   \n",
       "7                1.85           2.90                  0               4380   \n",
       "8            14084.51        4694.84                  6                 31   \n",
       "...               ...            ...                ...                ...   \n",
       "539611           0.69           0.43                  0              10184   \n",
       "539612           0.43           0.43                  0               2077   \n",
       "539613           0.61           0.52                  0               5792   \n",
       "539614           0.70           0.43                  0              10184   \n",
       "539615           0.43           0.43                  0               2077   \n",
       "\n",
       "        packet_length_mean  packet_length_std  packet_length_variance  \\\n",
       "0                     2.40               3.29                   10.80   \n",
       "1                   232.60             442.80               196012.66   \n",
       "4                    11.00              11.18                  125.00   \n",
       "7                   953.50             846.00               715368.30   \n",
       "8                    11.00              11.18                  125.00   \n",
       "...                    ...                ...                     ...   \n",
       "539611              854.00            2714.00              7361769.00   \n",
       "539612              363.20             720.00               518083.62   \n",
       "539613              853.50            1857.00              3449144.20   \n",
       "539614              857.00            2712.00              7358407.00   \n",
       "539615              381.50             725.50               526087.06   \n",
       "\n",
       "        fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "0                    0               0               0               1   \n",
       "1                    0               0               0               1   \n",
       "4                    0               0               0               0   \n",
       "7                    0               0               0               1   \n",
       "8                    0               0               0               0   \n",
       "...                ...             ...             ...             ...   \n",
       "539611               0               0               0               1   \n",
       "539612               0               0               0               1   \n",
       "539613               0               0               0               1   \n",
       "539614               0               0               0               1   \n",
       "539615               0               0               0               1   \n",
       "\n",
       "        ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "0                    0               0               0               0   \n",
       "1                    0               0               0               0   \n",
       "4                    1               0               0               0   \n",
       "7                    0               0               0               0   \n",
       "8                    1               0               0               0   \n",
       "...                ...             ...             ...             ...   \n",
       "539611               0               0               0               0   \n",
       "539612               0               0               0               0   \n",
       "539613               0               0               0               0   \n",
       "539614               0               0               0               0   \n",
       "539615               0               0               0               0   \n",
       "\n",
       "        down/up_ratio  average_packet_size  avg_fwd_segment_size  \\\n",
       "0                   0                 3.00                  4.00   \n",
       "1                   1               244.90                 78.10   \n",
       "4                   0                13.75                 14.34   \n",
       "7                   1               956.50                193.00   \n",
       "8                   0                13.75                 14.34   \n",
       "...               ...                  ...                   ...   \n",
       "539611              0               920.00                 40.75   \n",
       "539612              1               399.50                 94.20   \n",
       "539613              0               919.00                 44.84   \n",
       "539614              0               923.00                 46.12   \n",
       "539615              1               419.80                134.40   \n",
       "\n",
       "        avg_bwd_segment_size  fwd_header_length.1  fwd_avg_bytes/bulk  \\\n",
       "0                       0.00                   72                   0   \n",
       "1                     395.00                  296                   0   \n",
       "4                       6.00                   60                   0   \n",
       "7                    1444.00                 2464                   0   \n",
       "8                       6.00                   60                   0   \n",
       "...                      ...                  ...                 ...   \n",
       "539611               2326.00                  264                   0   \n",
       "539612                705.00                  168                   0   \n",
       "539613               1939.00                  232                   0   \n",
       "539614               2326.00                  264                   0   \n",
       "539615                705.00                  168                   0   \n",
       "\n",
       "        fwd_avg_packets/bulk  fwd_avg_bulk_rate  bwd_avg_bytes/bulk  \\\n",
       "0                          0                  0                   0   \n",
       "1                          0                  0                   0   \n",
       "4                          0                  0                   0   \n",
       "7                          0                  0                   0   \n",
       "8                          0                  0                   0   \n",
       "...                      ...                ...                 ...   \n",
       "539611                     0                  0                   0   \n",
       "539612                     0                  0                   0   \n",
       "539613                     0                  0                   0   \n",
       "539614                     0                  0                   0   \n",
       "539615                     0                  0                   0   \n",
       "\n",
       "        bwd_avg_packets/bulk  bwd_avg_bulk_rate  subflow_fwd_packets  \\\n",
       "0                          0                  0                    3   \n",
       "1                          0                  0                    9   \n",
       "4                          0                  0                    3   \n",
       "7                          0                  0                  122   \n",
       "8                          0                  0                    3   \n",
       "...                      ...                ...                  ...   \n",
       "539611                     0                  0                    8   \n",
       "539612                     0                  0                    5   \n",
       "539613                     0                  0                    7   \n",
       "539614                     0                  0                    8   \n",
       "539615                     0                  0                    5   \n",
       "\n",
       "        subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "0                      12                    1                  0   \n",
       "1                     703                   10               3950   \n",
       "4                      43                    1                  6   \n",
       "7                   23544                  191             275883   \n",
       "8                      43                    1                  6   \n",
       "...                   ...                  ...                ...   \n",
       "539611                326                    5              11632   \n",
       "539612                471                    5               3525   \n",
       "539613                314                    6              11632   \n",
       "539614                369                    5              11632   \n",
       "539615                672                    5               3525   \n",
       "\n",
       "        init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd  \\\n",
       "0                         8192                    42780                 2   \n",
       "1                        29200                      252                 4   \n",
       "4                         1017                        0                 2   \n",
       "7                        65535                       75               121   \n",
       "8                         1311                        0                 2   \n",
       "...                        ...                      ...               ...   \n",
       "539611                   29200                      235                 1   \n",
       "539612                   29200                      235                 1   \n",
       "539613                   29200                      235                 1   \n",
       "539614                   29200                      235                 1   \n",
       "539615                   29200                      237                 1   \n",
       "\n",
       "        min_seg_size_forward  active_mean  active_std  active_max  active_min  \\\n",
       "0                         20         0.00        0.00           0           0   \n",
       "1                         32         0.00        0.00           0           0   \n",
       "4                         20         0.00        0.00           0           0   \n",
       "7                         20  14600467.00        0.00    14600467    14600467   \n",
       "8                         20         0.00        0.00           0           0   \n",
       "...                      ...          ...         ...         ...         ...   \n",
       "539611                    32       892.00        0.00         892         892   \n",
       "539612                    32       918.00        0.00         918         918   \n",
       "539613                    32       899.00        0.00         899         899   \n",
       "539614                    32       914.00        0.00         914         914   \n",
       "539615                    32       821.00        0.00         821         821   \n",
       "\n",
       "         idle_mean  idle_std  idle_max  idle_min          label  \n",
       "0             0.00      0.00         0         0         BENIGN  \n",
       "1             0.00      0.00         0         0         BENIGN  \n",
       "4             0.00      0.00         0         0         BENIGN  \n",
       "7      51185856.00      0.00  51185854  51185854         BENIGN  \n",
       "8             0.00      0.00         0         0         BENIGN  \n",
       "...            ...       ...       ...       ...            ...  \n",
       "539611  6507197.00      0.00   6507197   6507197  DoS GoldenEye  \n",
       "539612  6508582.00      0.00   6508582   6508582  DoS GoldenEye  \n",
       "539613  6503248.00      0.00   6503248   6503248  DoS GoldenEye  \n",
       "539614  6504954.00      0.00   6504954   6504954  DoS GoldenEye  \n",
       "539615  6506213.00      0.00   6506213   6506213  DoS GoldenEye  \n",
       "\n",
       "[333847 rows x 79 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.select_dtypes(include=[np.number]).ge(0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>bwd_packet_length_max</th>\n",
       "      <th>bwd_packet_length_min</th>\n",
       "      <th>bwd_packet_length_mean</th>\n",
       "      <th>bwd_packet_length_std</th>\n",
       "      <th>flow_bytes/s</th>\n",
       "      <th>flow_packets/s</th>\n",
       "      <th>flow_iat_mean</th>\n",
       "      <th>flow_iat_std</th>\n",
       "      <th>flow_iat_max</th>\n",
       "      <th>flow_iat_min</th>\n",
       "      <th>fwd_iat_total</th>\n",
       "      <th>fwd_iat_mean</th>\n",
       "      <th>fwd_iat_std</th>\n",
       "      <th>fwd_iat_max</th>\n",
       "      <th>fwd_iat_min</th>\n",
       "      <th>bwd_iat_total</th>\n",
       "      <th>bwd_iat_mean</th>\n",
       "      <th>bwd_iat_std</th>\n",
       "      <th>bwd_iat_max</th>\n",
       "      <th>bwd_iat_min</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets/s</th>\n",
       "      <th>bwd_packets/s</th>\n",
       "      <th>min_packet_length</th>\n",
       "      <th>max_packet_length</th>\n",
       "      <th>packet_length_mean</th>\n",
       "      <th>packet_length_std</th>\n",
       "      <th>packet_length_variance</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down/up_ratio</th>\n",
       "      <th>average_packet_size</th>\n",
       "      <th>avg_fwd_segment_size</th>\n",
       "      <th>avg_bwd_segment_size</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>fwd_avg_bytes/bulk</th>\n",
       "      <th>fwd_avg_packets/bulk</th>\n",
       "      <th>fwd_avg_bulk_rate</th>\n",
       "      <th>bwd_avg_bytes/bulk</th>\n",
       "      <th>bwd_avg_packets/bulk</th>\n",
       "      <th>bwd_avg_bulk_rate</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6139</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>63846</td>\n",
       "      <td>51100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>253</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26940</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63662</td>\n",
       "      <td>31320</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33846</th>\n",
       "      <td>39070</td>\n",
       "      <td>-12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-666666.67</td>\n",
       "      <td>-166666.67</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12</td>\n",
       "      <td>-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.31</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35352</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>253</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35392</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>258</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35743</th>\n",
       "      <td>41008</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-6000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.46</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29200</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45839</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54158</th>\n",
       "      <td>55092</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7877</td>\n",
       "      <td>63808</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65651</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>63286</td>\n",
       "      <td>8190</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98335</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110434</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119215</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134665</th>\n",
       "      <td>51838</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7013</td>\n",
       "      <td>63808</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138183</th>\n",
       "      <td>38112</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-6000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.46</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29200</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158827</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175760</th>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>253</td>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176584</th>\n",
       "      <td>443</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>251</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188689</th>\n",
       "      <td>53290</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12000000.00</td>\n",
       "      <td>-2000000.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "6139                 443             -1                  1   \n",
       "10236                 80             -1                  1   \n",
       "26940                443             -1                  1   \n",
       "33846              39070            -12                  1   \n",
       "35352                443             -1                  1   \n",
       "35392                 80             -1                  1   \n",
       "35743              41008             -1                  1   \n",
       "45839                443             -1                  1   \n",
       "54158              55092             -1                  1   \n",
       "65651                 80             -1                  1   \n",
       "98335                443             -1                  1   \n",
       "110434                80             -1                  1   \n",
       "119215                80             -1                  1   \n",
       "134665             51838             -1                  1   \n",
       "138183             38112             -1                  1   \n",
       "158827               443             -1                  1   \n",
       "175760                80             -1                  1   \n",
       "176584               443             -1                  1   \n",
       "188689             53290             -1                  1   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "6139                         1                            6   \n",
       "10236                        1                            6   \n",
       "26940                        1                            0   \n",
       "33846                        1                            6   \n",
       "35352                        1                            6   \n",
       "35392                        1                            6   \n",
       "35743                        1                            6   \n",
       "45839                        1                            6   \n",
       "54158                        1                            6   \n",
       "65651                        1                            6   \n",
       "98335                        1                            0   \n",
       "110434                       1                            0   \n",
       "119215                       1                            6   \n",
       "134665                       1                            6   \n",
       "138183                       1                            6   \n",
       "158827                       1                            0   \n",
       "175760                       1                            6   \n",
       "176584                       1                            6   \n",
       "188689                       1                            6   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "6139                              6                      6   \n",
       "10236                             6                      6   \n",
       "26940                             0                      0   \n",
       "33846                             2                      6   \n",
       "35352                             6                      6   \n",
       "35392                             6                      6   \n",
       "35743                             0                      6   \n",
       "45839                             6                      6   \n",
       "54158                             6                      6   \n",
       "65651                             6                      6   \n",
       "98335                             0                      0   \n",
       "110434                            0                      0   \n",
       "119215                            6                      6   \n",
       "134665                            6                      6   \n",
       "138183                            0                      6   \n",
       "158827                            0                      0   \n",
       "175760                            6                      6   \n",
       "176584                            6                      6   \n",
       "188689                            6                      6   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "6139                        6                    6.00                   0.00   \n",
       "10236                       6                    6.00                   0.00   \n",
       "26940                       0                    0.00                   0.00   \n",
       "33846                       6                    6.00                   0.00   \n",
       "35352                       6                    6.00                   0.00   \n",
       "35392                       6                    6.00                   0.00   \n",
       "35743                       6                    6.00                   0.00   \n",
       "45839                       6                    6.00                   0.00   \n",
       "54158                       6                    6.00                   0.00   \n",
       "65651                       6                    6.00                   0.00   \n",
       "98335                       0                    0.00                   0.00   \n",
       "110434                      0                    0.00                   0.00   \n",
       "119215                      6                    6.00                   0.00   \n",
       "134665                      6                    6.00                   0.00   \n",
       "138183                      6                    6.00                   0.00   \n",
       "158827                      0                    0.00                   0.00   \n",
       "175760                      6                    6.00                   0.00   \n",
       "176584                      6                    6.00                   0.00   \n",
       "188689                      6                    6.00                   0.00   \n",
       "\n",
       "        bwd_packet_length_max  bwd_packet_length_min  bwd_packet_length_mean  \\\n",
       "6139                        6                      6                    6.00   \n",
       "10236                       6                      6                    6.00   \n",
       "26940                       0                      0                    0.00   \n",
       "33846                       2                      2                    2.00   \n",
       "35352                       6                      6                    6.00   \n",
       "35392                       6                      6                    6.00   \n",
       "35743                       0                      0                    0.00   \n",
       "45839                       6                      6                    6.00   \n",
       "54158                       6                      6                    6.00   \n",
       "65651                       6                      6                    6.00   \n",
       "98335                       0                      0                    0.00   \n",
       "110434                      0                      0                    0.00   \n",
       "119215                      6                      6                    6.00   \n",
       "134665                      6                      6                    6.00   \n",
       "138183                      0                      0                    0.00   \n",
       "158827                      0                      0                    0.00   \n",
       "175760                      6                      6                    6.00   \n",
       "176584                      6                      6                    6.00   \n",
       "188689                      6                      6                    6.00   \n",
       "\n",
       "        bwd_packet_length_std  flow_bytes/s  flow_packets/s  flow_iat_mean  \\\n",
       "6139                     0.00  -12000000.00     -2000000.00          -1.00   \n",
       "10236                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "26940                    0.00          0.00     -2000000.00          -1.00   \n",
       "33846                    0.00    -666666.67      -166666.67         -12.00   \n",
       "35352                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "35392                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "35743                    0.00   -6000000.00     -2000000.00          -1.00   \n",
       "45839                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "54158                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "65651                    0.00  -12000000.00     -2000000.00          -1.00   \n",
       "98335                    0.00          0.00     -2000000.00          -1.00   \n",
       "110434                   0.00          0.00     -2000000.00          -1.00   \n",
       "119215                   0.00  -12000000.00     -2000000.00          -1.00   \n",
       "134665                   0.00  -12000000.00     -2000000.00          -1.00   \n",
       "138183                   0.00   -6000000.00     -2000000.00          -1.00   \n",
       "158827                   0.00          0.00     -2000000.00          -1.00   \n",
       "175760                   0.00  -12000000.00     -2000000.00          -1.00   \n",
       "176584                   0.00  -12000000.00     -2000000.00          -1.00   \n",
       "188689                   0.00  -12000000.00     -2000000.00          -1.00   \n",
       "\n",
       "        flow_iat_std  flow_iat_max  flow_iat_min  fwd_iat_total  fwd_iat_mean  \\\n",
       "6139            0.00            -1            -1              0          0.00   \n",
       "10236           0.00            -1            -1              0          0.00   \n",
       "26940           0.00            -1            -1              0          0.00   \n",
       "33846           0.00           -12           -12              0          0.00   \n",
       "35352           0.00            -1            -1              0          0.00   \n",
       "35392           0.00            -1            -1              0          0.00   \n",
       "35743           0.00            -1            -1              0          0.00   \n",
       "45839           0.00            -1            -1              0          0.00   \n",
       "54158           0.00            -1            -1              0          0.00   \n",
       "65651           0.00            -1            -1              0          0.00   \n",
       "98335           0.00            -1            -1              0          0.00   \n",
       "110434          0.00            -1            -1              0          0.00   \n",
       "119215          0.00            -1            -1              0          0.00   \n",
       "134665          0.00            -1            -1              0          0.00   \n",
       "138183          0.00            -1            -1              0          0.00   \n",
       "158827          0.00            -1            -1              0          0.00   \n",
       "175760          0.00            -1            -1              0          0.00   \n",
       "176584          0.00            -1            -1              0          0.00   \n",
       "188689          0.00            -1            -1              0          0.00   \n",
       "\n",
       "        fwd_iat_std  fwd_iat_max  fwd_iat_min  bwd_iat_total  bwd_iat_mean  \\\n",
       "6139           0.00            0            0              0          0.00   \n",
       "10236          0.00            0            0              0          0.00   \n",
       "26940          0.00            0            0              0          0.00   \n",
       "33846          0.00            0            0              0          0.00   \n",
       "35352          0.00            0            0              0          0.00   \n",
       "35392          0.00            0            0              0          0.00   \n",
       "35743          0.00            0            0              0          0.00   \n",
       "45839          0.00            0            0              0          0.00   \n",
       "54158          0.00            0            0              0          0.00   \n",
       "65651          0.00            0            0              0          0.00   \n",
       "98335          0.00            0            0              0          0.00   \n",
       "110434         0.00            0            0              0          0.00   \n",
       "119215         0.00            0            0              0          0.00   \n",
       "134665         0.00            0            0              0          0.00   \n",
       "138183         0.00            0            0              0          0.00   \n",
       "158827         0.00            0            0              0          0.00   \n",
       "175760         0.00            0            0              0          0.00   \n",
       "176584         0.00            0            0              0          0.00   \n",
       "188689         0.00            0            0              0          0.00   \n",
       "\n",
       "        bwd_iat_std  bwd_iat_max  bwd_iat_min  fwd_psh_flags  bwd_psh_flags  \\\n",
       "6139           0.00            0            0              0              0   \n",
       "10236          0.00            0            0              0              0   \n",
       "26940          0.00            0            0              0              0   \n",
       "33846          0.00            0            0              0              0   \n",
       "35352          0.00            0            0              0              0   \n",
       "35392          0.00            0            0              0              0   \n",
       "35743          0.00            0            0              0              0   \n",
       "45839          0.00            0            0              0              0   \n",
       "54158          0.00            0            0              0              0   \n",
       "65651          0.00            0            0              0              0   \n",
       "98335          0.00            0            0              0              0   \n",
       "110434         0.00            0            0              0              0   \n",
       "119215         0.00            0            0              0              0   \n",
       "134665         0.00            0            0              0              0   \n",
       "138183         0.00            0            0              0              0   \n",
       "158827         0.00            0            0              0              0   \n",
       "175760         0.00            0            0              0              0   \n",
       "176584         0.00            0            0              0              0   \n",
       "188689         0.00            0            0              0              0   \n",
       "\n",
       "        fwd_urg_flags  bwd_urg_flags  fwd_header_length  bwd_header_length  \\\n",
       "6139                0              0                 20                 20   \n",
       "10236               0              0                 20                 20   \n",
       "26940               0              0                 20                 20   \n",
       "33846               0              0                 20                 24   \n",
       "35352               0              0                 20                 20   \n",
       "35392               0              0                 20                 20   \n",
       "35743               0              0                 20                 40   \n",
       "45839               0              0                 20                 20   \n",
       "54158               0              0                 20                 20   \n",
       "65651               0              0                 20                 20   \n",
       "98335               0              0                 32                 32   \n",
       "110434              0              0                 32                 32   \n",
       "119215              0              0                 20                 20   \n",
       "134665              0              0                 20                 20   \n",
       "138183              0              0                 20                 40   \n",
       "158827              0              0                 20                 20   \n",
       "175760              0              0                 20                 20   \n",
       "176584              0              0                 20                 20   \n",
       "188689              0              0                 20                 20   \n",
       "\n",
       "        fwd_packets/s  bwd_packets/s  min_packet_length  max_packet_length  \\\n",
       "6139             0.00           0.00                  6                  6   \n",
       "10236            0.00           0.00                  6                  6   \n",
       "26940            0.00           0.00                  0                  0   \n",
       "33846            0.00           0.00                  2                  6   \n",
       "35352            0.00           0.00                  6                  6   \n",
       "35392            0.00           0.00                  6                  6   \n",
       "35743            0.00           0.00                  0                  6   \n",
       "45839            0.00           0.00                  6                  6   \n",
       "54158            0.00           0.00                  6                  6   \n",
       "65651            0.00           0.00                  6                  6   \n",
       "98335            0.00           0.00                  0                  0   \n",
       "110434           0.00           0.00                  0                  0   \n",
       "119215           0.00           0.00                  6                  6   \n",
       "134665           0.00           0.00                  6                  6   \n",
       "138183           0.00           0.00                  0                  6   \n",
       "158827           0.00           0.00                  0                  0   \n",
       "175760           0.00           0.00                  6                  6   \n",
       "176584           0.00           0.00                  6                  6   \n",
       "188689           0.00           0.00                  6                  6   \n",
       "\n",
       "        packet_length_mean  packet_length_std  packet_length_variance  \\\n",
       "6139                  6.00               0.00                    0.00   \n",
       "10236                 6.00               0.00                    0.00   \n",
       "26940                 0.00               0.00                    0.00   \n",
       "33846                 4.67               2.31                    5.33   \n",
       "35352                 6.00               0.00                    0.00   \n",
       "35392                 6.00               0.00                    0.00   \n",
       "35743                 4.00               3.46                   12.00   \n",
       "45839                 6.00               0.00                    0.00   \n",
       "54158                 6.00               0.00                    0.00   \n",
       "65651                 6.00               0.00                    0.00   \n",
       "98335                 0.00               0.00                    0.00   \n",
       "110434                0.00               0.00                    0.00   \n",
       "119215                6.00               0.00                    0.00   \n",
       "134665                6.00               0.00                    0.00   \n",
       "138183                4.00               3.46                   12.00   \n",
       "158827                0.00               0.00                    0.00   \n",
       "175760                6.00               0.00                    0.00   \n",
       "176584                6.00               0.00                    0.00   \n",
       "188689                6.00               0.00                    0.00   \n",
       "\n",
       "        fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "6139                 0               0               0               0   \n",
       "10236                0               0               0               0   \n",
       "26940                0               0               0               0   \n",
       "33846                1               0               0               0   \n",
       "35352                0               0               0               0   \n",
       "35392                0               0               0               0   \n",
       "35743                1               0               0               0   \n",
       "45839                0               0               0               0   \n",
       "54158                0               0               0               0   \n",
       "65651                0               0               0               0   \n",
       "98335                0               0               0               0   \n",
       "110434               0               0               0               0   \n",
       "119215               0               0               0               0   \n",
       "134665               0               0               0               0   \n",
       "138183               1               0               0               0   \n",
       "158827               0               0               0               0   \n",
       "175760               0               0               0               0   \n",
       "176584               0               0               0               0   \n",
       "188689               0               0               0               0   \n",
       "\n",
       "        ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "6139                 1               0               0               0   \n",
       "10236                1               0               0               0   \n",
       "26940                1               0               0               0   \n",
       "33846                1               0               0               0   \n",
       "35352                1               0               0               0   \n",
       "35392                1               0               0               0   \n",
       "35743                1               0               0               0   \n",
       "45839                1               0               0               0   \n",
       "54158                1               0               0               0   \n",
       "65651                1               0               0               0   \n",
       "98335                1               0               0               0   \n",
       "110434               1               0               0               0   \n",
       "119215               1               0               0               0   \n",
       "134665               1               0               0               0   \n",
       "138183               1               0               0               0   \n",
       "158827               1               0               0               0   \n",
       "175760               1               0               0               0   \n",
       "176584               1               0               0               0   \n",
       "188689               1               0               0               0   \n",
       "\n",
       "        down/up_ratio  average_packet_size  avg_fwd_segment_size  \\\n",
       "6139                1                 9.00                  6.00   \n",
       "10236               1                 9.00                  6.00   \n",
       "26940               1                 0.00                  0.00   \n",
       "33846               1                 7.00                  6.00   \n",
       "35352               1                 9.00                  6.00   \n",
       "35392               1                 9.00                  6.00   \n",
       "35743               1                 6.00                  6.00   \n",
       "45839               1                 9.00                  6.00   \n",
       "54158               1                 9.00                  6.00   \n",
       "65651               1                 9.00                  6.00   \n",
       "98335               1                 0.00                  0.00   \n",
       "110434              1                 0.00                  0.00   \n",
       "119215              1                 9.00                  6.00   \n",
       "134665              1                 9.00                  6.00   \n",
       "138183              1                 6.00                  6.00   \n",
       "158827              1                 0.00                  0.00   \n",
       "175760              1                 9.00                  6.00   \n",
       "176584              1                 9.00                  6.00   \n",
       "188689              1                 9.00                  6.00   \n",
       "\n",
       "        avg_bwd_segment_size  fwd_header_length.1  fwd_avg_bytes/bulk  \\\n",
       "6139                    6.00                   20                   0   \n",
       "10236                   6.00                   20                   0   \n",
       "26940                   0.00                   20                   0   \n",
       "33846                   2.00                   20                   0   \n",
       "35352                   6.00                   20                   0   \n",
       "35392                   6.00                   20                   0   \n",
       "35743                   0.00                   20                   0   \n",
       "45839                   6.00                   20                   0   \n",
       "54158                   6.00                   20                   0   \n",
       "65651                   6.00                   20                   0   \n",
       "98335                   0.00                   32                   0   \n",
       "110434                  0.00                   32                   0   \n",
       "119215                  6.00                   20                   0   \n",
       "134665                  6.00                   20                   0   \n",
       "138183                  0.00                   20                   0   \n",
       "158827                  0.00                   20                   0   \n",
       "175760                  6.00                   20                   0   \n",
       "176584                  6.00                   20                   0   \n",
       "188689                  6.00                   20                   0   \n",
       "\n",
       "        fwd_avg_packets/bulk  fwd_avg_bulk_rate  bwd_avg_bytes/bulk  \\\n",
       "6139                       0                  0                   0   \n",
       "10236                      0                  0                   0   \n",
       "26940                      0                  0                   0   \n",
       "33846                      0                  0                   0   \n",
       "35352                      0                  0                   0   \n",
       "35392                      0                  0                   0   \n",
       "35743                      0                  0                   0   \n",
       "45839                      0                  0                   0   \n",
       "54158                      0                  0                   0   \n",
       "65651                      0                  0                   0   \n",
       "98335                      0                  0                   0   \n",
       "110434                     0                  0                   0   \n",
       "119215                     0                  0                   0   \n",
       "134665                     0                  0                   0   \n",
       "138183                     0                  0                   0   \n",
       "158827                     0                  0                   0   \n",
       "175760                     0                  0                   0   \n",
       "176584                     0                  0                   0   \n",
       "188689                     0                  0                   0   \n",
       "\n",
       "        bwd_avg_packets/bulk  bwd_avg_bulk_rate  subflow_fwd_packets  \\\n",
       "6139                       0                  0                    1   \n",
       "10236                      0                  0                    1   \n",
       "26940                      0                  0                    1   \n",
       "33846                      0                  0                    1   \n",
       "35352                      0                  0                    1   \n",
       "35392                      0                  0                    1   \n",
       "35743                      0                  0                    1   \n",
       "45839                      0                  0                    1   \n",
       "54158                      0                  0                    1   \n",
       "65651                      0                  0                    1   \n",
       "98335                      0                  0                    1   \n",
       "110434                     0                  0                    1   \n",
       "119215                     0                  0                    1   \n",
       "134665                     0                  0                    1   \n",
       "138183                     0                  0                    1   \n",
       "158827                     0                  0                    1   \n",
       "175760                     0                  0                    1   \n",
       "176584                     0                  0                    1   \n",
       "188689                     0                  0                    1   \n",
       "\n",
       "        subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "6139                    6                    1                  6   \n",
       "10236                   6                    1                  6   \n",
       "26940                   0                    1                  0   \n",
       "33846                   6                    1                  2   \n",
       "35352                   6                    1                  6   \n",
       "35392                   6                    1                  6   \n",
       "35743                   6                    1                  0   \n",
       "45839                   6                    1                  6   \n",
       "54158                   6                    1                  6   \n",
       "65651                   6                    1                  6   \n",
       "98335                   0                    1                  0   \n",
       "110434                  0                    1                  0   \n",
       "119215                  6                    1                  6   \n",
       "134665                  6                    1                  6   \n",
       "138183                  6                    1                  0   \n",
       "158827                  0                    1                  0   \n",
       "175760                  6                    1                  6   \n",
       "176584                  6                    1                  6   \n",
       "188689                  6                    1                  6   \n",
       "\n",
       "        init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd  \\\n",
       "6139                     63846                    51100                 0   \n",
       "10236                      253                      288                 0   \n",
       "26940                    63662                    31320                 0   \n",
       "33846                        0                     1024                 0   \n",
       "35352                      253                       60                 0   \n",
       "35392                      258                      335                 0   \n",
       "35743                        0                    29200                 0   \n",
       "45839                      256                      124                 0   \n",
       "54158                     7877                    63808                 0   \n",
       "65651                    63286                     8190                 0   \n",
       "98335                      305                      118                 0   \n",
       "110434                     242                       31                 0   \n",
       "119215                     256                      229                 0   \n",
       "134665                    7013                    63808                 0   \n",
       "138183                       0                    29200                 0   \n",
       "158827                     251                      119                 0   \n",
       "175760                     253                      946                 0   \n",
       "176584                     251                      114                 0   \n",
       "188689                     351                        0                 0   \n",
       "\n",
       "        min_seg_size_forward  active_mean  active_std  active_max  active_min  \\\n",
       "6139                      20         0.00        0.00           0           0   \n",
       "10236                     20         0.00        0.00           0           0   \n",
       "26940                     20         0.00        0.00           0           0   \n",
       "33846                     20         0.00        0.00           0           0   \n",
       "35352                     20         0.00        0.00           0           0   \n",
       "35392                     20         0.00        0.00           0           0   \n",
       "35743                     20         0.00        0.00           0           0   \n",
       "45839                     20         0.00        0.00           0           0   \n",
       "54158                     20         0.00        0.00           0           0   \n",
       "65651                     20         0.00        0.00           0           0   \n",
       "98335                     32         0.00        0.00           0           0   \n",
       "110434                    32         0.00        0.00           0           0   \n",
       "119215                    20         0.00        0.00           0           0   \n",
       "134665                    20         0.00        0.00           0           0   \n",
       "138183                    20         0.00        0.00           0           0   \n",
       "158827                    20         0.00        0.00           0           0   \n",
       "175760                    20         0.00        0.00           0           0   \n",
       "176584                    20         0.00        0.00           0           0   \n",
       "188689                    20         0.00        0.00           0           0   \n",
       "\n",
       "        idle_mean  idle_std  idle_max  idle_min   label  \n",
       "6139         0.00      0.00         0         0  BENIGN  \n",
       "10236        0.00      0.00         0         0  BENIGN  \n",
       "26940        0.00      0.00         0         0  BENIGN  \n",
       "33846        0.00      0.00         0         0  BENIGN  \n",
       "35352        0.00      0.00         0         0  BENIGN  \n",
       "35392        0.00      0.00         0         0  BENIGN  \n",
       "35743        0.00      0.00         0         0  BENIGN  \n",
       "45839        0.00      0.00         0         0  BENIGN  \n",
       "54158        0.00      0.00         0         0  BENIGN  \n",
       "65651        0.00      0.00         0         0  BENIGN  \n",
       "98335        0.00      0.00         0         0  BENIGN  \n",
       "110434       0.00      0.00         0         0  BENIGN  \n",
       "119215       0.00      0.00         0         0  BENIGN  \n",
       "134665       0.00      0.00         0         0  BENIGN  \n",
       "138183       0.00      0.00         0         0  BENIGN  \n",
       "158827       0.00      0.00         0         0  BENIGN  \n",
       "175760       0.00      0.00         0         0  BENIGN  \n",
       "176584       0.00      0.00         0         0  BENIGN  \n",
       "188689       0.00      0.00         0         0  BENIGN  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['flow_duration'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>bwd_packet_length_max</th>\n",
       "      <th>bwd_packet_length_min</th>\n",
       "      <th>bwd_packet_length_mean</th>\n",
       "      <th>bwd_packet_length_std</th>\n",
       "      <th>flow_bytes/s</th>\n",
       "      <th>flow_packets/s</th>\n",
       "      <th>flow_iat_mean</th>\n",
       "      <th>flow_iat_std</th>\n",
       "      <th>flow_iat_max</th>\n",
       "      <th>flow_iat_min</th>\n",
       "      <th>fwd_iat_total</th>\n",
       "      <th>fwd_iat_mean</th>\n",
       "      <th>fwd_iat_std</th>\n",
       "      <th>fwd_iat_max</th>\n",
       "      <th>fwd_iat_min</th>\n",
       "      <th>bwd_iat_total</th>\n",
       "      <th>bwd_iat_mean</th>\n",
       "      <th>bwd_iat_std</th>\n",
       "      <th>bwd_iat_max</th>\n",
       "      <th>bwd_iat_min</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets/s</th>\n",
       "      <th>bwd_packets/s</th>\n",
       "      <th>min_packet_length</th>\n",
       "      <th>max_packet_length</th>\n",
       "      <th>packet_length_mean</th>\n",
       "      <th>packet_length_std</th>\n",
       "      <th>packet_length_variance</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down/up_ratio</th>\n",
       "      <th>average_packet_size</th>\n",
       "      <th>avg_fwd_segment_size</th>\n",
       "      <th>avg_bwd_segment_size</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>fwd_avg_bytes/bulk</th>\n",
       "      <th>fwd_avg_packets/bulk</th>\n",
       "      <th>fwd_avg_bulk_rate</th>\n",
       "      <th>bwd_avg_bytes/bulk</th>\n",
       "      <th>bwd_avg_packets/bulk</th>\n",
       "      <th>bwd_avg_bulk_rate</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>443</td>\n",
       "      <td>115705212</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>857</td>\n",
       "      <td>5619</td>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "      <td>40.80</td>\n",
       "      <td>95.75</td>\n",
       "      <td>1460</td>\n",
       "      <td>0</td>\n",
       "      <td>295.80</td>\n",
       "      <td>542.00</td>\n",
       "      <td>55.97</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2966800.20</td>\n",
       "      <td>4537612.00</td>\n",
       "      <td>10000000</td>\n",
       "      <td>3</td>\n",
       "      <td>116000000</td>\n",
       "      <td>5785260.50</td>\n",
       "      <td>4939926.50</td>\n",
       "      <td>10000000</td>\n",
       "      <td>3</td>\n",
       "      <td>111000000</td>\n",
       "      <td>6144837.00</td>\n",
       "      <td>5018176.50</td>\n",
       "      <td>10000000</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>524</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0</td>\n",
       "      <td>1460</td>\n",
       "      <td>158.00</td>\n",
       "      <td>392.00</td>\n",
       "      <td>153571.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>161.90</td>\n",
       "      <td>40.80</td>\n",
       "      <td>295.80</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>857</td>\n",
       "      <td>19</td>\n",
       "      <td>5619</td>\n",
       "      <td>8192</td>\n",
       "      <td>123</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>53392.75</td>\n",
       "      <td>63815.81</td>\n",
       "      <td>256035</td>\n",
       "      <td>34810</td>\n",
       "      <td>9588708.00</td>\n",
       "      <td>1425203.20</td>\n",
       "      <td>10000000</td>\n",
       "      <td>5063080</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>443</td>\n",
       "      <td>117492940</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>844</td>\n",
       "      <td>6642</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>56.28</td>\n",
       "      <td>75.94</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>369.00</td>\n",
       "      <td>570.00</td>\n",
       "      <td>63.71</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3671654.50</td>\n",
       "      <td>14400000.00</td>\n",
       "      <td>58500000</td>\n",
       "      <td>1</td>\n",
       "      <td>117000000</td>\n",
       "      <td>8392353.00</td>\n",
       "      <td>21200000.00</td>\n",
       "      <td>58500000</td>\n",
       "      <td>177</td>\n",
       "      <td>117000000</td>\n",
       "      <td>6896788.00</td>\n",
       "      <td>19400000.00</td>\n",
       "      <td>58500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>372</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>1430</td>\n",
       "      <td>220.10</td>\n",
       "      <td>442.00</td>\n",
       "      <td>195444.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>226.90</td>\n",
       "      <td>56.28</td>\n",
       "      <td>369.00</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>844</td>\n",
       "      <td>18</td>\n",
       "      <td>6642</td>\n",
       "      <td>8192</td>\n",
       "      <td>360</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>219402.50</td>\n",
       "      <td>21680.60</td>\n",
       "      <td>234733</td>\n",
       "      <td>204072</td>\n",
       "      <td>58400000.00</td>\n",
       "      <td>128001.18</td>\n",
       "      <td>58500000</td>\n",
       "      <td>58300000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>443</td>\n",
       "      <td>115158103</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>1060</td>\n",
       "      <td>1490</td>\n",
       "      <td>638</td>\n",
       "      <td>0</td>\n",
       "      <td>66.25</td>\n",
       "      <td>185.20</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "      <td>106.44</td>\n",
       "      <td>327.00</td>\n",
       "      <td>22.14</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3970969.00</td>\n",
       "      <td>4871286.00</td>\n",
       "      <td>10000000</td>\n",
       "      <td>68</td>\n",
       "      <td>115000000</td>\n",
       "      <td>7675666.00</td>\n",
       "      <td>4164308.80</td>\n",
       "      <td>10000000</td>\n",
       "      <td>25795</td>\n",
       "      <td>115000000</td>\n",
       "      <td>8856550.00</td>\n",
       "      <td>2990824.20</td>\n",
       "      <td>10000000</td>\n",
       "      <td>540604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>448</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>1213</td>\n",
       "      <td>82.25</td>\n",
       "      <td>253.20</td>\n",
       "      <td>64115.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.00</td>\n",
       "      <td>66.25</td>\n",
       "      <td>106.44</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1060</td>\n",
       "      <td>14</td>\n",
       "      <td>1490</td>\n",
       "      <td>474</td>\n",
       "      <td>1282</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>444734.53</td>\n",
       "      <td>1398633.10</td>\n",
       "      <td>4661772</td>\n",
       "      <td>22947</td>\n",
       "      <td>10000000.00</td>\n",
       "      <td>6231.22</td>\n",
       "      <td>10000000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>80</td>\n",
       "      <td>115534692</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>1816</td>\n",
       "      <td>200</td>\n",
       "      <td>1805</td>\n",
       "      <td>0</td>\n",
       "      <td>113.50</td>\n",
       "      <td>451.00</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>53.44</td>\n",
       "      <td>17.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3983955.00</td>\n",
       "      <td>4859992.00</td>\n",
       "      <td>10100000</td>\n",
       "      <td>59</td>\n",
       "      <td>116000000</td>\n",
       "      <td>7702313.00</td>\n",
       "      <td>4160389.00</td>\n",
       "      <td>10100000</td>\n",
       "      <td>857</td>\n",
       "      <td>111000000</td>\n",
       "      <td>8520057.00</td>\n",
       "      <td>3769811.20</td>\n",
       "      <td>10100000</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>288</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>1805</td>\n",
       "      <td>65.06</td>\n",
       "      <td>325.00</td>\n",
       "      <td>105564.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.20</td>\n",
       "      <td>113.50</td>\n",
       "      <td>14.29</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1816</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>8192</td>\n",
       "      <td>127</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>72623.91</td>\n",
       "      <td>73833.51</td>\n",
       "      <td>295240</td>\n",
       "      <td>50227</td>\n",
       "      <td>9996546.00</td>\n",
       "      <td>35284.62</td>\n",
       "      <td>10100000</td>\n",
       "      <td>9899902</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>443</td>\n",
       "      <td>116855896</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>330</td>\n",
       "      <td>9228</td>\n",
       "      <td>196</td>\n",
       "      <td>6</td>\n",
       "      <td>33.00</td>\n",
       "      <td>59.62</td>\n",
       "      <td>1529</td>\n",
       "      <td>42</td>\n",
       "      <td>923.00</td>\n",
       "      <td>627.00</td>\n",
       "      <td>81.79</td>\n",
       "      <td>0.17</td>\n",
       "      <td>6150310.50</td>\n",
       "      <td>18374902.00</td>\n",
       "      <td>58341403</td>\n",
       "      <td>1</td>\n",
       "      <td>116855896</td>\n",
       "      <td>12983988.00</td>\n",
       "      <td>25687842.00</td>\n",
       "      <td>58341403</td>\n",
       "      <td>1</td>\n",
       "      <td>116775664</td>\n",
       "      <td>12975074.00</td>\n",
       "      <td>25723868.00</td>\n",
       "      <td>58421249</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>6</td>\n",
       "      <td>1529</td>\n",
       "      <td>464.50</td>\n",
       "      <td>616.50</td>\n",
       "      <td>380112.38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>487.80</td>\n",
       "      <td>33.00</td>\n",
       "      <td>923.00</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>330</td>\n",
       "      <td>10</td>\n",
       "      <td>9228</td>\n",
       "      <td>1267</td>\n",
       "      <td>309</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>95023.50</td>\n",
       "      <td>21485.44</td>\n",
       "      <td>110216</td>\n",
       "      <td>79831</td>\n",
       "      <td>58293000.00</td>\n",
       "      <td>68452.18</td>\n",
       "      <td>58341403</td>\n",
       "      <td>58244597</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534692</th>\n",
       "      <td>444</td>\n",
       "      <td>119259886</td>\n",
       "      <td>2782</td>\n",
       "      <td>2091</td>\n",
       "      <td>12264</td>\n",
       "      <td>7879536</td>\n",
       "      <td>4344</td>\n",
       "      <td>0</td>\n",
       "      <td>4.41</td>\n",
       "      <td>83.40</td>\n",
       "      <td>15928</td>\n",
       "      <td>0</td>\n",
       "      <td>3768.00</td>\n",
       "      <td>2374.00</td>\n",
       "      <td>66173.13</td>\n",
       "      <td>40.86</td>\n",
       "      <td>24478.63</td>\n",
       "      <td>153117.48</td>\n",
       "      <td>995232</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42883.79</td>\n",
       "      <td>200919.11</td>\n",
       "      <td>996222</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>57062.14</td>\n",
       "      <td>229801.25</td>\n",
       "      <td>995232</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89024</td>\n",
       "      <td>66912</td>\n",
       "      <td>23.33</td>\n",
       "      <td>17.53</td>\n",
       "      <td>0</td>\n",
       "      <td>15928</td>\n",
       "      <td>1620.00</td>\n",
       "      <td>2428.00</td>\n",
       "      <td>5894571.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1620.00</td>\n",
       "      <td>4.41</td>\n",
       "      <td>3768.00</td>\n",
       "      <td>89024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2782</td>\n",
       "      <td>12264</td>\n",
       "      <td>2091</td>\n",
       "      <td>7879536</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534695</th>\n",
       "      <td>444</td>\n",
       "      <td>119259012</td>\n",
       "      <td>2801</td>\n",
       "      <td>2069</td>\n",
       "      <td>12264</td>\n",
       "      <td>7879536</td>\n",
       "      <td>4344</td>\n",
       "      <td>0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>83.10</td>\n",
       "      <td>15928</td>\n",
       "      <td>0</td>\n",
       "      <td>3808.00</td>\n",
       "      <td>2460.00</td>\n",
       "      <td>66173.62</td>\n",
       "      <td>40.84</td>\n",
       "      <td>24493.53</td>\n",
       "      <td>153287.53</td>\n",
       "      <td>995262</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42592.48</td>\n",
       "      <td>200437.97</td>\n",
       "      <td>996257</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>57668.76</td>\n",
       "      <td>231138.53</td>\n",
       "      <td>995262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89632</td>\n",
       "      <td>66208</td>\n",
       "      <td>23.49</td>\n",
       "      <td>17.35</td>\n",
       "      <td>0</td>\n",
       "      <td>15928</td>\n",
       "      <td>1621.00</td>\n",
       "      <td>2472.00</td>\n",
       "      <td>6109806.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1621.00</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3808.00</td>\n",
       "      <td>89632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2801</td>\n",
       "      <td>12264</td>\n",
       "      <td>2069</td>\n",
       "      <td>7879536</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534708</th>\n",
       "      <td>444</td>\n",
       "      <td>119257653</td>\n",
       "      <td>2802</td>\n",
       "      <td>2067</td>\n",
       "      <td>20858</td>\n",
       "      <td>7812389</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>7.45</td>\n",
       "      <td>126.06</td>\n",
       "      <td>13032</td>\n",
       "      <td>0</td>\n",
       "      <td>3780.00</td>\n",
       "      <td>2380.00</td>\n",
       "      <td>65683.39</td>\n",
       "      <td>40.83</td>\n",
       "      <td>24498.28</td>\n",
       "      <td>153803.62</td>\n",
       "      <td>1995346</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42576.80</td>\n",
       "      <td>201441.28</td>\n",
       "      <td>1996118</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>57723.93</td>\n",
       "      <td>233324.72</td>\n",
       "      <td>1995346</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89664</td>\n",
       "      <td>66144</td>\n",
       "      <td>23.50</td>\n",
       "      <td>17.33</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1610.00</td>\n",
       "      <td>2428.00</td>\n",
       "      <td>5893055.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1610.00</td>\n",
       "      <td>7.45</td>\n",
       "      <td>3780.00</td>\n",
       "      <td>89664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2802</td>\n",
       "      <td>20858</td>\n",
       "      <td>2067</td>\n",
       "      <td>7812389</td>\n",
       "      <td>235</td>\n",
       "      <td>349</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534711</th>\n",
       "      <td>444</td>\n",
       "      <td>119299621</td>\n",
       "      <td>2805</td>\n",
       "      <td>2028</td>\n",
       "      <td>13712</td>\n",
       "      <td>7878627</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>4.89</td>\n",
       "      <td>110.10</td>\n",
       "      <td>17376</td>\n",
       "      <td>0</td>\n",
       "      <td>3884.00</td>\n",
       "      <td>2524.00</td>\n",
       "      <td>66155.61</td>\n",
       "      <td>40.51</td>\n",
       "      <td>24689.49</td>\n",
       "      <td>152728.52</td>\n",
       "      <td>996008</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42531.96</td>\n",
       "      <td>199646.58</td>\n",
       "      <td>996884</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>58855.26</td>\n",
       "      <td>232967.11</td>\n",
       "      <td>996008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89772</td>\n",
       "      <td>64896</td>\n",
       "      <td>23.51</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0</td>\n",
       "      <td>17376</td>\n",
       "      <td>1634.00</td>\n",
       "      <td>2520.00</td>\n",
       "      <td>6348930.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1634.00</td>\n",
       "      <td>4.89</td>\n",
       "      <td>3884.00</td>\n",
       "      <td>89772</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2805</td>\n",
       "      <td>13712</td>\n",
       "      <td>2028</td>\n",
       "      <td>7878627</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534712</th>\n",
       "      <td>444</td>\n",
       "      <td>119296592</td>\n",
       "      <td>2797</td>\n",
       "      <td>2006</td>\n",
       "      <td>13712</td>\n",
       "      <td>7878088</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>110.25</td>\n",
       "      <td>13032</td>\n",
       "      <td>0</td>\n",
       "      <td>3928.00</td>\n",
       "      <td>2444.00</td>\n",
       "      <td>66152.77</td>\n",
       "      <td>40.26</td>\n",
       "      <td>24843.10</td>\n",
       "      <td>153403.72</td>\n",
       "      <td>995988</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42666.88</td>\n",
       "      <td>200137.52</td>\n",
       "      <td>996943</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>59480.77</td>\n",
       "      <td>234233.89</td>\n",
       "      <td>995988</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89504</td>\n",
       "      <td>64192</td>\n",
       "      <td>23.45</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>2498.00</td>\n",
       "      <td>6244886.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>3928.00</td>\n",
       "      <td>89504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2797</td>\n",
       "      <td>13712</td>\n",
       "      <td>2006</td>\n",
       "      <td>7878088</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15464 rows √ó 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "18                   443      115705212                 21   \n",
       "41                   443      117492940                 15   \n",
       "51                   443      115158103                 16   \n",
       "65                    80      115534692                 16   \n",
       "81                   443      116855896                 10   \n",
       "...                  ...            ...                ...   \n",
       "534692               444      119259886               2782   \n",
       "534695               444      119259012               2801   \n",
       "534708               444      119257653               2802   \n",
       "534711               444      119299621               2805   \n",
       "534712               444      119296592               2797   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "18                          19                          857   \n",
       "41                          18                          844   \n",
       "51                          14                         1060   \n",
       "65                          14                         1816   \n",
       "81                          10                          330   \n",
       "...                        ...                          ...   \n",
       "534692                    2091                        12264   \n",
       "534695                    2069                        12264   \n",
       "534708                    2067                        20858   \n",
       "534711                    2028                        13712   \n",
       "534712                    2006                        13712   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "18                             5619                    398   \n",
       "41                             6642                    219   \n",
       "51                             1490                    638   \n",
       "65                              200                   1805   \n",
       "81                             9228                    196   \n",
       "...                             ...                    ...   \n",
       "534692                      7879536                   4344   \n",
       "534695                      7879536                   4344   \n",
       "534708                      7812389                   5792   \n",
       "534711                      7878627                   5792   \n",
       "534712                      7878088                   5792   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "18                          0                   40.80                  95.75   \n",
       "41                          0                   56.28                  75.94   \n",
       "51                          0                   66.25                 185.20   \n",
       "65                          0                  113.50                 451.00   \n",
       "81                          6                   33.00                  59.62   \n",
       "...                       ...                     ...                    ...   \n",
       "534692                      0                    4.41                  83.40   \n",
       "534695                      0                    4.38                  83.10   \n",
       "534708                      0                    7.45                 126.06   \n",
       "534711                      0                    4.89                 110.10   \n",
       "534712                      0                    4.90                 110.25   \n",
       "\n",
       "        bwd_packet_length_max  bwd_packet_length_min  bwd_packet_length_mean  \\\n",
       "18                       1460                      0                  295.80   \n",
       "41                       1430                      0                  369.00   \n",
       "51                       1213                      0                  106.44   \n",
       "65                        200                      0                   14.29   \n",
       "81                       1529                     42                  923.00   \n",
       "...                       ...                    ...                     ...   \n",
       "534692                  15928                      0                 3768.00   \n",
       "534695                  15928                      0                 3808.00   \n",
       "534708                  13032                      0                 3780.00   \n",
       "534711                  17376                      0                 3884.00   \n",
       "534712                  13032                      0                 3928.00   \n",
       "\n",
       "        bwd_packet_length_std  flow_bytes/s  flow_packets/s  flow_iat_mean  \\\n",
       "18                     542.00         55.97            0.35     2966800.20   \n",
       "41                     570.00         63.71            0.28     3671654.50   \n",
       "51                     327.00         22.14            0.26     3970969.00   \n",
       "65                      53.44         17.45            0.26     3983955.00   \n",
       "81                     627.00         81.79            0.17     6150310.50   \n",
       "...                       ...           ...             ...            ...   \n",
       "534692                2374.00      66173.13           40.86       24478.63   \n",
       "534695                2460.00      66173.62           40.84       24493.53   \n",
       "534708                2380.00      65683.39           40.83       24498.28   \n",
       "534711                2524.00      66155.61           40.51       24689.49   \n",
       "534712                2444.00      66152.77           40.26       24843.10   \n",
       "\n",
       "        flow_iat_std  flow_iat_max  flow_iat_min  fwd_iat_total  fwd_iat_mean  \\\n",
       "18        4537612.00      10000000             3      116000000    5785260.50   \n",
       "41       14400000.00      58500000             1      117000000    8392353.00   \n",
       "51        4871286.00      10000000            68      115000000    7675666.00   \n",
       "65        4859992.00      10100000            59      116000000    7702313.00   \n",
       "81       18374902.00      58341403             1      116855896   12983988.00   \n",
       "...              ...           ...           ...            ...           ...   \n",
       "534692     153117.48        995232             0      119000000      42883.79   \n",
       "534695     153287.53        995262             0      119000000      42592.48   \n",
       "534708     153803.62       1995346            -1      119000000      42576.80   \n",
       "534711     152728.52        996008            -1      119000000      42531.96   \n",
       "534712     153403.72        995988            -1      119000000      42666.88   \n",
       "\n",
       "        fwd_iat_std  fwd_iat_max  fwd_iat_min  bwd_iat_total  bwd_iat_mean  \\\n",
       "18       4939926.50     10000000            3      111000000    6144837.00   \n",
       "41      21200000.00     58500000          177      117000000    6896788.00   \n",
       "51       4164308.80     10000000        25795      115000000    8856550.00   \n",
       "65       4160389.00     10100000          857      111000000    8520057.00   \n",
       "81      25687842.00     58341403            1      116775664   12975074.00   \n",
       "...             ...          ...          ...            ...           ...   \n",
       "534692    200919.11       996222            0      119000000      57062.14   \n",
       "534695    200437.97       996257            0      119000000      57668.76   \n",
       "534708    201441.28      1996118            0      119000000      57723.93   \n",
       "534711    199646.58       996884            0      119000000      58855.26   \n",
       "534712    200137.52       996943            0      119000000      59480.77   \n",
       "\n",
       "        bwd_iat_std  bwd_iat_max  bwd_iat_min  fwd_psh_flags  bwd_psh_flags  \\\n",
       "18       5018176.50     10000000           48              0              0   \n",
       "41      19400000.00     58500000            1              0              0   \n",
       "51       2990824.20     10000000       540604              0              0   \n",
       "65       3769811.20     10100000          243              0              0   \n",
       "81      25723868.00     58421249            1              1              0   \n",
       "...             ...          ...          ...            ...            ...   \n",
       "534692    229801.25       995232            1              0              0   \n",
       "534695    231138.53       995262            2              0              0   \n",
       "534708    233324.72      1995346            1              0              0   \n",
       "534711    232967.11       996008            1              0              0   \n",
       "534712    234233.89       995988            1              0              0   \n",
       "\n",
       "        fwd_urg_flags  bwd_urg_flags  fwd_header_length  bwd_header_length  \\\n",
       "18                  0              0                432                524   \n",
       "41                  0              0                312                372   \n",
       "51                  0              0                512                448   \n",
       "65                  0              0                332                288   \n",
       "81                  0              0                200                200   \n",
       "...               ...            ...                ...                ...   \n",
       "534692              0              0              89024              66912   \n",
       "534695              0              0              89632              66208   \n",
       "534708              0              0              89664              66144   \n",
       "534711              0              0              89772              64896   \n",
       "534712              0              0              89504              64192   \n",
       "\n",
       "        fwd_packets/s  bwd_packets/s  min_packet_length  max_packet_length  \\\n",
       "18               0.18           0.16                  0               1460   \n",
       "41               0.13           0.15                  0               1430   \n",
       "51               0.14           0.12                  0               1213   \n",
       "65               0.14           0.12                  0               1805   \n",
       "81               0.09           0.09                  6               1529   \n",
       "...               ...            ...                ...                ...   \n",
       "534692          23.33          17.53                  0              15928   \n",
       "534695          23.49          17.35                  0              15928   \n",
       "534708          23.50          17.33                  0              13032   \n",
       "534711          23.51          17.00                  0              17376   \n",
       "534712          23.45          16.82                  0              13032   \n",
       "\n",
       "        packet_length_mean  packet_length_std  packet_length_variance  \\\n",
       "18                  158.00             392.00               153571.95   \n",
       "41                  220.10             442.00               195444.10   \n",
       "51                   82.25             253.20                64115.60   \n",
       "65                   65.06             325.00               105564.37   \n",
       "81                  464.50             616.50               380112.38   \n",
       "...                    ...                ...                     ...   \n",
       "534692             1620.00            2428.00              5894571.00   \n",
       "534695             1621.00            2472.00              6109806.00   \n",
       "534708             1610.00            2428.00              5893055.50   \n",
       "534711             1634.00            2520.00              6348930.50   \n",
       "534712             1644.00            2498.00              6244886.00   \n",
       "\n",
       "        fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "18                   0               0               0               1   \n",
       "41                   0               0               0               1   \n",
       "51                   0               0               0               0   \n",
       "65                   0               0               0               1   \n",
       "81                   0               1               0               0   \n",
       "...                ...             ...             ...             ...   \n",
       "534692               0               0               0               0   \n",
       "534695               0               0               0               0   \n",
       "534708               0               0               0               0   \n",
       "534711               0               0               0               0   \n",
       "534712               0               0               0               0   \n",
       "\n",
       "        ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "18                   0               0               0               0   \n",
       "41                   0               0               0               0   \n",
       "51                   1               0               0               0   \n",
       "65                   0               0               0               0   \n",
       "81                   1               0               0               0   \n",
       "...                ...             ...             ...             ...   \n",
       "534692               1               0               0               0   \n",
       "534695               1               0               0               0   \n",
       "534708               1               0               0               0   \n",
       "534711               1               0               0               0   \n",
       "534712               1               0               0               0   \n",
       "\n",
       "        down/up_ratio  average_packet_size  avg_fwd_segment_size  \\\n",
       "18                  0               161.90                 40.80   \n",
       "41                  1               226.90                 56.28   \n",
       "51                  0                85.00                 66.25   \n",
       "65                  0                67.20                113.50   \n",
       "81                  1               487.80                 33.00   \n",
       "...               ...                  ...                   ...   \n",
       "534692              0              1620.00                  4.41   \n",
       "534695              0              1621.00                  4.38   \n",
       "534708              0              1610.00                  7.45   \n",
       "534711              0              1634.00                  4.89   \n",
       "534712              0              1644.00                  4.90   \n",
       "\n",
       "        avg_bwd_segment_size  fwd_header_length.1  fwd_avg_bytes/bulk  \\\n",
       "18                    295.80                  432                   0   \n",
       "41                    369.00                  312                   0   \n",
       "51                    106.44                  512                   0   \n",
       "65                     14.29                  332                   0   \n",
       "81                    923.00                  200                   0   \n",
       "...                      ...                  ...                 ...   \n",
       "534692               3768.00                89024                   0   \n",
       "534695               3808.00                89632                   0   \n",
       "534708               3780.00                89664                   0   \n",
       "534711               3884.00                89772                   0   \n",
       "534712               3928.00                89504                   0   \n",
       "\n",
       "        fwd_avg_packets/bulk  fwd_avg_bulk_rate  bwd_avg_bytes/bulk  \\\n",
       "18                         0                  0                   0   \n",
       "41                         0                  0                   0   \n",
       "51                         0                  0                   0   \n",
       "65                         0                  0                   0   \n",
       "81                         0                  0                   0   \n",
       "...                      ...                ...                 ...   \n",
       "534692                     0                  0                   0   \n",
       "534695                     0                  0                   0   \n",
       "534708                     0                  0                   0   \n",
       "534711                     0                  0                   0   \n",
       "534712                     0                  0                   0   \n",
       "\n",
       "        bwd_avg_packets/bulk  bwd_avg_bulk_rate  subflow_fwd_packets  \\\n",
       "18                         0                  0                   21   \n",
       "41                         0                  0                   15   \n",
       "51                         0                  0                   16   \n",
       "65                         0                  0                   16   \n",
       "81                         0                  0                   10   \n",
       "...                      ...                ...                  ...   \n",
       "534692                     0                  0                 2782   \n",
       "534695                     0                  0                 2801   \n",
       "534708                     0                  0                 2802   \n",
       "534711                     0                  0                 2805   \n",
       "534712                     0                  0                 2797   \n",
       "\n",
       "        subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "18                    857                   19               5619   \n",
       "41                    844                   18               6642   \n",
       "51                   1060                   14               1490   \n",
       "65                   1816                   14                200   \n",
       "81                    330                   10               9228   \n",
       "...                   ...                  ...                ...   \n",
       "534692              12264                 2091            7879536   \n",
       "534695              12264                 2069            7879536   \n",
       "534708              20858                 2067            7812389   \n",
       "534711              13712                 2028            7878627   \n",
       "534712              13712                 2006            7878088   \n",
       "\n",
       "        init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd  \\\n",
       "18                        8192                      123                20   \n",
       "41                        8192                      360                 8   \n",
       "51                         474                     1282                 2   \n",
       "65                        8192                      127                12   \n",
       "81                        1267                      309                 9   \n",
       "...                        ...                      ...               ...   \n",
       "534692                     235                      235               120   \n",
       "534695                     235                      235               120   \n",
       "534708                     235                      349               123   \n",
       "534711                     349                      349               120   \n",
       "534712                     349                      349               120   \n",
       "\n",
       "        min_seg_size_forward  active_mean  active_std  active_max  active_min  \\\n",
       "18                        20     53392.75    63815.81      256035       34810   \n",
       "41                        20    219402.50    21680.60      234733      204072   \n",
       "51                        32    444734.53  1398633.10     4661772       22947   \n",
       "65                        20     72623.91    73833.51      295240       50227   \n",
       "81                        20     95023.50    21485.44      110216       79831   \n",
       "...                      ...          ...         ...         ...         ...   \n",
       "534692                    32         0.00        0.00           0           0   \n",
       "534695                    32         0.00        0.00           0           0   \n",
       "534708                    32         0.00        0.00           0           0   \n",
       "534711                    32         0.00        0.00           0           0   \n",
       "534712                    32         0.00        0.00           0           0   \n",
       "\n",
       "         idle_mean   idle_std  idle_max  idle_min       label  \n",
       "18      9588708.00 1425203.20  10000000   5063080      BENIGN  \n",
       "41     58400000.00  128001.18  58500000  58300000      BENIGN  \n",
       "51     10000000.00    6231.22  10000000  10000000      BENIGN  \n",
       "65      9996546.00   35284.62  10100000   9899902      BENIGN  \n",
       "81     58293000.00   68452.18  58341403  58244597      BENIGN  \n",
       "...            ...        ...       ...       ...         ...  \n",
       "534692        0.00       0.00         0         0  Heartbleed  \n",
       "534695        0.00       0.00         0         0  Heartbleed  \n",
       "534708        0.00       0.00         0         0  Heartbleed  \n",
       "534711        0.00       0.00         0         0  Heartbleed  \n",
       "534712        0.00       0.00         0         0  Heartbleed  \n",
       "\n",
       "[15464 rows x 79 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['flow_duration'] > 110000000.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>bwd_packet_length_max</th>\n",
       "      <th>bwd_packet_length_min</th>\n",
       "      <th>bwd_packet_length_mean</th>\n",
       "      <th>bwd_packet_length_std</th>\n",
       "      <th>flow_bytes/s</th>\n",
       "      <th>flow_packets/s</th>\n",
       "      <th>flow_iat_mean</th>\n",
       "      <th>flow_iat_std</th>\n",
       "      <th>flow_iat_max</th>\n",
       "      <th>flow_iat_min</th>\n",
       "      <th>fwd_iat_total</th>\n",
       "      <th>fwd_iat_mean</th>\n",
       "      <th>fwd_iat_std</th>\n",
       "      <th>fwd_iat_max</th>\n",
       "      <th>fwd_iat_min</th>\n",
       "      <th>bwd_iat_total</th>\n",
       "      <th>bwd_iat_mean</th>\n",
       "      <th>bwd_iat_std</th>\n",
       "      <th>bwd_iat_max</th>\n",
       "      <th>bwd_iat_min</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets/s</th>\n",
       "      <th>bwd_packets/s</th>\n",
       "      <th>min_packet_length</th>\n",
       "      <th>max_packet_length</th>\n",
       "      <th>packet_length_mean</th>\n",
       "      <th>packet_length_std</th>\n",
       "      <th>packet_length_variance</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down/up_ratio</th>\n",
       "      <th>average_packet_size</th>\n",
       "      <th>avg_fwd_segment_size</th>\n",
       "      <th>avg_bwd_segment_size</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>fwd_avg_bytes/bulk</th>\n",
       "      <th>fwd_avg_packets/bulk</th>\n",
       "      <th>fwd_avg_bulk_rate</th>\n",
       "      <th>bwd_avg_bytes/bulk</th>\n",
       "      <th>bwd_avg_packets/bulk</th>\n",
       "      <th>bwd_avg_bulk_rate</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>443</td>\n",
       "      <td>115305121</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>683</td>\n",
       "      <td>23655</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>25.30</td>\n",
       "      <td>74.94</td>\n",
       "      <td>2896</td>\n",
       "      <td>0</td>\n",
       "      <td>815.50</td>\n",
       "      <td>834.50</td>\n",
       "      <td>211.07</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2096456.80</td>\n",
       "      <td>4105428.50</td>\n",
       "      <td>10200000</td>\n",
       "      <td>-1</td>\n",
       "      <td>115000000</td>\n",
       "      <td>4434812.50</td>\n",
       "      <td>5079964.00</td>\n",
       "      <td>10200000</td>\n",
       "      <td>17</td>\n",
       "      <td>113000000</td>\n",
       "      <td>4018234.00</td>\n",
       "      <td>5081003.00</td>\n",
       "      <td>10200000</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>872</td>\n",
       "      <td>936</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>2896</td>\n",
       "      <td>427.00</td>\n",
       "      <td>714.50</td>\n",
       "      <td>510155.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>434.50</td>\n",
       "      <td>25.30</td>\n",
       "      <td>815.50</td>\n",
       "      <td>872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>683</td>\n",
       "      <td>29</td>\n",
       "      <td>23655</td>\n",
       "      <td>29200</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>29552.54</td>\n",
       "      <td>24875.20</td>\n",
       "      <td>104553</td>\n",
       "      <td>21951</td>\n",
       "      <td>10200000.00</td>\n",
       "      <td>63927.73</td>\n",
       "      <td>10200000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>443</td>\n",
       "      <td>118418517</td>\n",
       "      <td>121</td>\n",
       "      <td>134</td>\n",
       "      <td>4861</td>\n",
       "      <td>170005</td>\n",
       "      <td>1268</td>\n",
       "      <td>0</td>\n",
       "      <td>40.20</td>\n",
       "      <td>204.50</td>\n",
       "      <td>2836</td>\n",
       "      <td>0</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>523.00</td>\n",
       "      <td>1476.68</td>\n",
       "      <td>2.15</td>\n",
       "      <td>466214.62</td>\n",
       "      <td>2058548.80</td>\n",
       "      <td>10000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>118000000</td>\n",
       "      <td>986821.00</td>\n",
       "      <td>2925038.20</td>\n",
       "      <td>10000000</td>\n",
       "      <td>1</td>\n",
       "      <td>114000000</td>\n",
       "      <td>855024.06</td>\n",
       "      <td>2775126.00</td>\n",
       "      <td>10000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3880</td>\n",
       "      <td>4296</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0</td>\n",
       "      <td>2836</td>\n",
       "      <td>683.00</td>\n",
       "      <td>735.00</td>\n",
       "      <td>540388.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>685.50</td>\n",
       "      <td>40.20</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>3880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>4861</td>\n",
       "      <td>134</td>\n",
       "      <td>170005</td>\n",
       "      <td>29200</td>\n",
       "      <td>424</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>327637.72</td>\n",
       "      <td>965977.40</td>\n",
       "      <td>3240169</td>\n",
       "      <td>36253</td>\n",
       "      <td>10000000.00</td>\n",
       "      <td>2724.28</td>\n",
       "      <td>10000000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>80</td>\n",
       "      <td>11019155</td>\n",
       "      <td>10593</td>\n",
       "      <td>8104</td>\n",
       "      <td>3603</td>\n",
       "      <td>20200000</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>11.41</td>\n",
       "      <td>11680</td>\n",
       "      <td>0</td>\n",
       "      <td>2492.00</td>\n",
       "      <td>1514.00</td>\n",
       "      <td>1833440.95</td>\n",
       "      <td>1696.77</td>\n",
       "      <td>589.39</td>\n",
       "      <td>47195.49</td>\n",
       "      <td>6391143</td>\n",
       "      <td>-1</td>\n",
       "      <td>11000000</td>\n",
       "      <td>1040.33</td>\n",
       "      <td>62713.59</td>\n",
       "      <td>6391245</td>\n",
       "      <td>0</td>\n",
       "      <td>11000000</td>\n",
       "      <td>1358.02</td>\n",
       "      <td>71948.18</td>\n",
       "      <td>6407401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>318120</td>\n",
       "      <td>162092</td>\n",
       "      <td>961.33</td>\n",
       "      <td>735.45</td>\n",
       "      <td>0</td>\n",
       "      <td>11680</td>\n",
       "      <td>1080.00</td>\n",
       "      <td>1587.00</td>\n",
       "      <td>2518463.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1081.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2492.00</td>\n",
       "      <td>318120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10593</td>\n",
       "      <td>3603</td>\n",
       "      <td>8104</td>\n",
       "      <td>20199367</td>\n",
       "      <td>8192</td>\n",
       "      <td>1252</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1795748.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1795748</td>\n",
       "      <td>1795748</td>\n",
       "      <td>6391143.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6391143</td>\n",
       "      <td>6391143</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>443</td>\n",
       "      <td>119820454</td>\n",
       "      <td>1032</td>\n",
       "      <td>1030</td>\n",
       "      <td>1251</td>\n",
       "      <td>558001</td>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>27.77</td>\n",
       "      <td>1708</td>\n",
       "      <td>0</td>\n",
       "      <td>541.50</td>\n",
       "      <td>265.20</td>\n",
       "      <td>4667.42</td>\n",
       "      <td>17.21</td>\n",
       "      <td>58137.05</td>\n",
       "      <td>131034.26</td>\n",
       "      <td>1661402</td>\n",
       "      <td>-1</td>\n",
       "      <td>120000000</td>\n",
       "      <td>116217.70</td>\n",
       "      <td>166736.22</td>\n",
       "      <td>1661447</td>\n",
       "      <td>19</td>\n",
       "      <td>120000000</td>\n",
       "      <td>116301.48</td>\n",
       "      <td>167119.73</td>\n",
       "      <td>1661420</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33024</td>\n",
       "      <td>32960</td>\n",
       "      <td>8.61</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1708</td>\n",
       "      <td>271.50</td>\n",
       "      <td>329.80</td>\n",
       "      <td>108734.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271.80</td>\n",
       "      <td>1.21</td>\n",
       "      <td>541.50</td>\n",
       "      <td>33024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>1251</td>\n",
       "      <td>1030</td>\n",
       "      <td>558001</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>443</td>\n",
       "      <td>115912151</td>\n",
       "      <td>103</td>\n",
       "      <td>105</td>\n",
       "      <td>4748</td>\n",
       "      <td>125290</td>\n",
       "      <td>1747</td>\n",
       "      <td>0</td>\n",
       "      <td>46.10</td>\n",
       "      <td>235.00</td>\n",
       "      <td>2836</td>\n",
       "      <td>0</td>\n",
       "      <td>1193.00</td>\n",
       "      <td>563.50</td>\n",
       "      <td>1121.87</td>\n",
       "      <td>1.79</td>\n",
       "      <td>559962.06</td>\n",
       "      <td>2262796.80</td>\n",
       "      <td>10000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>116000000</td>\n",
       "      <td>1136037.50</td>\n",
       "      <td>3139435.20</td>\n",
       "      <td>10000000</td>\n",
       "      <td>1</td>\n",
       "      <td>116000000</td>\n",
       "      <td>1114151.60</td>\n",
       "      <td>3114785.50</td>\n",
       "      <td>10100000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3304</td>\n",
       "      <td>3368</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "      <td>2836</td>\n",
       "      <td>622.00</td>\n",
       "      <td>719.00</td>\n",
       "      <td>516610.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>625.00</td>\n",
       "      <td>46.10</td>\n",
       "      <td>1193.00</td>\n",
       "      <td>3304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>4748</td>\n",
       "      <td>105</td>\n",
       "      <td>125290</td>\n",
       "      <td>29200</td>\n",
       "      <td>442</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>523658.00</td>\n",
       "      <td>1614832.00</td>\n",
       "      <td>5392559</td>\n",
       "      <td>36192</td>\n",
       "      <td>10000000.00</td>\n",
       "      <td>2242.80</td>\n",
       "      <td>10000000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534708</th>\n",
       "      <td>444</td>\n",
       "      <td>119257653</td>\n",
       "      <td>2802</td>\n",
       "      <td>2067</td>\n",
       "      <td>20858</td>\n",
       "      <td>7812389</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>7.45</td>\n",
       "      <td>126.06</td>\n",
       "      <td>13032</td>\n",
       "      <td>0</td>\n",
       "      <td>3780.00</td>\n",
       "      <td>2380.00</td>\n",
       "      <td>65683.39</td>\n",
       "      <td>40.83</td>\n",
       "      <td>24498.28</td>\n",
       "      <td>153803.62</td>\n",
       "      <td>1995346</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42576.80</td>\n",
       "      <td>201441.28</td>\n",
       "      <td>1996118</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>57723.93</td>\n",
       "      <td>233324.72</td>\n",
       "      <td>1995346</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89664</td>\n",
       "      <td>66144</td>\n",
       "      <td>23.50</td>\n",
       "      <td>17.33</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1610.00</td>\n",
       "      <td>2428.00</td>\n",
       "      <td>5893055.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1610.00</td>\n",
       "      <td>7.45</td>\n",
       "      <td>3780.00</td>\n",
       "      <td>89664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2802</td>\n",
       "      <td>20858</td>\n",
       "      <td>2067</td>\n",
       "      <td>7812389</td>\n",
       "      <td>235</td>\n",
       "      <td>349</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534711</th>\n",
       "      <td>444</td>\n",
       "      <td>119299621</td>\n",
       "      <td>2805</td>\n",
       "      <td>2028</td>\n",
       "      <td>13712</td>\n",
       "      <td>7878627</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>4.89</td>\n",
       "      <td>110.10</td>\n",
       "      <td>17376</td>\n",
       "      <td>0</td>\n",
       "      <td>3884.00</td>\n",
       "      <td>2524.00</td>\n",
       "      <td>66155.61</td>\n",
       "      <td>40.51</td>\n",
       "      <td>24689.49</td>\n",
       "      <td>152728.52</td>\n",
       "      <td>996008</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42531.96</td>\n",
       "      <td>199646.58</td>\n",
       "      <td>996884</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>58855.26</td>\n",
       "      <td>232967.11</td>\n",
       "      <td>996008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89772</td>\n",
       "      <td>64896</td>\n",
       "      <td>23.51</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0</td>\n",
       "      <td>17376</td>\n",
       "      <td>1634.00</td>\n",
       "      <td>2520.00</td>\n",
       "      <td>6348930.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1634.00</td>\n",
       "      <td>4.89</td>\n",
       "      <td>3884.00</td>\n",
       "      <td>89772</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2805</td>\n",
       "      <td>13712</td>\n",
       "      <td>2028</td>\n",
       "      <td>7878627</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534712</th>\n",
       "      <td>444</td>\n",
       "      <td>119296592</td>\n",
       "      <td>2797</td>\n",
       "      <td>2006</td>\n",
       "      <td>13712</td>\n",
       "      <td>7878088</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>110.25</td>\n",
       "      <td>13032</td>\n",
       "      <td>0</td>\n",
       "      <td>3928.00</td>\n",
       "      <td>2444.00</td>\n",
       "      <td>66152.77</td>\n",
       "      <td>40.26</td>\n",
       "      <td>24843.10</td>\n",
       "      <td>153403.72</td>\n",
       "      <td>995988</td>\n",
       "      <td>-1</td>\n",
       "      <td>119000000</td>\n",
       "      <td>42666.88</td>\n",
       "      <td>200137.52</td>\n",
       "      <td>996943</td>\n",
       "      <td>0</td>\n",
       "      <td>119000000</td>\n",
       "      <td>59480.77</td>\n",
       "      <td>234233.89</td>\n",
       "      <td>995988</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89504</td>\n",
       "      <td>64192</td>\n",
       "      <td>23.45</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>2498.00</td>\n",
       "      <td>6244886.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1644.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>3928.00</td>\n",
       "      <td>89504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2797</td>\n",
       "      <td>13712</td>\n",
       "      <td>2006</td>\n",
       "      <td>7878088</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heartbleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537855</th>\n",
       "      <td>80</td>\n",
       "      <td>11774601</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>407</td>\n",
       "      <td>11632</td>\n",
       "      <td>407</td>\n",
       "      <td>0</td>\n",
       "      <td>45.22</td>\n",
       "      <td>135.60</td>\n",
       "      <td>5792</td>\n",
       "      <td>0</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>2640.00</td>\n",
       "      <td>1022.46</td>\n",
       "      <td>1.19</td>\n",
       "      <td>905738.56</td>\n",
       "      <td>2239551.00</td>\n",
       "      <td>6772201</td>\n",
       "      <td>-1</td>\n",
       "      <td>6775695</td>\n",
       "      <td>846961.90</td>\n",
       "      <td>2394782.80</td>\n",
       "      <td>6773747</td>\n",
       "      <td>-1</td>\n",
       "      <td>11800000</td>\n",
       "      <td>2943427.50</td>\n",
       "      <td>3474753.50</td>\n",
       "      <td>6772201</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>160</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>5792</td>\n",
       "      <td>802.50</td>\n",
       "      <td>1802.00</td>\n",
       "      <td>3245484.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860.00</td>\n",
       "      <td>45.22</td>\n",
       "      <td>2326.00</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>5</td>\n",
       "      <td>11632</td>\n",
       "      <td>28960</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>891.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>6772201.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6772201</td>\n",
       "      <td>6772201</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539383</th>\n",
       "      <td>80</td>\n",
       "      <td>11857427</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>581</td>\n",
       "      <td>11632</td>\n",
       "      <td>581</td>\n",
       "      <td>0</td>\n",
       "      <td>72.60</td>\n",
       "      <td>205.40</td>\n",
       "      <td>11632</td>\n",
       "      <td>0</td>\n",
       "      <td>2908.00</td>\n",
       "      <td>5816.00</td>\n",
       "      <td>1029.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1077947.90</td>\n",
       "      <td>2432765.50</td>\n",
       "      <td>6853719</td>\n",
       "      <td>-11</td>\n",
       "      <td>6858257</td>\n",
       "      <td>979751.00</td>\n",
       "      <td>2591271.80</td>\n",
       "      <td>6856203</td>\n",
       "      <td>195</td>\n",
       "      <td>11900000</td>\n",
       "      <td>3952473.80</td>\n",
       "      <td>3545086.00</td>\n",
       "      <td>6853708</td>\n",
       "      <td>904</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>136</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>11632</td>\n",
       "      <td>939.50</td>\n",
       "      <td>3216.00</td>\n",
       "      <td>10300000.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1018.00</td>\n",
       "      <td>72.60</td>\n",
       "      <td>2908.00</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>581</td>\n",
       "      <td>4</td>\n",
       "      <td>11632</td>\n",
       "      <td>29200</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>899.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>899</td>\n",
       "      <td>899</td>\n",
       "      <td>6853719.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6853719</td>\n",
       "      <td>6853719</td>\n",
       "      <td>DoS GoldenEye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows √ó 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "257                  443      115305121                 27   \n",
       "405                  443      118418517                121   \n",
       "2764                  80       11019155              10593   \n",
       "4419                 443      119820454               1032   \n",
       "4822                 443      115912151                103   \n",
       "...                  ...            ...                ...   \n",
       "534708               444      119257653               2802   \n",
       "534711               444      119299621               2805   \n",
       "534712               444      119296592               2797   \n",
       "537855                80       11774601                  9   \n",
       "539383                80       11857427                  8   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "257                         29                          683   \n",
       "405                        134                         4861   \n",
       "2764                      8104                         3603   \n",
       "4419                      1030                         1251   \n",
       "4822                       105                         4748   \n",
       "...                        ...                          ...   \n",
       "534708                    2067                        20858   \n",
       "534711                    2028                        13712   \n",
       "534712                    2006                        13712   \n",
       "537855                       5                          407   \n",
       "539383                       4                          581   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "257                           23655                    327   \n",
       "405                          170005                   1268   \n",
       "2764                       20200000                    601   \n",
       "4419                         558001                    883   \n",
       "4822                         125290                   1747   \n",
       "...                             ...                    ...   \n",
       "534708                      7812389                   5792   \n",
       "534711                      7878627                   5792   \n",
       "534712                      7878088                   5792   \n",
       "537855                        11632                    407   \n",
       "539383                        11632                    581   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "257                         0                   25.30                  74.94   \n",
       "405                         0                   40.20                 204.50   \n",
       "2764                        0                    0.34                  11.41   \n",
       "4419                        0                    1.21                  27.77   \n",
       "4822                        0                   46.10                 235.00   \n",
       "...                       ...                     ...                    ...   \n",
       "534708                      0                    7.45                 126.06   \n",
       "534711                      0                    4.89                 110.10   \n",
       "534712                      0                    4.90                 110.25   \n",
       "537855                      0                   45.22                 135.60   \n",
       "539383                      0                   72.60                 205.40   \n",
       "\n",
       "        bwd_packet_length_max  bwd_packet_length_min  bwd_packet_length_mean  \\\n",
       "257                      2896                      0                  815.50   \n",
       "405                      2836                      0                 1269.00   \n",
       "2764                    11680                      0                 2492.00   \n",
       "4419                     1708                      0                  541.50   \n",
       "4822                     2836                      0                 1193.00   \n",
       "...                       ...                    ...                     ...   \n",
       "534708                  13032                      0                 3780.00   \n",
       "534711                  17376                      0                 3884.00   \n",
       "534712                  13032                      0                 3928.00   \n",
       "537855                   5792                      0                 2326.00   \n",
       "539383                  11632                      0                 2908.00   \n",
       "\n",
       "        bwd_packet_length_std  flow_bytes/s  flow_packets/s  flow_iat_mean  \\\n",
       "257                    834.50        211.07            0.49     2096456.80   \n",
       "405                    523.00       1476.68            2.15      466214.62   \n",
       "2764                  1514.00    1833440.95         1696.77         589.39   \n",
       "4419                   265.20       4667.42           17.21       58137.05   \n",
       "4822                   563.50       1121.87            1.79      559962.06   \n",
       "...                       ...           ...             ...            ...   \n",
       "534708                2380.00      65683.39           40.83       24498.28   \n",
       "534711                2524.00      66155.61           40.51       24689.49   \n",
       "534712                2444.00      66152.77           40.26       24843.10   \n",
       "537855                2640.00       1022.46            1.19      905738.56   \n",
       "539383                5816.00       1029.99            1.01     1077947.90   \n",
       "\n",
       "        flow_iat_std  flow_iat_max  flow_iat_min  fwd_iat_total  fwd_iat_mean  \\\n",
       "257       4105428.50      10200000            -1      115000000    4434812.50   \n",
       "405       2058548.80      10000000            -1      118000000     986821.00   \n",
       "2764        47195.49       6391143            -1       11000000       1040.33   \n",
       "4419       131034.26       1661402            -1      120000000     116217.70   \n",
       "4822      2262796.80      10000000            -1      116000000    1136037.50   \n",
       "...              ...           ...           ...            ...           ...   \n",
       "534708     153803.62       1995346            -1      119000000      42576.80   \n",
       "534711     152728.52        996008            -1      119000000      42531.96   \n",
       "534712     153403.72        995988            -1      119000000      42666.88   \n",
       "537855    2239551.00       6772201            -1        6775695     846961.90   \n",
       "539383    2432765.50       6853719           -11        6858257     979751.00   \n",
       "\n",
       "        fwd_iat_std  fwd_iat_max  fwd_iat_min  bwd_iat_total  bwd_iat_mean  \\\n",
       "257      5079964.00     10200000           17      113000000    4018234.00   \n",
       "405      2925038.20     10000000            1      114000000     855024.06   \n",
       "2764       62713.59      6391245            0       11000000       1358.02   \n",
       "4419      166736.22      1661447           19      120000000     116301.48   \n",
       "4822     3139435.20     10000000            1      116000000    1114151.60   \n",
       "...             ...          ...          ...            ...           ...   \n",
       "534708    201441.28      1996118            0      119000000      57723.93   \n",
       "534711    199646.58       996884            0      119000000      58855.26   \n",
       "534712    200137.52       996943            0      119000000      59480.77   \n",
       "537855   2394782.80      6773747           -1       11800000    2943427.50   \n",
       "539383   2591271.80      6856203          195       11900000    3952473.80   \n",
       "\n",
       "        bwd_iat_std  bwd_iat_max  bwd_iat_min  fwd_psh_flags  bwd_psh_flags  \\\n",
       "257      5081003.00     10200000           48              0              0   \n",
       "405      2775126.00     10000000            1              0              0   \n",
       "2764       71948.18      6407401            0              0              0   \n",
       "4419      167119.73      1661420          210              1              0   \n",
       "4822     3114785.50     10100000            2              0              0   \n",
       "...             ...          ...          ...            ...            ...   \n",
       "534708    233324.72      1995346            1              0              0   \n",
       "534711    232967.11       996008            1              0              0   \n",
       "534712    234233.89       995988            1              0              0   \n",
       "537855   3474753.50      6772201           48              0              0   \n",
       "539383   3545086.00      6853708          904              0              0   \n",
       "\n",
       "        fwd_urg_flags  bwd_urg_flags  fwd_header_length  bwd_header_length  \\\n",
       "257                 0              0                872                936   \n",
       "405                 0              0               3880               4296   \n",
       "2764                0              0             318120             162092   \n",
       "4419                0              0              33024              32960   \n",
       "4822                0              0               3304               3368   \n",
       "...               ...            ...                ...                ...   \n",
       "534708              0              0              89664              66144   \n",
       "534711              0              0              89772              64896   \n",
       "534712              0              0              89504              64192   \n",
       "537855              0              0                304                160   \n",
       "539383              0              0                264                136   \n",
       "\n",
       "        fwd_packets/s  bwd_packets/s  min_packet_length  max_packet_length  \\\n",
       "257              0.23           0.25                  0               2896   \n",
       "405              1.02           1.13                  0               2836   \n",
       "2764           961.33         735.45                  0              11680   \n",
       "4419             8.61           8.60                  0               1708   \n",
       "4822             0.89           0.91                  0               2836   \n",
       "...               ...            ...                ...                ...   \n",
       "534708          23.50          17.33                  0              13032   \n",
       "534711          23.51          17.00                  0              17376   \n",
       "534712          23.45          16.82                  0              13032   \n",
       "537855           0.76           0.42                  0               5792   \n",
       "539383           0.67           0.34                  0              11632   \n",
       "\n",
       "        packet_length_mean  packet_length_std  packet_length_variance  \\\n",
       "257                 427.00             714.50               510155.66   \n",
       "405                 683.00             735.00               540388.06   \n",
       "2764               1080.00            1587.00              2518463.80   \n",
       "4419                271.50             329.80               108734.02   \n",
       "4822                622.00             719.00               516610.94   \n",
       "...                    ...                ...                     ...   \n",
       "534708             1610.00            2428.00              5893055.50   \n",
       "534711             1634.00            2520.00              6348930.50   \n",
       "534712             1644.00            2498.00              6244886.00   \n",
       "537855              802.50            1802.00              3245484.20   \n",
       "539383              939.50            3216.00             10300000.00   \n",
       "\n",
       "        fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "257                  0               0               0               1   \n",
       "405                  0               0               0               1   \n",
       "2764                 0               0               0               1   \n",
       "4419                 0               1               0               0   \n",
       "4822                 0               0               0               1   \n",
       "...                ...             ...             ...             ...   \n",
       "534708               0               0               0               0   \n",
       "534711               0               0               0               0   \n",
       "534712               0               0               0               0   \n",
       "537855               0               0               0               1   \n",
       "539383               0               0               0               1   \n",
       "\n",
       "        ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "257                  0               0               0               0   \n",
       "405                  0               0               0               0   \n",
       "2764                 0               0               0               0   \n",
       "4419                 1               0               0               0   \n",
       "4822                 0               0               0               0   \n",
       "...                ...             ...             ...             ...   \n",
       "534708               1               0               0               0   \n",
       "534711               1               0               0               0   \n",
       "534712               1               0               0               0   \n",
       "537855               1               0               0               0   \n",
       "539383               0               0               0               0   \n",
       "\n",
       "        down/up_ratio  average_packet_size  avg_fwd_segment_size  \\\n",
       "257                 1               434.50                 25.30   \n",
       "405                 1               685.50                 40.20   \n",
       "2764                0              1081.00                  0.34   \n",
       "4419                0               271.80                  1.21   \n",
       "4822                1               625.00                 46.10   \n",
       "...               ...                  ...                   ...   \n",
       "534708              0              1610.00                  7.45   \n",
       "534711              0              1634.00                  4.89   \n",
       "534712              0              1644.00                  4.90   \n",
       "537855              0               860.00                 45.22   \n",
       "539383              0              1018.00                 72.60   \n",
       "\n",
       "        avg_bwd_segment_size  fwd_header_length.1  fwd_avg_bytes/bulk  \\\n",
       "257                   815.50                  872                   0   \n",
       "405                  1269.00                 3880                   0   \n",
       "2764                 2492.00               318120                   0   \n",
       "4419                  541.50                33024                   0   \n",
       "4822                 1193.00                 3304                   0   \n",
       "...                      ...                  ...                 ...   \n",
       "534708               3780.00                89664                   0   \n",
       "534711               3884.00                89772                   0   \n",
       "534712               3928.00                89504                   0   \n",
       "537855               2326.00                  304                   0   \n",
       "539383               2908.00                  264                   0   \n",
       "\n",
       "        fwd_avg_packets/bulk  fwd_avg_bulk_rate  bwd_avg_bytes/bulk  \\\n",
       "257                        0                  0                   0   \n",
       "405                        0                  0                   0   \n",
       "2764                       0                  0                   0   \n",
       "4419                       0                  0                   0   \n",
       "4822                       0                  0                   0   \n",
       "...                      ...                ...                 ...   \n",
       "534708                     0                  0                   0   \n",
       "534711                     0                  0                   0   \n",
       "534712                     0                  0                   0   \n",
       "537855                     0                  0                   0   \n",
       "539383                     0                  0                   0   \n",
       "\n",
       "        bwd_avg_packets/bulk  bwd_avg_bulk_rate  subflow_fwd_packets  \\\n",
       "257                        0                  0                   27   \n",
       "405                        0                  0                  121   \n",
       "2764                       0                  0                10593   \n",
       "4419                       0                  0                 1032   \n",
       "4822                       0                  0                  103   \n",
       "...                      ...                ...                  ...   \n",
       "534708                     0                  0                 2802   \n",
       "534711                     0                  0                 2805   \n",
       "534712                     0                  0                 2797   \n",
       "537855                     0                  0                    9   \n",
       "539383                     0                  0                    8   \n",
       "\n",
       "        subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "257                   683                   29              23655   \n",
       "405                  4861                  134             170005   \n",
       "2764                 3603                 8104           20199367   \n",
       "4419                 1251                 1030             558001   \n",
       "4822                 4748                  105             125290   \n",
       "...                   ...                  ...                ...   \n",
       "534708              20858                 2067            7812389   \n",
       "534711              13712                 2028            7878627   \n",
       "534712              13712                 2006            7878088   \n",
       "537855                407                    5              11632   \n",
       "539383                581                    4              11632   \n",
       "\n",
       "        init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd  \\\n",
       "257                      29200                      122                 4   \n",
       "405                      29200                      424                 7   \n",
       "2764                      8192                     1252                10   \n",
       "4419                       258                      258                 8   \n",
       "4822                     29200                      442                 6   \n",
       "...                        ...                      ...               ...   \n",
       "534708                     235                      349               123   \n",
       "534711                     349                      349               120   \n",
       "534712                     349                      349               120   \n",
       "537855                   28960                      235                 1   \n",
       "539383                   29200                      236                 1   \n",
       "\n",
       "        min_seg_size_forward  active_mean  active_std  active_max  active_min  \\\n",
       "257                       32     29552.54    24875.20      104553       21951   \n",
       "405                       32    327637.72   965977.40     3240169       36253   \n",
       "2764                      20   1795748.00        0.00     1795748     1795748   \n",
       "4419                      32         0.00        0.00           0           0   \n",
       "4822                      32    523658.00  1614832.00     5392559       36192   \n",
       "...                      ...          ...         ...         ...         ...   \n",
       "534708                    32         0.00        0.00           0           0   \n",
       "534711                    32         0.00        0.00           0           0   \n",
       "534712                    32         0.00        0.00           0           0   \n",
       "537855                    32       891.00        0.00         891         891   \n",
       "539383                    32       899.00        0.00         899         899   \n",
       "\n",
       "         idle_mean  idle_std  idle_max  idle_min          label  \n",
       "257    10200000.00  63927.73  10200000  10000000         BENIGN  \n",
       "405    10000000.00   2724.28  10000000  10000000         BENIGN  \n",
       "2764    6391143.00      0.00   6391143   6391143         BENIGN  \n",
       "4419          0.00      0.00         0         0         BENIGN  \n",
       "4822   10000000.00   2242.80  10000000  10000000         BENIGN  \n",
       "...            ...       ...       ...       ...            ...  \n",
       "534708        0.00      0.00         0         0     Heartbleed  \n",
       "534711        0.00      0.00         0         0     Heartbleed  \n",
       "534712        0.00      0.00         0         0     Heartbleed  \n",
       "537855  6772201.00      0.00   6772201   6772201  DoS GoldenEye  \n",
       "539383  6853719.00      0.00   6853719   6853719  DoS GoldenEye  \n",
       "\n",
       "[402 rows x 79 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['flow_iat_min'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "BENIGN                        240000\n",
       "DoS Hulk                      115974\n",
       "PortScan                       79660\n",
       "DDoS                           64366\n",
       "DoS GoldenEye                  10293\n",
       "FTP-Patator                     7938\n",
       "SSH-Patator                     5897\n",
       "DoS slowloris                   5796\n",
       "DoS Slowhttptest                5499\n",
       "Bot                             1966\n",
       "Web Attack ÔøΩ Brute Force        1507\n",
       "Web Attack ÔøΩ XSS                 652\n",
       "Infiltration                      36\n",
       "Web Attack ÔøΩ Sql Injection        21\n",
       "Heartbleed                        11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'PortScan', 'DoS Hulk', 'DDoS', 'Bot', 'Infiltration',\n",
       "       'Web Attack ÔøΩ Brute Force', 'Web Attack ÔøΩ XSS',\n",
       "       'Web Attack ÔøΩ Sql Injection', 'FTP-Patator', 'SSH-Patator',\n",
       "       'DoS slowloris', 'DoS Slowhttptest', 'DoS GoldenEye', 'Heartbleed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_encoder\n",
    "label_encoder = LabelEncoder()\n",
    "#label_encoder.fit(df['label'])\n",
    "df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk',\n",
       "       'DoS Slowhttptest', 'DoS slowloris', 'FTP-Patator', 'Heartbleed',\n",
       "       'Infiltration', 'PortScan', 'SSH-Patator',\n",
       "       'Web Attack ÔøΩ Brute Force', 'Web Attack ÔøΩ Sql Injection',\n",
       "       'Web Attack ÔøΩ XSS'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
      "–î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: 0.8\n",
      "C—Ç—Ä–æ–∫, —Å—Ç–æ–ª–±—Ü–æ–≤: (431692, 78)\n",
      "–î–æ–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞: 3.026\n",
      "- - - - - - - - - - \n",
      "–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
      "–î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: 0.2\n",
      "C—Ç—Ä–æ–∫, —Å—Ç–æ–ª–±—Ü–æ–≤: (107924, 78)\n",
      "–î–æ–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞: 3.026\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "df_sample = df#.sample(160000, random_state=RANDOM_STATE).copy() # sample!!!!!!!!!!!!\n",
    "df_sample = df_sample.fillna(-1)\n",
    "df_sample.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "#df_sample[np.isfinite(df_sample) == True] = 0\n",
    "\n",
    "X, y = df_sample.drop('label', axis=1), df_sample['label'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=df_sample['label']\n",
    "    )\n",
    "\n",
    "print('–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:')\n",
    "print('–î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞:', round(y_train.shape[0]/X.shape[0], 2))\n",
    "print('C—Ç—Ä–æ–∫, —Å—Ç–æ–ª–±—Ü–æ–≤:', X_train.shape)\n",
    "print('–î–æ–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:', round(y_train.mean(), 3))\n",
    "print('- '*10)\n",
    "\n",
    "\n",
    "print('–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:')\n",
    "print('–î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞:', round(y_test.shape[0]/X.shape[0], 2))\n",
    "print('C—Ç—Ä–æ–∫, —Å—Ç–æ–ª–±—Ü–æ–≤:', X_test.shape)\n",
    "print('–î–æ–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:', round(y_test.mean(), 3))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['destination_port']\n",
      "–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['urg_flag_count', 'active_std', 'fwd_packet_length_max', 'fin_flag_count', 'total_length_of_fwd_packets', 'active_min', 'flow_iat_mean', 'bwd_packet_length_mean', 'packet_length_std', 'min_packet_length', 'average_packet_size', 'fwd_header_length.1', 'bwd_packets/s', 'idle_max', 'subflow_bwd_packets', 'avg_bwd_segment_size', 'fwd_iat_mean', 'bwd_psh_flags', 'bwd_avg_bytes/bulk', 'psh_flag_count', 'fwd_packets/s', 'flow_duration', 'max_packet_length', 'flow_packets/s', 'down/up_ratio', 'total_backward_packets', 'fwd_urg_flags', 'total_fwd_packets', 'bwd_urg_flags', 'active_mean', 'fwd_iat_total', 'init_win_bytes_backward', 'rst_flag_count', 'active_max', 'bwd_avg_packets/bulk', 'fwd_iat_std', 'min_seg_size_forward', 'fwd_avg_bulk_rate', 'fwd_psh_flags', 'cwe_flag_count', 'act_data_pkt_fwd', 'bwd_iat_total', 'flow_iat_max', 'bwd_packet_length_min', 'idle_min', 'fwd_packet_length_std', 'bwd_packet_length_std', 'bwd_iat_max', 'bwd_avg_bulk_rate', 'flow_iat_min', 'subflow_fwd_bytes', 'subflow_bwd_bytes', 'bwd_packet_length_max', 'fwd_iat_max', 'bwd_header_length', 'avg_fwd_segment_size', 'flow_bytes/s', 'syn_flag_count', 'idle_std', 'ece_flag_count', 'fwd_avg_bytes/bulk', 'subflow_fwd_packets', 'bwd_iat_min', 'fwd_packet_length_min', 'total_length_of_bwd_packets', 'idle_mean', 'fwd_packet_length_mean', 'packet_length_mean', 'fwd_iat_min', 'packet_length_variance', 'ack_flag_count', 'init_win_bytes_forward', 'fwd_avg_packets/bulk', 'flow_iat_std', 'bwd_iat_std', 'bwd_iat_mean', 'fwd_header_length']\n"
     ]
    }
   ],
   "source": [
    "#categorical = list(X_train.select_dtypes('object').columns)\n",
    "categorical = ['destination_port']\n",
    "print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical}\")\n",
    "\n",
    "numerical = list(set(X_train.select_dtypes('number').columns) - set(categorical))\n",
    "print(f\"–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {numerical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = OneVsRestClassifier(Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_pipe = Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "# num_pipe = Pipeline([('scaler', StandardScaler())])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     [\n",
    "#         ('cat', cat_pipe, categorical),\n",
    "#         ('num', num_pipe, numerical),  # 0.888529\n",
    "#     ], remainder='passthrough')\n",
    "\n",
    "\n",
    "pipe_w = Pipeline(\n",
    "    [\n",
    "        # ('preprocessor', preprocessor),\n",
    "        (\"regressor\", clf)\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid_w = [\n",
    "    {\n",
    "        'regressor': [CatBoostClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            #silent=True,\n",
    "            loss_function='MultiClass',\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=300,\n",
    "            task_type='GPU',\n",
    "            cat_features=categorical\n",
    "        )],\n",
    "         'regressor__iterations': range(900, 1200, 100),\n",
    "         'regressor__depth' : range(6, 9, 1),\n",
    "    }]\n",
    "\n",
    "\n",
    "grid_w = RandomizedSearchCV(pipe_w, param_grid_w, n_iter=3,\n",
    "                            cv=5, scoring='f1_macro', verbose=3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.6400721\ttotal: 125ms\tremaining: 2m 4s\n",
      "300:\tlearn: 0.0051678\ttotal: 33.9s\tremaining: 1m 18s\n",
      "600:\tlearn: 0.0041532\ttotal: 1m 4s\tremaining: 42.8s\n",
      "900:\tlearn: 0.0036411\ttotal: 1m 33s\tremaining: 10.3s\n",
      "999:\tlearn: 0.0035229\ttotal: 1m 43s\tremaining: 0us\n",
      "[CV 1/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=8, regressor__iterations=1000;, score=0.917 total time= 1.8min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.6453553\ttotal: 102ms\tremaining: 1m 41s\n",
      "300:\tlearn: 0.0052443\ttotal: 30.1s\tremaining: 1m 9s\n",
      "600:\tlearn: 0.0041170\ttotal: 1m 1s\tremaining: 40.9s\n",
      "900:\tlearn: 0.0036489\ttotal: 1m 33s\tremaining: 10.3s\n",
      "999:\tlearn: 0.0035173\ttotal: 1m 44s\tremaining: 0us\n",
      "[CV 2/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=8, regressor__iterations=1000;, score=0.925 total time= 1.8min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.6489378\ttotal: 107ms\tremaining: 1m 46s\n",
      "300:\tlearn: 0.0053579\ttotal: 31.6s\tremaining: 1m 13s\n",
      "600:\tlearn: 0.0042781\ttotal: 1m 4s\tremaining: 43.1s\n",
      "900:\tlearn: 0.0038163\ttotal: 1m 38s\tremaining: 10.9s\n",
      "999:\tlearn: 0.0037004\ttotal: 1m 50s\tremaining: 0us\n",
      "[CV 3/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=8, regressor__iterations=1000;, score=0.904 total time= 1.9min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.6394531\ttotal: 146ms\tremaining: 2m 25s\n",
      "300:\tlearn: 0.0052303\ttotal: 34.6s\tremaining: 1m 20s\n",
      "600:\tlearn: 0.0041795\ttotal: 1m 8s\tremaining: 45.3s\n",
      "900:\tlearn: 0.0037103\ttotal: 1m 42s\tremaining: 11.2s\n",
      "999:\tlearn: 0.0035923\ttotal: 1m 53s\tremaining: 0us\n",
      "[CV 4/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=8, regressor__iterations=1000;, score=0.888 total time= 2.0min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.6527175\ttotal: 118ms\tremaining: 1m 57s\n",
      "300:\tlearn: 0.0051675\ttotal: 32.5s\tremaining: 1m 15s\n",
      "600:\tlearn: 0.0041393\ttotal: 1m 3s\tremaining: 42.3s\n",
      "900:\tlearn: 0.0036680\ttotal: 1m 34s\tremaining: 10.3s\n",
      "999:\tlearn: 0.0035484\ttotal: 1m 44s\tremaining: 0us\n",
      "[CV 5/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=8, regressor__iterations=1000;, score=0.918 total time= 1.8min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.7234390\ttotal: 76.6ms\tremaining: 1m 16s\n",
      "300:\tlearn: 0.0059180\ttotal: 21.1s\tremaining: 49s\n",
      "600:\tlearn: 0.0047991\ttotal: 43.2s\tremaining: 28.6s\n",
      "900:\tlearn: 0.0042930\ttotal: 1m 6s\tremaining: 7.27s\n",
      "999:\tlearn: 0.0041760\ttotal: 1m 14s\tremaining: 0us\n",
      "[CV 1/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=6, regressor__iterations=1000;, score=0.913 total time= 1.3min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.7220228\ttotal: 109ms\tremaining: 1m 48s\n",
      "300:\tlearn: 0.0060754\ttotal: 23.2s\tremaining: 53.8s\n",
      "600:\tlearn: 0.0048168\ttotal: 45.5s\tremaining: 30.2s\n",
      "900:\tlearn: 0.0042919\ttotal: 1m 7s\tremaining: 7.41s\n",
      "999:\tlearn: 0.0041493\ttotal: 1m 14s\tremaining: 0us\n",
      "[CV 2/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=6, regressor__iterations=1000;, score=0.926 total time= 1.3min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.7252327\ttotal: 77.2ms\tremaining: 1m 17s\n",
      "300:\tlearn: 0.0061967\ttotal: 22s\tremaining: 51.1s\n",
      "600:\tlearn: 0.0050081\ttotal: 43.4s\tremaining: 28.8s\n",
      "900:\tlearn: 0.0045239\ttotal: 1m 4s\tremaining: 7.04s\n",
      "999:\tlearn: 0.0043940\ttotal: 1m 11s\tremaining: 0us\n",
      "[CV 3/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=6, regressor__iterations=1000;, score=0.909 total time= 1.2min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.7220726\ttotal: 81.1ms\tremaining: 1m 21s\n",
      "300:\tlearn: 0.0059998\ttotal: 20.7s\tremaining: 48.1s\n",
      "600:\tlearn: 0.0048453\ttotal: 41.4s\tremaining: 27.5s\n",
      "900:\tlearn: 0.0043466\ttotal: 1m 2s\tremaining: 6.84s\n",
      "999:\tlearn: 0.0042252\ttotal: 1m 9s\tremaining: 0us\n",
      "[CV 4/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=6, regressor__iterations=1000;, score=0.877 total time= 1.2min\n",
      "Learning rate set to 0.21384\n",
      "0:\tlearn: 0.7329917\ttotal: 99.4ms\tremaining: 1m 39s\n",
      "300:\tlearn: 0.0058751\ttotal: 21.1s\tremaining: 49.1s\n",
      "600:\tlearn: 0.0046997\ttotal: 42.1s\tremaining: 27.9s\n",
      "900:\tlearn: 0.0042359\ttotal: 1m 2s\tremaining: 6.9s\n",
      "999:\tlearn: 0.0041152\ttotal: 1m 9s\tremaining: 0us\n",
      "[CV 5/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=6, regressor__iterations=1000;, score=0.913 total time= 1.2min\n",
      "Learning rate set to 0.197519\n",
      "0:\tlearn: 0.7654349\ttotal: 96.5ms\tremaining: 1m 46s\n",
      "300:\tlearn: 0.0056720\ttotal: 25.1s\tremaining: 1m 6s\n",
      "600:\tlearn: 0.0045286\ttotal: 49.9s\tremaining: 41.5s\n",
      "900:\tlearn: 0.0040064\ttotal: 1m 14s\tremaining: 16.5s\n",
      "1099:\tlearn: 0.0037936\ttotal: 1m 31s\tremaining: 0us\n",
      "[CV 1/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=7, regressor__iterations=1100;, score=0.904 total time= 1.6min\n",
      "Learning rate set to 0.197519\n",
      "0:\tlearn: 0.7701313\ttotal: 106ms\tremaining: 1m 56s\n",
      "300:\tlearn: 0.0057720\ttotal: 24s\tremaining: 1m 3s\n",
      "600:\tlearn: 0.0045059\ttotal: 48s\tremaining: 39.9s\n",
      "900:\tlearn: 0.0040011\ttotal: 1m 12s\tremaining: 16s\n",
      "1099:\tlearn: 0.0037745\ttotal: 1m 28s\tremaining: 0us\n",
      "[CV 2/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=7, regressor__iterations=1100;, score=0.926 total time= 1.5min\n",
      "Learning rate set to 0.197519\n",
      "0:\tlearn: 0.7730627\ttotal: 89.2ms\tremaining: 1m 37s\n",
      "300:\tlearn: 0.0058350\ttotal: 23.9s\tremaining: 1m 3s\n",
      "600:\tlearn: 0.0047131\ttotal: 50.3s\tremaining: 41.8s\n",
      "900:\tlearn: 0.0042165\ttotal: 1m 14s\tremaining: 16.4s\n",
      "1099:\tlearn: 0.0039961\ttotal: 1m 29s\tremaining: 0us\n",
      "[CV 3/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=7, regressor__iterations=1100;, score=0.906 total time= 1.5min\n",
      "Learning rate set to 0.197519\n",
      "0:\tlearn: 0.7649412\ttotal: 86.8ms\tremaining: 1m 35s\n",
      "300:\tlearn: 0.0056803\ttotal: 25s\tremaining: 1m 6s\n",
      "600:\tlearn: 0.0045613\ttotal: 50.5s\tremaining: 41.9s\n",
      "900:\tlearn: 0.0040722\ttotal: 1m 16s\tremaining: 16.9s\n",
      "1099:\tlearn: 0.0038488\ttotal: 1m 32s\tremaining: 0us\n",
      "[CV 4/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=7, regressor__iterations=1100;, score=0.875 total time= 1.6min\n",
      "Learning rate set to 0.197519\n",
      "0:\tlearn: 0.7781002\ttotal: 92.3ms\tremaining: 1m 41s\n",
      "300:\tlearn: 0.0056325\ttotal: 24.5s\tremaining: 1m 5s\n",
      "600:\tlearn: 0.0045153\ttotal: 49.4s\tremaining: 41s\n",
      "900:\tlearn: 0.0040143\ttotal: 1m 14s\tremaining: 16.4s\n",
      "1099:\tlearn: 0.0037878\ttotal: 1m 30s\tremaining: 0us\n",
      "[CV 5/5] END regressor=<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>, regressor__depth=7, regressor__iterations=1100;, score=0.895 total time= 1.6min\n",
      "Learning rate set to 0.2238\n",
      "0:\tlearn: 0.6027223\ttotal: 126ms\tremaining: 2m 5s\n",
      "300:\tlearn: 0.0052118\ttotal: 33.8s\tremaining: 1m 18s\n",
      "600:\tlearn: 0.0043220\ttotal: 1m 7s\tremaining: 45s\n",
      "900:\tlearn: 0.0039131\ttotal: 1m 41s\tremaining: 11.2s\n",
      "999:\tlearn: 0.0038096\ttotal: 1m 52s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;regressor&#x27;,\n",
       "                                              OneVsRestClassifier(estimator=Ridge()))]),\n",
       "                   n_iter=3,\n",
       "                   param_distributions=[{&#x27;regressor&#x27;: [&lt;catboost.core.CatBoostClassifier object at 0x00000216367FFE80&gt;],\n",
       "                                         &#x27;regressor__depth&#x27;: range(6, 9),\n",
       "                                         &#x27;regressor__iterations&#x27;: range(900, 1200, 100)}],\n",
       "                   random_state=42, scoring=&#x27;f1_macro&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;regressor&#x27;,\n",
       "                                              OneVsRestClassifier(estimator=Ridge()))]),\n",
       "                   n_iter=3,\n",
       "                   param_distributions=[{&#x27;regressor&#x27;: [&lt;catboost.core.CatBoostClassifier object at 0x00000216367FFE80&gt;],\n",
       "                                         &#x27;regressor__depth&#x27;: range(6, 9),\n",
       "                                         &#x27;regressor__iterations&#x27;: range(900, 1200, 100)}],\n",
       "                   random_state=42, scoring=&#x27;f1_macro&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;regressor&#x27;, OneVsRestClassifier(estimator=Ridge()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">regressor: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=Ridge())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('regressor',\n",
       "                                              OneVsRestClassifier(estimator=Ridge()))]),\n",
       "                   n_iter=3,\n",
       "                   param_distributions=[{'regressor': [<catboost.core.CatBoostClassifier object at 0x00000216367FFE80>],\n",
       "                                         'regressor__depth': range(6, 9),\n",
       "                                         'regressor__iterations': range(900, 1200, 100)}],\n",
       "                   random_state=42, scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_w.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__iterations</th>\n",
       "      <th>param_regressor__depth</th>\n",
       "      <th>param_regressor</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.16</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'regressor__iterations': 1000, 'regressor__de...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.14</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'regressor__iterations': 1000, 'regressor__de...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.67</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1100</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>{'regressor__iterations': 1100, 'regressor__de...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         110.16          4.04             0.76            0.36   \n",
       "1          74.14          2.35             0.48            0.12   \n",
       "2          92.67          1.24             0.45            0.01   \n",
       "\n",
       "  param_regressor__iterations param_regressor__depth  \\\n",
       "0                        1000                      8   \n",
       "1                        1000                      6   \n",
       "2                        1100                      7   \n",
       "\n",
       "                                     param_regressor  \\\n",
       "0  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "1  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "2  <catboost.core.CatBoostClassifier object at 0x...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'regressor__iterations': 1000, 'regressor__de...               0.92   \n",
       "1  {'regressor__iterations': 1000, 'regressor__de...               0.91   \n",
       "2  {'regressor__iterations': 1100, 'regressor__de...               0.90   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0               0.93               0.90               0.89               0.92   \n",
       "1               0.93               0.91               0.88               0.91   \n",
       "2               0.93               0.91               0.87               0.90   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.91            0.01                1  \n",
       "1             0.91            0.02                2  \n",
       "2             0.90            0.02                3  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(grid_w.cv_results_)\n",
    "result.sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "539611    3\n",
       "539612    3\n",
       "539613    3\n",
       "539614    3\n",
       "539615    3\n",
       "Name: label, Length: 539616, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     48000\n",
      "           1       0.96      0.98      0.97       393\n",
      "           2       1.00      1.00      1.00     12873\n",
      "           3       1.00      1.00      1.00      2059\n",
      "           4       1.00      1.00      1.00     23195\n",
      "           5       1.00      0.99      0.99      1100\n",
      "           6       1.00      0.99      0.99      1159\n",
      "           7       1.00      1.00      1.00      1588\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      0.86      0.92         7\n",
      "          10       1.00      1.00      1.00     15932\n",
      "          11       1.00      1.00      1.00      1180\n",
      "          12       0.73      0.79      0.76       302\n",
      "          13       1.00      0.75      0.86         4\n",
      "          14       0.41      0.35      0.37       130\n",
      "\n",
      "    accuracy                           1.00    107924\n",
      "   macro avg       0.94      0.91      0.92    107924\n",
      "weighted avg       1.00      1.00      1.00    107924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#target_names = list(label_encoder.inverse_transform(df['label'].unique()))\n",
    "\n",
    "y_pred = grid_w.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 960x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matr = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=120)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matr)\n",
    "disp = disp.plot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web Attack ÔøΩ Brute Force', 'Web Attack ÔøΩ Sql Injection', 'Web Attack ÔøΩ XSS']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_encoder.inverse_transform([12,13,14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web = df[df['label'].isin([12, 13, 14])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2180, 78), (2180,))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_web = df_web.fillna(-1)\n",
    "df_web.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "#df_sample[np.isfinite(df_sample) == True] = 0\n",
    "\n",
    "X_w, y_w = df_web.drop('label', axis=1), df_web['label']\n",
    "X_w.shape, y_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "num_pipe = Pipeline([('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "     [\n",
    "         ('cat', cat_pipe, categorical),\n",
    "         ('num', num_pipe, numerical), #0.888529\n",
    "     ], remainder ='passthrough') \n",
    "\n",
    "\n",
    "pipe_w = Pipeline(\n",
    "    [\n",
    "        # ('preprocessor', preprocessor),\n",
    "        (\"regressor\", clf)\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid_w = [\n",
    "    {\n",
    "        'regressor': [CatBoostClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            # silent=True,\n",
    "            loss_function='MultiClass',\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=100,\n",
    "            task_type='GPU',\n",
    "            cat_features=categorical\n",
    "        )],\n",
    "        #  'regressor__iterations': range(50, 400, 50),\n",
    "        #  'regressor__depth' : range(1, 8, 1),\n",
    "        #  'regressor__l2_leaf_reg': range(1, 13, 1),\n",
    "        #  'regressor__learning_rate': np.linspace(0.01, 0.5, 15),\n",
    "        #  'regressor__auto_class_weights': ['Balanced', 'SqrtBalanced']\n",
    "    },\n",
    "    {\n",
    "        'regressor': [RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "        )]\n",
    "    },\n",
    "    {\n",
    "        'regressor': [XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            # objective='multi:softmax',\n",
    "            eval_metric='mlogloss',\n",
    "            # early_stopping_rounds=10,\n",
    "            # num_classes=15\n",
    "        )],\n",
    "\n",
    "        'regressor__max_depth':range(2, 12, 2),\n",
    "        \"regressor__subsample\":[0.5, 0.75, 1],\n",
    "        \"regressor__colsample_bytree\":[0.5, 0.75, 1],\n",
    "        \"regressor__min_child_weight\":[1, 5, 15],\n",
    "        \"regressor__learning_rate\":[0.3, 0.1, 0.03],\n",
    "        \"regressor__n_estimators\":[100]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "grid_w = RandomizedSearchCV(pipe_w, param_grid_w, n_iter=30,\n",
    "                            cv=5, scoring='f1_macro', verbose=3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=6, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=6, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.03, regressor__max_depth=10, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.03, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=2, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=1, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.1, regressor__max_depth=10, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.1, regressor__max_depth=8, regressor__min_child_weight=1, regressor__n_estimators=100, regressor__subsample=0.5;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.75, regressor__learning_rate=0.3, regressor__max_depth=8, regressor__min_child_weight=15, regressor__n_estimators=100, regressor__subsample=1;, score=nan total time=   0.0s\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...), regressor__colsample_bytree=0.5, regressor__learning_rate=0.3, regressor__max_depth=4, regressor__min_child_weight=5, regressor__n_estimators=100, regressor__subsample=0.75;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n    raise ValueError(\nValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [12 13 14]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\Analyzis&ModelCreating.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/GITlocal/yandex_hackaton_infobez/Analyzis%26ModelCreating.ipynb#Y154sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grid_w\u001b[39m.\u001b[39;49mfit(X_w, y_w)\n",
      "File \u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1810\u001b[0m         ParameterSampler(\n\u001b[0;32m   1811\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1812\u001b[0m         )\n\u001b[0;32m   1813\u001b[0m     )\n",
      "File \u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"e:\\GITlocal\\yandex_hackaton_infobez\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n    raise ValueError(\nValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [12 13 14]\n"
     ]
    }
   ],
   "source": [
    "grid_w.fit(X_w, y_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'feature_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\Analyzis&ModelCreating.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/GITlocal/yandex_hackaton_infobez/Analyzis%26ModelCreating.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grid\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfeature_importance()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'feature_importance'"
     ]
    }
   ],
   "source": [
    "grid.best_estimator_.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'get_feature_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\GITlocal\\yandex_hackaton_infobez\\Analyzis&ModelCreating.ipynb Cell 50\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/GITlocal/yandex_hackaton_infobez/Analyzis%26ModelCreating.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m feature_importance \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mget_feature_importance()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITlocal/yandex_hackaton_infobez/Analyzis%26ModelCreating.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(feature_importance)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITlocal/yandex_hackaton_infobez/Analyzis%26ModelCreating.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m24\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'get_feature_importance'"
     ]
    }
   ],
   "source": [
    "feature_importance = grid.get_feature_importance()\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(10, 24))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "num_pipe = Pipeline([('scaler', MinM())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "     [\n",
    "         ('cat', cat_pipe, categorical),\n",
    "         ('num', num_pipe, numerical), #0.888529\n",
    "     ], remainder ='passthrough') \n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"regressor\", XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "     {\n",
    "         'regressor': [XGBClassifier(\n",
    "             tree_method=\"hist\", \n",
    "             device=\"cuda\", \n",
    "             #objective='multi:softmax', \n",
    "             eval_metric='mlogloss', \n",
    "             #early_stopping_rounds=10,\n",
    "             #num_classes=15\n",
    "             )],\n",
    "\n",
    "             'regressor__max_depth':range(2, 12, 2),\n",
    "             \"regressor__subsample\":[0.5, 0.75, 1],\n",
    "             \"regressor__colsample_bytree\":[0.5, 0.75, 1],\n",
    "             \"regressor__min_child_weight\":[1,5,15],\n",
    "             \"regressor__learning_rate\":[0.3, 0.1, 0.03],\n",
    "             \"regressor__n_estimators\":[100]\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "grid_lgbm = RandomizedSearchCV(\n",
    "    pipe, \n",
    "    param_grid, \n",
    "    n_iter=30 ,\n",
    "    cv=5, \n",
    "    scoring='f1_macro', \n",
    "    verbose=3, \n",
    "    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...);, score=0.847 total time=  20.4s\n",
      "[CV 2/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...);, score=0.901 total time=  20.5s\n",
      "[CV 3/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...);, score=0.913 total time=  28.4s\n",
      "[CV 4/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...);, score=0.858 total time=  22.2s\n",
      "[CV 5/5] END regressor=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...);, score=0.903 total time=  20.4s\n",
      "CPU times: total: 2min 18s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;regressor&#x27;,\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            device=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_c...\n",
       "                                                                     interaction_constraints=None,\n",
       "                                                                     learning_rate=None,\n",
       "                                                                     max_bin=None,\n",
       "                                                                     max_cat_threshold=None,\n",
       "                                                                     max_cat_to_onehot=None,\n",
       "                                                                     max_delta_step=None,\n",
       "                                                                     max_depth=None,\n",
       "                                                                     max_leaves=None,\n",
       "                                                                     min_child_weight=None,\n",
       "                                                                     missing=nan,\n",
       "                                                                     monotone_constraints=None,\n",
       "                                                                     multi_strategy=None,\n",
       "                                                                     n_estimators=None,\n",
       "                                                                     n_jobs=None,\n",
       "                                                                     num_parallel_tree=None,\n",
       "                                                                     random_state=None, ...)]}],\n",
       "                   random_state=42, scoring=&#x27;f1_macro&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;regressor&#x27;,\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            device=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_c...\n",
       "                                                                     interaction_constraints=None,\n",
       "                                                                     learning_rate=None,\n",
       "                                                                     max_bin=None,\n",
       "                                                                     max_cat_threshold=None,\n",
       "                                                                     max_cat_to_onehot=None,\n",
       "                                                                     max_delta_step=None,\n",
       "                                                                     max_depth=None,\n",
       "                                                                     max_leaves=None,\n",
       "                                                                     min_child_weight=None,\n",
       "                                                                     missing=nan,\n",
       "                                                                     monotone_constraints=None,\n",
       "                                                                     multi_strategy=None,\n",
       "                                                                     n_estimators=None,\n",
       "                                                                     n_jobs=None,\n",
       "                                                                     num_parallel_tree=None,\n",
       "                                                                     random_state=None, ...)]}],\n",
       "                   random_state=42, scoring=&#x27;f1_macro&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;regressor&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, device=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=None, n_jobs=None,\n",
       "                               num_parallel_tree=None, random_state=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('regressor',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            device=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_c...\n",
       "                                                                     interaction_constraints=None,\n",
       "                                                                     learning_rate=None,\n",
       "                                                                     max_bin=None,\n",
       "                                                                     max_cat_threshold=None,\n",
       "                                                                     max_cat_to_onehot=None,\n",
       "                                                                     max_delta_step=None,\n",
       "                                                                     max_depth=None,\n",
       "                                                                     max_leaves=None,\n",
       "                                                                     min_child_weight=None,\n",
       "                                                                     missing=nan,\n",
       "                                                                     monotone_constraints=None,\n",
       "                                                                     multi_strategy=None,\n",
       "                                                                     n_estimators=None,\n",
       "                                                                     n_jobs=None,\n",
       "                                                                     num_parallel_tree=None,\n",
       "                                                                     random_state=None, ...)]}],\n",
       "                   random_state=42, scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.96</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'regressor': XGBClassifier(base_score=None, b...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0          21.96          3.07             0.50            0.03   \n",
       "\n",
       "                                     param_regressor  \\\n",
       "0  XGBClassifier(base_score=None, booster=None, c...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'regressor': XGBClassifier(base_score=None, b...               0.85   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0               0.90               0.91               0.86               0.90   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.88            0.03                1  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_lgbm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
